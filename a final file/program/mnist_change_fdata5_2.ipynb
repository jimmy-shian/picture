{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 777\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "```text\n",
    ".\n",
    "<!-- ├── dataset\n",
    "│   └── mnist -->\n",
    "│       └── data_new\n",
    "│           ├── \n",
    "│           │   ├── 0.jpg\n",
    "│           │   ├── 1.jpg\n",
    "│           │   ├── ...\n",
    "│           │   └── 59999.jpg\n",
    "│           └── label.txt\n",
    "└── mnist_change.ipynb  (訓練 MNIST)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\",device)\n",
    "DATASET_PATH = './fdata5'  # TODO: adjust path\n",
    "# DATASET_PATH = './data_new'  # TODO: adjust path\n",
    "# DATASET_PATH = './data_color'  # TODO: adjust path\n",
    "# DATASET_PATH = './data_color2'  # TODO: adjust path\n",
    "\n",
    "output_path = './model/hira_fdata5.pt'  # TODO: adjust path\n",
    "epoch = 3000\n",
    "batch_size = 512\n",
    "learning_rate = 0.01\n",
    "img_size=(83, 84)  # TODO: adjust img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def img_convert(input: np.ndarray) -> torch.Tensor: #回傳的是torch.Tensor，用於測試\n",
    "    # 将图像转换为灰度影像\n",
    "    gray_image = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY)\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(gray_image, (83, 84))\n",
    "\n",
    "    equalized_image = cv2.equalizeHist(resized_image)\n",
    "\n",
    "    # 多次侵蚀后膨胀\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    eroded_image = cv2.erode(equalized_image, kernel, iterations=2)\n",
    "    dilated_image = cv2.dilate(eroded_image, kernel, iterations=2)\n",
    "\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(dilated_image, (83, 84))\n",
    "\n",
    "    # 将图像转换为张量并调整形状\n",
    "    tensor_image = transforms.ToTensor()(resized_image)\n",
    "    # tensor_image = torch.unsqueeze(tensor_image, 0)\n",
    "\n",
    "    return tensor_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, path: str=f'/{DATASET_PATH}', data_transform: Optional[Callable]=None) -> None:\n",
    "        super().__init__()\n",
    "        self.dataset_path = Path(path)\n",
    "        # self.data_transform = data_transform\n",
    "        self.image, self.label = self.read_dataset()\n",
    "\n",
    "        assert len(self.image) == len(self.label)\n",
    "        self.length = len(self.image)\n",
    "\n",
    "    def read_dataset(self):  # TODO: adjust this function\n",
    "        image = dict()\n",
    "        for path in self.dataset_path.joinpath('').glob('**/*'):\n",
    "            if path.suffix in ['.jpg']:\n",
    "                test_image = cv2.imread(str(path))  # 使用 cv2 讀取圖片\n",
    "                converted_image = img_convert(test_image)  # 將圖片轉換成您需要的格式\n",
    "                image[int(path.stem)] = np.array(converted_image)\n",
    "\n",
    "                # image[int(path.stem)] = np.array(Image.open(path).copy())\n",
    "        with open(self.dataset_path.joinpath('label.txt'), mode='r') as f:\n",
    "            label = f.read().split(\"\\n\")  # 使用換行符分隔標籤\n",
    "            label = [int(label) for label in label]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.image[index], self.label[index]\n",
    "        # 假设您直接在此处对图像进行转换\n",
    "        # image = img_convert(image)\n",
    "        image = torch.Tensor(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, in_channels=1, out_num=50):  # adjust out_num to 50\n",
    "#         super(CNN, self).__init__()\n",
    "\n",
    "#         self.cnn1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=5, stride=1, padding=0) # out_shape=(32, 79, 80)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2) # out_shape=(32, 39, 40)\n",
    "\n",
    "#         self.cnn2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0) # out_shape=(64, 35, 36)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2) # out_shape=(64, 17, 18)\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_features=64 * 17 * 18, out_features=out_num)  # adjust in_features based on the output shape of the last convolutional layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.maxpool1(x)\n",
    "\n",
    "#         x = self.cnn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.maxpool2(x)\n",
    "\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_num=50):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Add a new hidden layer\n",
    "        self.hidden_layer = nn.Linear(in_features=128 * 17 * 18, out_features=256)  # Adjust the number of output features as needed\n",
    "        # self.dropout = nn.Dropout(0.01)  # Adjust the dropout rate as needed\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=out_num)  # Adjust the input features based on the output shape of the hidden layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.cnn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Pass through the hidden layer and apply dropout\n",
    "        x = self.hidden_layer(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build\n",
    "(1)model, (2)loss-function, (3)optimizer, (4)dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(output_path)  # TODO: adjust path\n",
    "use_pretrained_model = True  # TODO: set True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set new model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (hidden_layer): Linear(in_features=39168, out_features=256, bias=True)\n",
       "  (fc1): Linear(in_features=256, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢查模型檔案是否存在\n",
    "if model_path.exists() and use_pretrained_model:\n",
    "    # 載入預訓練模型\n",
    "    model = torch.jit.load(model_path, map_location=device).to(device)\n",
    "    print('load model')\n",
    "else:\n",
    "    # 建立新模型\n",
    "    model = CNN().to(device)\n",
    "    print('set new model')\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# data_transform = transforms.Compose([\n",
    "#     transforms.Resize(img_size),\n",
    "#     transforms.ToTensor(),   # [0, 1]\n",
    "#     # transforms.Normalize((0.5), (0.5))  # [-1, 1]\n",
    "# ])\n",
    "dataset = MnistDataset(path=DATASET_PATH)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred: torch.Tensor, label: torch.Tensor):\n",
    "    _, pred_label = pred.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    acc = num_correct / label.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_choice = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/3000] loss: 2.283818211790692, acc: 0.5648805353139014\n",
      "Epoch[2/3000] loss: 1.2880408682363451, acc: 0.7988307525224215\n",
      "Epoch[3/3000] loss: 1.2025850380482694, acc: 0.8137077844730941\n",
      "Epoch[4/3000] loss: 1.1287590341450389, acc: 0.8233788186659193\n",
      "Epoch[5/3000] loss: 1.0264995151571095, acc: 0.8325830297085202\n",
      "Epoch[6/3000] loss: 0.8766365289153539, acc: 0.8430396931053811\n",
      "Epoch[7/3000] loss: 0.7247693977682045, acc: 0.8541199551569507\n",
      "Epoch[8/3000] loss: 0.6214639290565868, acc: 0.8662617362668161\n",
      "Epoch[9/3000] loss: 0.5632272235748479, acc: 0.8750797015134529\n",
      "Epoch[10/3000] loss: 0.5286210552192055, acc: 0.8809110496076232\n",
      "Epoch[11/3000] loss: 0.505066721963241, acc: 0.88511596132287\n",
      "Epoch[12/3000] loss: 0.4882740463242937, acc: 0.8884003643497758\n",
      "Epoch[13/3000] loss: 0.4752057046366379, acc: 0.8903228349215246\n",
      "Epoch[14/3000] loss: 0.46461463823179494, acc: 0.8923293862107624\n",
      "Epoch[15/3000] loss: 0.4560605029621466, acc: 0.8940127522421525\n",
      "Epoch[16/3000] loss: 0.44724314796817677, acc: 0.8957907090807176\n",
      "Epoch[17/3000] loss: 0.44017503273594005, acc: 0.8971333730381167\n",
      "Epoch[18/3000] loss: 0.43394597930491235, acc: 0.8980897911995516\n",
      "Epoch[19/3000] loss: 0.42795123661045537, acc: 0.8991670753923767\n",
      "Epoch[20/3000] loss: 0.42223869397768526, acc: 0.9006577564461884\n",
      "Epoch[21/3000] loss: 0.41658434141510803, acc: 0.901452144058296\n",
      "Epoch[22/3000] loss: 0.4113597585214093, acc: 0.9025977438340808\n",
      "Epoch[23/3000] loss: 0.40655780560232596, acc: 0.9033737387892378\n",
      "Epoch[24/3000] loss: 0.4015792082377079, acc: 0.9045377312219731\n",
      "Epoch[25/3000] loss: 0.3968065398757768, acc: 0.9056701933856502\n",
      "Epoch[26/3000] loss: 0.39211422138149965, acc: 0.906450567544843\n",
      "Epoch[27/3000] loss: 0.38781422630553825, acc: 0.9074534052690583\n",
      "Epoch[28/3000] loss: 0.38327870564744076, acc: 0.907777466367713\n",
      "Epoch[29/3000] loss: 0.3786536366282023, acc: 0.909109620235426\n",
      "Epoch[30/3000] loss: 0.3743467300863009, acc: 0.9097060678251122\n",
      "Epoch[31/3000] loss: 0.37015541519284784, acc: 0.9105889153587443\n",
      "Epoch[32/3000] loss: 0.3656192621040772, acc: 0.9113176149103139\n",
      "Epoch[33/3000] loss: 0.36133680275470154, acc: 0.9123134459080717\n",
      "Epoch[34/3000] loss: 0.35674974061715764, acc: 0.9135588915358744\n",
      "Epoch[35/3000] loss: 0.3525925281670596, acc: 0.9142113929372198\n",
      "Epoch[36/3000] loss: 0.3483618627147824, acc: 0.9153307174887892\n",
      "Epoch[37/3000] loss: 0.34358655514337555, acc: 0.9161163466928252\n",
      "Epoch[38/3000] loss: 0.3389654228705996, acc: 0.9169650364349775\n",
      "Epoch[39/3000] loss: 0.3347756303011569, acc: 0.918003783632287\n",
      "Epoch[40/3000] loss: 0.33041046051968376, acc: 0.9188174397421525\n",
      "Epoch[41/3000] loss: 0.32602613529549584, acc: 0.919551394338565\n",
      "Epoch[42/3000] loss: 0.3217192318607873, acc: 0.9208905549327355\n",
      "Epoch[43/3000] loss: 0.31679033334346096, acc: 0.9218539798206279\n",
      "Epoch[44/3000] loss: 0.3116961899359665, acc: 0.9228060187780269\n",
      "Epoch[45/3000] loss: 0.30747840667118403, acc: 0.9236862387892377\n",
      "Epoch[46/3000] loss: 0.30259597521993614, acc: 0.9247276135089686\n",
      "Epoch[47/3000] loss: 0.2986048392183043, acc: 0.9255412696188341\n",
      "Epoch[48/3000] loss: 0.2933358203603013, acc: 0.9267762051569507\n",
      "Epoch[49/3000] loss: 0.28813246016144217, acc: 0.9283255675448431\n",
      "Epoch[50/3000] loss: 0.283540979963247, acc: 0.929116451793722\n",
      "Epoch[51/3000] loss: 0.2790644634264467, acc: 0.9301376821748878\n",
      "Epoch[52/3000] loss: 0.273904742380696, acc: 0.9312517516816144\n",
      "Epoch[53/3000] loss: 0.26887577548288977, acc: 0.9323123948991031\n",
      "Epoch[54/3000] loss: 0.26373731117745686, acc: 0.9336944716928252\n",
      "Epoch[55/3000] loss: 0.25842012314652113, acc: 0.9349609375000001\n",
      "Epoch[56/3000] loss: 0.2537230662500377, acc: 0.9360723794843049\n",
      "Epoch[57/3000] loss: 0.24842798389126902, acc: 0.9374894899103139\n",
      "Epoch[58/3000] loss: 0.24342572709102803, acc: 0.9381122127242152\n",
      "Epoch[59/3000] loss: 0.23850001672057292, acc: 0.93937254764574\n",
      "Epoch[60/3000] loss: 0.23327092325206294, acc: 0.9406495235426009\n",
      "Epoch[61/3000] loss: 0.2277259743133468, acc: 0.9423854400224215\n",
      "Epoch[62/3000] loss: 0.22316574859913155, acc: 0.9427340246636772\n",
      "Epoch[63/3000] loss: 0.21730151931199793, acc: 0.9442702494394619\n",
      "Epoch[64/3000] loss: 0.21180754968470522, acc: 0.9459212093609866\n",
      "Epoch[65/3000] loss: 0.2068799458811636, acc: 0.9467821608744394\n",
      "Epoch[66/3000] loss: 0.20156797606314245, acc: 0.9483499159192825\n",
      "Epoch[67/3000] loss: 0.19638105191190147, acc: 0.9495883548206278\n",
      "Epoch[68/3000] loss: 0.19043427197922505, acc: 0.9510256095852018\n",
      "Epoch[69/3000] loss: 0.18575835805970992, acc: 0.9519102088004484\n",
      "Epoch[70/3000] loss: 0.18005089832661933, acc: 0.9534446818946188\n",
      "Epoch[71/3000] loss: 0.1749112707617037, acc: 0.95472778867713\n",
      "Epoch[72/3000] loss: 0.1693962239263571, acc: 0.956064321748879\n",
      "Epoch[73/3000] loss: 0.16415390878448038, acc: 0.9574901905829596\n",
      "Epoch[74/3000] loss: 0.15969062075352988, acc: 0.9585105451233183\n",
      "Epoch[75/3000] loss: 0.15420606085871902, acc: 0.9600222463565022\n",
      "Epoch[76/3000] loss: 0.14855233498017884, acc: 0.9615646020179373\n",
      "Epoch[77/3000] loss: 0.15925513007448394, acc: 0.9616740821188341\n",
      "Epoch[78/3000] loss: 0.13745169154464396, acc: 0.9649847603699552\n",
      "Epoch[79/3000] loss: 0.1317142256339302, acc: 0.9664123808856503\n",
      "Epoch[80/3000] loss: 0.12749046070559678, acc: 0.967402956838565\n",
      "Epoch[81/3000] loss: 0.12320307491038145, acc: 0.9686054862668162\n",
      "Epoch[82/3000] loss: 0.1180071839519814, acc: 0.9702546945067265\n",
      "Epoch[83/3000] loss: 0.11157432141726327, acc: 0.972035278867713\n",
      "Epoch[84/3000] loss: 0.10630190938777988, acc: 0.9735215807174888\n",
      "Epoch[85/3000] loss: 0.10215917926264985, acc: 0.9741915989349776\n",
      "Epoch[86/3000] loss: 0.09743660507147354, acc: 0.9761648682735427\n",
      "Epoch[87/3000] loss: 0.33554658538457255, acc: 0.9433225896860987\n",
      "Epoch[88/3000] loss: 0.12105867996678224, acc: 0.9681185187780269\n",
      "Epoch[89/3000] loss: 0.09757976979017258, acc: 0.976146475616592\n",
      "Epoch[90/3000] loss: 0.0881270194351005, acc: 0.9793678181053812\n",
      "Epoch[91/3000] loss: 0.0812987914776655, acc: 0.9810362948430493\n",
      "Epoch[92/3000] loss: 0.07545116184605077, acc: 0.9832460411995516\n",
      "Epoch[93/3000] loss: 0.0847036792758869, acc: 0.9828904498318386\n",
      "Epoch[94/3000] loss: 0.06546010451199227, acc: 0.986307980661435\n",
      "Epoch[95/3000] loss: 0.06033347798051989, acc: 0.9877312219730942\n",
      "Epoch[96/3000] loss: 0.055226562661640846, acc: 0.9892499299327355\n",
      "Epoch[97/3000] loss: 0.05354163517565738, acc: 0.9901502942825111\n",
      "Epoch[98/3000] loss: 0.047535452089635784, acc: 0.9917784823430493\n",
      "Epoch[99/3000] loss: 0.04393187749111866, acc: 0.9924809066704036\n",
      "Epoch[100/3000] loss: 0.03986482500963147, acc: 0.9938726177130044\n",
      "Epoch[101/3000] loss: 0.03655858702890809, acc: 0.9948885930493273\n",
      "Epoch[102/3000] loss: 0.033216517406093965, acc: 0.9959185818385651\n",
      "Epoch[103/3000] loss: 0.0305892140896899, acc: 0.9965001401345291\n",
      "Epoch[104/3000] loss: 0.02796609917730761, acc: 0.9969774733744394\n",
      "Epoch[105/3000] loss: 0.0252994098825873, acc: 0.9975169913116592\n",
      "Epoch[106/3000] loss: 0.023072115377410243, acc: 0.998002207118834\n",
      "Epoch[107/3000] loss: 0.021053270350164428, acc: 0.9983490400784754\n",
      "Epoch[108/3000] loss: 0.019444425743816614, acc: 0.9984760369955157\n",
      "Epoch[109/3000] loss: 0.01762665909476946, acc: 0.9987694436659192\n",
      "Epoch[110/3000] loss: 0.016314524410693435, acc: 0.9987992222533632\n",
      "Epoch[111/3000] loss: 0.015256146716597101, acc: 0.9989209641255605\n",
      "Epoch[112/3000] loss: 0.014488732911053327, acc: 0.9988789237668162\n",
      "Epoch[113/3000] loss: 0.01341352074181519, acc: 0.9990190582959642\n",
      "Epoch[114/3000] loss: 0.012606012190153379, acc: 0.9990146790919282\n",
      "Epoch[115/3000] loss: 0.011950821980140736, acc: 0.9990006656390134\n",
      "Epoch[116/3000] loss: 0.011326905796220818, acc: 0.9990628503363229\n",
      "Epoch[117/3000] loss: 0.010805315979142494, acc: 0.9990759879484304\n",
      "Epoch[118/3000] loss: 0.010341723113333296, acc: 0.9991022631726457\n",
      "Epoch[119/3000] loss: 0.009905417889331556, acc: 0.9990803671524664\n",
      "Epoch[120/3000] loss: 0.009568561609801022, acc: 0.999128538396861\n",
      "Epoch[121/3000] loss: 0.009194763468666161, acc: 0.9991241591928252\n",
      "Epoch[122/3000] loss: 0.00885000853431766, acc: 0.9991723304372198\n",
      "Epoch[123/3000] loss: 0.008558631019516443, acc: 0.9991591928251121\n",
      "Epoch[124/3000] loss: 0.008303309898852734, acc: 0.9991451793721973\n",
      "Epoch[125/3000] loss: 0.00803888018969697, acc: 0.9991591928251121\n",
      "Epoch[126/3000] loss: 0.00780396253985641, acc: 0.9991854680493274\n",
      "Epoch[127/3000] loss: 0.0075949680664672645, acc: 0.9991986056614349\n",
      "Epoch[128/3000] loss: 0.007393991698890991, acc: 0.9991986056614349\n",
      "Epoch[129/3000] loss: 0.007198759954477004, acc: 0.9991986056614349\n",
      "Epoch[130/3000] loss: 0.007026576776905278, acc: 0.9992029848654709\n",
      "Epoch[131/3000] loss: 0.006824652624688207, acc: 0.9992117432735426\n",
      "Epoch[132/3000] loss: 0.006706274663215094, acc: 0.9991767096412556\n",
      "Epoch[133/3000] loss: 0.006536634135657228, acc: 0.9992555353139013\n",
      "Epoch[134/3000] loss: 0.006417580440143466, acc: 0.9992599145179372\n",
      "Epoch[135/3000] loss: 0.006257155572895311, acc: 0.9992423977017937\n",
      "Epoch[136/3000] loss: 0.006149979933653645, acc: 0.9991942264573991\n",
      "Epoch[137/3000] loss: 0.006035691778689222, acc: 0.9992161224775785\n",
      "Epoch[138/3000] loss: 0.005933121571864415, acc: 0.9992205016816144\n",
      "Epoch[139/3000] loss: 0.005817323105071459, acc: 0.9992196258408071\n",
      "Epoch[140/3000] loss: 0.005728715453860705, acc: 0.9992511561098655\n",
      "Epoch[141/3000] loss: 0.005658189626201787, acc: 0.9992327634529148\n",
      "Epoch[142/3000] loss: 0.005524616577556031, acc: 0.999268672926009\n",
      "Epoch[143/3000] loss: 0.005424113213314336, acc: 0.9992861897421524\n",
      "Epoch[144/3000] loss: 0.005344049055484278, acc: 0.9992818105381166\n",
      "Epoch[145/3000] loss: 0.005272688470977137, acc: 0.9992774313340808\n",
      "Epoch[146/3000] loss: 0.005176856737707077, acc: 0.999268672926009\n",
      "Epoch[147/3000] loss: 0.005111309899134152, acc: 0.9992818105381166\n",
      "Epoch[148/3000] loss: 0.005031462116146068, acc: 0.9993168441704036\n",
      "Epoch[149/3000] loss: 0.00498498419890673, acc: 0.9992993273542601\n",
      "Epoch[150/3000] loss: 0.004921819178143207, acc: 0.9992642937219731\n",
      "Epoch[151/3000] loss: 0.0048519779788121385, acc: 0.9992599145179372\n",
      "Epoch[152/3000] loss: 0.004757777271967699, acc: 0.9992949481502242\n",
      "Epoch[153/3000] loss: 0.004744015218514632, acc: 0.9992818105381166\n",
      "Epoch[154/3000] loss: 0.004657490386454297, acc: 0.9992993273542601\n",
      "Epoch[155/3000] loss: 0.004610355545123496, acc: 0.9993080857623319\n",
      "Epoch[156/3000] loss: 0.004549398544534723, acc: 0.9993168441704036\n",
      "Epoch[157/3000] loss: 0.004495414509020345, acc: 0.9993168441704036\n",
      "Epoch[158/3000] loss: 0.0044515587565919276, acc: 0.9993256025784754\n",
      "Epoch[159/3000] loss: 0.004401715510188614, acc: 0.9993299817825112\n",
      "Epoch[160/3000] loss: 0.004362070075791713, acc: 0.9993299817825112\n",
      "Epoch[161/3000] loss: 0.004321240494758592, acc: 0.999334360986547\n",
      "Epoch[162/3000] loss: 0.004264366083450598, acc: 0.9993212233744395\n",
      "Epoch[163/3000] loss: 0.004225340217660962, acc: 0.9993256025784754\n",
      "Epoch[164/3000] loss: 0.00417993739383053, acc: 0.9993299817825112\n",
      "Epoch[165/3000] loss: 0.004153108104072603, acc: 0.9993256025784754\n",
      "Epoch[166/3000] loss: 0.0041148829176779755, acc: 0.9993431193946188\n",
      "Epoch[167/3000] loss: 0.004072341717927896, acc: 0.9993431193946188\n",
      "Epoch[168/3000] loss: 0.004013993309193704, acc: 0.9993562570067265\n",
      "Epoch[169/3000] loss: 0.003992967132422572, acc: 0.9993299817825112\n",
      "Epoch[170/3000] loss: 0.003962711244456119, acc: 0.9993431193946188\n",
      "Epoch[171/3000] loss: 0.003931566660194303, acc: 0.9993650154147982\n",
      "Epoch[172/3000] loss: 0.003911066967670309, acc: 0.9993693946188341\n",
      "Epoch[173/3000] loss: 0.00387006290930411, acc: 0.9993562570067265\n",
      "Epoch[174/3000] loss: 0.003810632914266102, acc: 0.9993781530269058\n",
      "Epoch[175/3000] loss: 0.0038120409128185144, acc: 0.9993562570067265\n",
      "Epoch[176/3000] loss: 0.003768918811314376, acc: 0.9993869114349776\n",
      "Epoch[177/3000] loss: 0.003754387008921022, acc: 0.9993474985986547\n",
      "Epoch[178/3000] loss: 0.003717667142721728, acc: 0.99937377382287\n",
      "Epoch[179/3000] loss: 0.0036883408508071418, acc: 0.9993912906390134\n",
      "Epoch[180/3000] loss: 0.0036536069894094656, acc: 0.9993781530269058\n",
      "Epoch[181/3000] loss: 0.003639188726540162, acc: 0.9993606362107623\n",
      "Epoch[182/3000] loss: 0.003612576185145119, acc: 0.9993825322309418\n",
      "Epoch[183/3000] loss: 0.0035992039483971894, acc: 0.9993606362107623\n",
      "Epoch[184/3000] loss: 0.0035600939695357926, acc: 0.99937377382287\n",
      "Epoch[185/3000] loss: 0.003543671385245189, acc: 0.9993869114349776\n",
      "Epoch[186/3000] loss: 0.003524629611520915, acc: 0.9993474985986547\n",
      "Epoch[187/3000] loss: 0.0034828708552016203, acc: 0.99937377382287\n",
      "Epoch[188/3000] loss: 0.0034758899569189835, acc: 0.9993869114349776\n",
      "Epoch[189/3000] loss: 0.0034559729491828233, acc: 0.9993693946188341\n",
      "Epoch[190/3000] loss: 0.0034302162593011346, acc: 0.9993825322309418\n",
      "Epoch[191/3000] loss: 0.003406366251535388, acc: 0.9993956698430493\n",
      "Epoch[192/3000] loss: 0.0033971877344998827, acc: 0.9993825322309418\n",
      "Epoch[193/3000] loss: 0.0033802637828288457, acc: 0.9994000490470852\n",
      "Epoch[194/3000] loss: 0.003372997629159702, acc: 0.99937377382287\n",
      "Epoch[195/3000] loss: 0.0033170671147166966, acc: 0.9993956698430493\n",
      "Epoch[196/3000] loss: 0.0033254004276729284, acc: 0.9993912906390134\n",
      "Epoch[197/3000] loss: 0.0032990135046530246, acc: 0.99937377382287\n",
      "Epoch[198/3000] loss: 0.0032688557557164453, acc: 0.9993912906390134\n",
      "Epoch[199/3000] loss: 0.0032467215300815374, acc: 0.99937377382287\n",
      "Epoch[200/3000] loss: 0.0032384624483372867, acc: 0.9993912906390134\n",
      "Epoch[201/3000] loss: 0.003225115992493234, acc: 0.9993869114349776\n",
      "Epoch[202/3000] loss: 0.0032086701362316534, acc: 0.9993912906390134\n",
      "Epoch[203/3000] loss: 0.0031960263519157394, acc: 0.9994044282511211\n",
      "Epoch[204/3000] loss: 0.003185067646364126, acc: 0.9993781530269058\n",
      "Epoch[205/3000] loss: 0.003173577910494853, acc: 0.9993825322309418\n",
      "Epoch[206/3000] loss: 0.003150794942207601, acc: 0.9994000490470852\n",
      "Epoch[207/3000] loss: 0.003139636546531024, acc: 0.9993956698430493\n",
      "Epoch[208/3000] loss: 0.0031282053639143373, acc: 0.9994131866591929\n",
      "Epoch[209/3000] loss: 0.003152945376995266, acc: 0.9994079316143497\n",
      "Epoch[210/3000] loss: 0.0031190349530134218, acc: 0.9993728979820627\n",
      "Epoch[211/3000] loss: 0.003089248633817164, acc: 0.9994254484304932\n",
      "Epoch[212/3000] loss: 0.00307884528873292, acc: 0.9993869114349776\n",
      "Epoch[213/3000] loss: 0.0030393704390644007, acc: 0.9994000490470852\n",
      "Epoch[214/3000] loss: 0.0030212892091393494, acc: 0.9994175658632287\n",
      "Epoch[215/3000] loss: 0.003030133996792597, acc: 0.9994175658632287\n",
      "Epoch[216/3000] loss: 0.003023727061414899, acc: 0.9993947940022421\n",
      "Epoch[217/3000] loss: 0.003024591908100032, acc: 0.9994044282511211\n",
      "Epoch[218/3000] loss: 0.002989697707501123, acc: 0.9994000490470852\n",
      "Epoch[219/3000] loss: 0.002985816599405475, acc: 0.9994088074551569\n",
      "Epoch[220/3000] loss: 0.002974554135805025, acc: 0.9994175658632287\n",
      "Epoch[221/3000] loss: 0.002968759388603362, acc: 0.9994166900224215\n",
      "Epoch[222/3000] loss: 0.002947819993255405, acc: 0.9994044282511211\n",
      "Epoch[223/3000] loss: 0.0029248819726147594, acc: 0.9994350826793722\n",
      "Epoch[224/3000] loss: 0.0029327011033694257, acc: 0.9994219450672646\n",
      "Epoch[225/3000] loss: 0.0029134600189140675, acc: 0.9994131866591929\n",
      "Epoch[226/3000] loss: 0.0029011823213711432, acc: 0.9993912906390134\n",
      "Epoch[227/3000] loss: 0.0028875521053988154, acc: 0.9994044282511211\n",
      "Epoch[228/3000] loss: 0.002886628962240407, acc: 0.9994044282511211\n",
      "Epoch[229/3000] loss: 0.002876548180260044, acc: 0.9994131866591929\n",
      "Epoch[230/3000] loss: 0.002847598111082073, acc: 0.999439461883408\n",
      "Epoch[231/3000] loss: 0.0028525715677648147, acc: 0.9994131866591929\n",
      "Epoch[232/3000] loss: 0.0028519250446384474, acc: 0.9994131866591929\n",
      "Epoch[233/3000] loss: 0.002832672413988791, acc: 0.9994131866591929\n",
      "Epoch[234/3000] loss: 0.0028363961080139326, acc: 0.9994219450672646\n",
      "Epoch[235/3000] loss: 0.0028139522134466304, acc: 0.9994088074551569\n",
      "Epoch[236/3000] loss: 0.002793431868296401, acc: 0.9994307034753364\n",
      "Epoch[237/3000] loss: 0.002812991101404907, acc: 0.9994131866591929\n",
      "Epoch[238/3000] loss: 0.0027867690443412105, acc: 0.9994175658632287\n",
      "Epoch[239/3000] loss: 0.00276775194374811, acc: 0.9994175658632287\n",
      "Epoch[240/3000] loss: 0.0027688963980452602, acc: 0.9994263242713004\n",
      "Epoch[241/3000] loss: 0.0027501372598407103, acc: 0.9994525994955157\n",
      "Epoch[242/3000] loss: 0.0027659289851954894, acc: 0.9994131866591929\n",
      "Epoch[243/3000] loss: 0.0027497190978971096, acc: 0.9994350826793722\n",
      "Epoch[244/3000] loss: 0.0027484216531159173, acc: 0.9994263242713004\n",
      "Epoch[245/3000] loss: 0.002729871884931188, acc: 0.9994088074551569\n",
      "Epoch[246/3000] loss: 0.0027188248864496167, acc: 0.9994307034753364\n",
      "Epoch[247/3000] loss: 0.0027241050745355332, acc: 0.9994219450672646\n",
      "Epoch[248/3000] loss: 0.0027029202206137553, acc: 0.9994263242713004\n",
      "Epoch[249/3000] loss: 0.002700768898550793, acc: 0.9994044282511211\n",
      "Epoch[250/3000] loss: 0.0026892307257507363, acc: 0.9994569786995515\n",
      "Epoch[251/3000] loss: 0.0026813815273459365, acc: 0.9994088074551569\n",
      "Epoch[252/3000] loss: 0.0027159106030991307, acc: 0.9994079316143497\n",
      "Epoch[253/3000] loss: 0.0026751674676205412, acc: 0.9994219450672646\n",
      "Epoch[254/3000] loss: 0.0026660032434370202, acc: 0.9994350826793722\n",
      "Epoch[255/3000] loss: 0.0026680249596174365, acc: 0.9994088074551569\n",
      "Epoch[256/3000] loss: 0.0026608340850939366, acc: 0.9994307034753364\n",
      "Epoch[257/3000] loss: 0.0026434823694991808, acc: 0.9994175658632287\n",
      "Epoch[258/3000] loss: 0.002629534720266739, acc: 0.999443841087444\n",
      "Epoch[259/3000] loss: 0.0026311776349545487, acc: 0.9994350826793722\n",
      "Epoch[260/3000] loss: 0.002633732759222522, acc: 0.9994263242713004\n",
      "Epoch[261/3000] loss: 0.0026202089085495266, acc: 0.9994131866591929\n",
      "Epoch[262/3000] loss: 0.0026036396478704748, acc: 0.9994482202914798\n",
      "Epoch[263/3000] loss: 0.002598193276260098, acc: 0.9994350826793722\n",
      "Epoch[264/3000] loss: 0.002598491643379012, acc: 0.999443841087444\n",
      "Epoch[265/3000] loss: 0.0025807709327816436, acc: 0.999443841087444\n",
      "Epoch[266/3000] loss: 0.0025925978229536712, acc: 0.9994219450672646\n",
      "Epoch[267/3000] loss: 0.00258649468621356, acc: 0.9994263242713004\n",
      "Epoch[268/3000] loss: 0.002579013139858838, acc: 0.9994350826793722\n",
      "Epoch[269/3000] loss: 0.002576259716903068, acc: 0.9994307034753364\n",
      "Epoch[270/3000] loss: 0.002565625839203183, acc: 0.9994307034753364\n",
      "Epoch[271/3000] loss: 0.0025585332900784795, acc: 0.9994350826793722\n",
      "Epoch[272/3000] loss: 0.002590403317592854, acc: 0.9994210692264573\n",
      "Epoch[273/3000] loss: 0.0025474469735618102, acc: 0.9994569786995515\n",
      "Epoch[274/3000] loss: 0.0025667156458382325, acc: 0.9994254484304932\n",
      "Epoch[275/3000] loss: 0.002547099857461626, acc: 0.9994307034753364\n",
      "Epoch[276/3000] loss: 0.0025442560659376945, acc: 0.9994307034753364\n",
      "Epoch[277/3000] loss: 0.0025429582147469595, acc: 0.9994131866591929\n",
      "Epoch[278/3000] loss: 0.002537641813926349, acc: 0.9994350826793722\n",
      "Epoch[279/3000] loss: 0.0025282925578991094, acc: 0.9994263242713004\n",
      "Epoch[280/3000] loss: 0.0025219642651171294, acc: 0.999443841087444\n",
      "Epoch[281/3000] loss: 0.0025001089494506258, acc: 0.999443841087444\n",
      "Epoch[282/3000] loss: 0.0025148865660587784, acc: 0.9994175658632287\n",
      "Epoch[283/3000] loss: 0.002497281418419485, acc: 0.999439461883408\n",
      "Epoch[284/3000] loss: 0.0024878768758162346, acc: 0.9994569786995515\n",
      "Epoch[285/3000] loss: 0.0024866162807912454, acc: 0.9994350826793722\n",
      "Epoch[286/3000] loss: 0.0024937549916580486, acc: 0.9994219450672646\n",
      "Epoch[287/3000] loss: 0.0025129288862665044, acc: 0.9994079316143497\n",
      "Epoch[288/3000] loss: 0.0024690986357758767, acc: 0.9994525994955157\n",
      "Epoch[289/3000] loss: 0.0024738958521903754, acc: 0.9994482202914798\n",
      "Epoch[290/3000] loss: 0.0024409007211633428, acc: 0.9994263242713004\n",
      "Epoch[291/3000] loss: 0.002476977332283204, acc: 0.9994263242713004\n",
      "Epoch[292/3000] loss: 0.0024584387295937893, acc: 0.9994088074551569\n",
      "Epoch[293/3000] loss: 0.0024616902309361295, acc: 0.9994350826793722\n",
      "Epoch[294/3000] loss: 0.0024546964582543396, acc: 0.9994263242713004\n",
      "Epoch[295/3000] loss: 0.0024558469275823866, acc: 0.9994219450672646\n",
      "Epoch[296/3000] loss: 0.0024484466356637724, acc: 0.9994350826793722\n",
      "Epoch[297/3000] loss: 0.0024405115503544246, acc: 0.999439461883408\n",
      "Epoch[298/3000] loss: 0.0024468111381458846, acc: 0.9994350826793722\n",
      "Epoch[299/3000] loss: 0.00248098994290452, acc: 0.9994166900224215\n",
      "Epoch[300/3000] loss: 0.0024339788380750315, acc: 0.999443841087444\n",
      "Epoch[301/3000] loss: 0.002436277191155964, acc: 0.9994350826793722\n",
      "Epoch[302/3000] loss: 0.002427959026971878, acc: 0.9994263242713004\n",
      "Epoch[303/3000] loss: 0.002413061325544817, acc: 0.999439461883408\n",
      "Epoch[304/3000] loss: 0.0024168476242258322, acc: 0.9994307034753364\n",
      "Epoch[305/3000] loss: 0.0024305637995033803, acc: 0.9994429652466367\n",
      "Epoch[306/3000] loss: 0.0024140543883067094, acc: 0.999439461883408\n",
      "Epoch[307/3000] loss: 0.002409104542302191, acc: 0.9994307034753364\n",
      "Epoch[308/3000] loss: 0.002402464476726728, acc: 0.9994307034753364\n",
      "Epoch[309/3000] loss: 0.0023733291974277634, acc: 0.9994613579035875\n",
      "Epoch[310/3000] loss: 0.0023994324619217773, acc: 0.999439461883408\n",
      "Epoch[311/3000] loss: 0.002379631732243078, acc: 0.9994525994955157\n",
      "Epoch[312/3000] loss: 0.0023824473077665195, acc: 0.9994482202914798\n",
      "Epoch[313/3000] loss: 0.0023889844305813313, acc: 0.9994350826793722\n",
      "Epoch[314/3000] loss: 0.0023678265879507024, acc: 0.9994525994955157\n",
      "Epoch[315/3000] loss: 0.0023818584884654713, acc: 0.9994219450672646\n",
      "Epoch[316/3000] loss: 0.002369978764041425, acc: 0.999439461883408\n",
      "Epoch[317/3000] loss: 0.0023711585759361707, acc: 0.9994263242713004\n",
      "Epoch[318/3000] loss: 0.0023637153948250023, acc: 0.9994525994955157\n",
      "Epoch[319/3000] loss: 0.0023662109679574847, acc: 0.9994263242713004\n",
      "Epoch[320/3000] loss: 0.00235642818649218, acc: 0.999439461883408\n",
      "Epoch[321/3000] loss: 0.0023560354766313765, acc: 0.9994307034753364\n",
      "Epoch[322/3000] loss: 0.002350939582841995, acc: 0.9994350826793722\n",
      "Epoch[323/3000] loss: 0.002352564651791827, acc: 0.9994219450672646\n",
      "Epoch[324/3000] loss: 0.002347400381554528, acc: 0.9994350826793722\n",
      "Epoch[325/3000] loss: 0.0023491886978699605, acc: 0.999439461883408\n",
      "Epoch[326/3000] loss: 0.0023393045831641346, acc: 0.9994350826793722\n",
      "Epoch[327/3000] loss: 0.0023333165909982215, acc: 0.9994263242713004\n",
      "Epoch[328/3000] loss: 0.0023288250340393122, acc: 0.9994482202914798\n",
      "Epoch[329/3000] loss: 0.0023177563731068946, acc: 0.9994350826793722\n",
      "Epoch[330/3000] loss: 0.002313923331422861, acc: 0.999443841087444\n",
      "Epoch[331/3000] loss: 0.002315742365787257, acc: 0.999443841087444\n",
      "Epoch[332/3000] loss: 0.0023120177532948193, acc: 0.999443841087444\n",
      "Epoch[333/3000] loss: 0.002316899126828356, acc: 0.9994350826793722\n",
      "Epoch[334/3000] loss: 0.002329914205153723, acc: 0.9994131866591929\n",
      "Epoch[335/3000] loss: 0.0023174349016903046, acc: 0.9994350826793722\n",
      "Epoch[336/3000] loss: 0.0023200318551775813, acc: 0.9994131866591929\n",
      "Epoch[337/3000] loss: 0.002302713293283071, acc: 0.9994701163116592\n",
      "Epoch[338/3000] loss: 0.0023145409833536497, acc: 0.9994219450672646\n",
      "Epoch[339/3000] loss: 0.002299437688291908, acc: 0.999439461883408\n",
      "Epoch[340/3000] loss: 0.0023011907517117543, acc: 0.9994263242713004\n",
      "Epoch[341/3000] loss: 0.002281579190927543, acc: 0.9994525994955157\n",
      "Epoch[342/3000] loss: 0.002284652394308623, acc: 0.999443841087444\n",
      "Epoch[343/3000] loss: 0.002326174194157393, acc: 0.9994079316143497\n",
      "Epoch[344/3000] loss: 0.002286076751073526, acc: 0.9994482202914798\n",
      "Epoch[345/3000] loss: 0.0023160845943363258, acc: 0.999434206838565\n",
      "Epoch[346/3000] loss: 0.002274150625681724, acc: 0.9994569786995515\n",
      "Epoch[347/3000] loss: 0.002262657547113538, acc: 0.999478874719731\n",
      "Epoch[348/3000] loss: 0.0022803748174880594, acc: 0.999443841087444\n",
      "Epoch[349/3000] loss: 0.00227549116726497, acc: 0.9994350826793722\n",
      "Epoch[350/3000] loss: 0.0022812533738392283, acc: 0.999443841087444\n",
      "Epoch[351/3000] loss: 0.0022740585071315588, acc: 0.999443841087444\n",
      "Epoch[352/3000] loss: 0.0022647628831993496, acc: 0.9994569786995515\n",
      "Epoch[353/3000] loss: 0.0022713907836020755, acc: 0.9994350826793722\n",
      "Epoch[354/3000] loss: 0.0022902888590558704, acc: 0.9994298276345291\n",
      "Epoch[355/3000] loss: 0.0022714323357150646, acc: 0.999439461883408\n",
      "Epoch[356/3000] loss: 0.0022622932354131853, acc: 0.9994482202914798\n",
      "Epoch[357/3000] loss: 0.0022653079207468727, acc: 0.9994131866591929\n",
      "Epoch[358/3000] loss: 0.002248497911322509, acc: 0.9994569786995515\n",
      "Epoch[359/3000] loss: 0.002258984266730462, acc: 0.9994613579035875\n",
      "Epoch[360/3000] loss: 0.0022602741510086306, acc: 0.9994350826793722\n",
      "Epoch[361/3000] loss: 0.002254822639896714, acc: 0.9994350826793722\n",
      "Epoch[362/3000] loss: 0.002241959885049104, acc: 0.999439461883408\n",
      "Epoch[363/3000] loss: 0.0022429213452274544, acc: 0.9994482202914798\n",
      "Epoch[364/3000] loss: 0.002235291565788258, acc: 0.9994569786995515\n",
      "Epoch[365/3000] loss: 0.00223988310774948, acc: 0.999443841087444\n",
      "Epoch[366/3000] loss: 0.0022386586060747504, acc: 0.9994482202914798\n",
      "Epoch[367/3000] loss: 0.0022427311924473076, acc: 0.999439461883408\n",
      "Epoch[368/3000] loss: 0.002229536758014575, acc: 0.999443841087444\n",
      "Epoch[369/3000] loss: 0.0022240208256065703, acc: 0.9994350826793722\n",
      "Epoch[370/3000] loss: 0.0022259278609490464, acc: 0.9994525994955157\n",
      "Epoch[371/3000] loss: 0.0022291331013287166, acc: 0.999443841087444\n",
      "Epoch[372/3000] loss: 0.002215323191716371, acc: 0.9994525994955157\n",
      "Epoch[373/3000] loss: 0.002242633991754946, acc: 0.9994517236547085\n",
      "Epoch[374/3000] loss: 0.0022357126231714584, acc: 0.9994307034753364\n",
      "Epoch[375/3000] loss: 0.002218766075443067, acc: 0.9994525994955157\n",
      "Epoch[376/3000] loss: 0.0022257236311159457, acc: 0.9994307034753364\n",
      "Epoch[377/3000] loss: 0.0021989683342416044, acc: 0.999439461883408\n",
      "Epoch[378/3000] loss: 0.0022196082892016436, acc: 0.999443841087444\n",
      "Epoch[379/3000] loss: 0.0022466281967158243, acc: 0.9994254484304932\n",
      "Epoch[380/3000] loss: 0.0022225738906642453, acc: 0.9994307034753364\n",
      "Epoch[381/3000] loss: 0.0021946430604681978, acc: 0.9994482202914798\n",
      "Epoch[382/3000] loss: 0.0022096328889612306, acc: 0.9994525994955157\n",
      "Epoch[383/3000] loss: 0.0021989878933370895, acc: 0.999443841087444\n",
      "Epoch[384/3000] loss: 0.0022435909373810484, acc: 0.9994429652466367\n",
      "Epoch[385/3000] loss: 0.0022172086078722153, acc: 0.9994525994955157\n",
      "Epoch[386/3000] loss: 0.0022079568943115797, acc: 0.999439461883408\n",
      "Epoch[387/3000] loss: 0.002186232807947116, acc: 0.999478874719731\n",
      "Epoch[388/3000] loss: 0.0021934357173286688, acc: 0.999439461883408\n",
      "Epoch[389/3000] loss: 0.002187164032541011, acc: 0.999439461883408\n",
      "Epoch[390/3000] loss: 0.0021889709627463047, acc: 0.9994569786995515\n",
      "Epoch[391/3000] loss: 0.0021885715021019397, acc: 0.9994569786995515\n",
      "Epoch[392/3000] loss: 0.0021902752187373973, acc: 0.9994482202914798\n",
      "Epoch[393/3000] loss: 0.0021822564002069553, acc: 0.9994525994955157\n",
      "Epoch[394/3000] loss: 0.002188969884044804, acc: 0.999443841087444\n",
      "Epoch[395/3000] loss: 0.0021909306917157895, acc: 0.999443841087444\n",
      "Epoch[396/3000] loss: 0.002175628522610931, acc: 0.9994307034753364\n",
      "Epoch[397/3000] loss: 0.0021732065025255013, acc: 0.9994525994955157\n",
      "Epoch[398/3000] loss: 0.0021796775785039363, acc: 0.9994525994955157\n",
      "Epoch[399/3000] loss: 0.0021785362776981735, acc: 0.9994482202914798\n",
      "Epoch[400/3000] loss: 0.002184231511986451, acc: 0.9994263242713004\n",
      "Epoch[401/3000] loss: 0.0021505051271205836, acc: 0.9994701163116592\n",
      "Epoch[402/3000] loss: 0.0021626017074154965, acc: 0.9994569786995515\n",
      "Epoch[403/3000] loss: 0.00216710488319777, acc: 0.9994482202914798\n",
      "Epoch[404/3000] loss: 0.002164463876984646, acc: 0.9994613579035875\n",
      "Epoch[405/3000] loss: 0.0021564081464118885, acc: 0.999443841087444\n",
      "Epoch[406/3000] loss: 0.0021644567855035536, acc: 0.9994657371076233\n",
      "Epoch[407/3000] loss: 0.0021507924693365656, acc: 0.9994525994955157\n",
      "Epoch[408/3000] loss: 0.002163593580705143, acc: 0.9994482202914798\n",
      "Epoch[409/3000] loss: 0.0021626276927639945, acc: 0.999443841087444\n",
      "Epoch[410/3000] loss: 0.0021562728153947833, acc: 0.9994613579035875\n",
      "Epoch[411/3000] loss: 0.0021521722373692006, acc: 0.9994613579035875\n",
      "Epoch[412/3000] loss: 0.0021535893195142215, acc: 0.9994525994955157\n",
      "Epoch[413/3000] loss: 0.0021537158124941306, acc: 0.9994525994955157\n",
      "Epoch[414/3000] loss: 0.0021783620587757877, acc: 0.9994385860426008\n",
      "Epoch[415/3000] loss: 0.00216377790175632, acc: 0.9994482202914798\n",
      "Epoch[416/3000] loss: 0.002146134837013732, acc: 0.9994525994955157\n",
      "Epoch[417/3000] loss: 0.0021421265366305536, acc: 0.9994525994955157\n",
      "Epoch[418/3000] loss: 0.0021432287906998177, acc: 0.9994657371076233\n",
      "Epoch[419/3000] loss: 0.002139605340577094, acc: 0.9994701163116592\n",
      "Epoch[420/3000] loss: 0.002136571265436289, acc: 0.9994482202914798\n",
      "Epoch[421/3000] loss: 0.002144631167645086, acc: 0.999443841087444\n",
      "Epoch[422/3000] loss: 0.0021338536260642254, acc: 0.9994482202914798\n",
      "Epoch[423/3000] loss: 0.0021872200563053013, acc: 0.9994517236547085\n",
      "Epoch[424/3000] loss: 0.0021218640938340666, acc: 0.999478874719731\n",
      "Epoch[425/3000] loss: 0.00216170264646957, acc: 0.9994657371076233\n",
      "Epoch[426/3000] loss: 0.0021528306811896044, acc: 0.9994701163116592\n",
      "Epoch[427/3000] loss: 0.0021314179799699585, acc: 0.9994657371076233\n",
      "Epoch[428/3000] loss: 0.0021348000322210167, acc: 0.9994569786995515\n",
      "Epoch[429/3000] loss: 0.00212853671103244, acc: 0.9994569786995515\n",
      "Epoch[430/3000] loss: 0.002128904719212909, acc: 0.9994613579035875\n",
      "Epoch[431/3000] loss: 0.0021313339139145877, acc: 0.999443841087444\n",
      "Epoch[432/3000] loss: 0.002122355709734661, acc: 0.9994525994955157\n",
      "Epoch[433/3000] loss: 0.0021230916984926393, acc: 0.9994832539237668\n",
      "Epoch[434/3000] loss: 0.002120491429485251, acc: 0.999443841087444\n",
      "Epoch[435/3000] loss: 0.002116786375680454, acc: 0.9994350826793722\n",
      "Epoch[436/3000] loss: 0.0021071713728709625, acc: 0.999443841087444\n",
      "Epoch[437/3000] loss: 0.002118066526065067, acc: 0.9994482202914798\n",
      "Epoch[438/3000] loss: 0.0021235726915929997, acc: 0.999443841087444\n",
      "Epoch[439/3000] loss: 0.002108643201719202, acc: 0.9994613579035875\n",
      "Epoch[440/3000] loss: 0.0021020592097793353, acc: 0.9994876331278026\n",
      "Epoch[441/3000] loss: 0.002110785785210246, acc: 0.9994657371076233\n",
      "Epoch[442/3000] loss: 0.0021138984935500163, acc: 0.9994525994955157\n",
      "Epoch[443/3000] loss: 0.002106625241352216, acc: 0.9994569786995515\n",
      "Epoch[444/3000] loss: 0.002128788713465503, acc: 0.9994561028587443\n",
      "Epoch[445/3000] loss: 0.002127125261457441, acc: 0.9994525994955157\n",
      "Epoch[446/3000] loss: 0.00210462029325236, acc: 0.9994482202914798\n",
      "Epoch[447/3000] loss: 0.002131330470983649, acc: 0.999434206838565\n",
      "Epoch[448/3000] loss: 0.002105786649552289, acc: 0.999439461883408\n",
      "Epoch[449/3000] loss: 0.0021037932301257037, acc: 0.9994744955156951\n",
      "Epoch[450/3000] loss: 0.002096373172203599, acc: 0.9994482202914798\n",
      "Epoch[451/3000] loss: 0.0020981766378784746, acc: 0.9994613579035875\n",
      "Epoch[452/3000] loss: 0.0021026310911730992, acc: 0.9994482202914798\n",
      "Epoch[453/3000] loss: 0.0021296798207130463, acc: 0.9994561028587443\n",
      "Epoch[454/3000] loss: 0.0021032847944460966, acc: 0.999439461883408\n",
      "Epoch[455/3000] loss: 0.0020849450486539883, acc: 0.9994657371076233\n",
      "Epoch[456/3000] loss: 0.0020930034021442107, acc: 0.9994657371076233\n",
      "Epoch[457/3000] loss: 0.0020975717963640905, acc: 0.9994701163116592\n",
      "Epoch[458/3000] loss: 0.002115792588585562, acc: 0.9994604820627803\n",
      "Epoch[459/3000] loss: 0.0020795594018595837, acc: 0.9994482202914798\n",
      "Epoch[460/3000] loss: 0.0021202515747691525, acc: 0.9994473444506726\n",
      "Epoch[461/3000] loss: 0.002099240500830488, acc: 0.999443841087444\n",
      "Epoch[462/3000] loss: 0.0020843362088866987, acc: 0.9994569786995515\n",
      "Epoch[463/3000] loss: 0.00207097300349457, acc: 0.9994832539237668\n",
      "Epoch[464/3000] loss: 0.002079774575842275, acc: 0.9994701163116592\n",
      "Epoch[465/3000] loss: 0.0020694895010575183, acc: 0.9994569786995515\n",
      "Epoch[466/3000] loss: 0.002082799409766475, acc: 0.9994701163116592\n",
      "Epoch[467/3000] loss: 0.0020834062978315466, acc: 0.9994569786995515\n",
      "Epoch[468/3000] loss: 0.002086896072054384, acc: 0.9994525994955157\n",
      "Epoch[469/3000] loss: 0.0020847832401661042, acc: 0.9994569786995515\n",
      "Epoch[470/3000] loss: 0.002080632998835663, acc: 0.9994525994955157\n",
      "Epoch[471/3000] loss: 0.0020734271241587927, acc: 0.9994569786995515\n",
      "Epoch[472/3000] loss: 0.0020819808592165783, acc: 0.9994350826793722\n",
      "Epoch[473/3000] loss: 0.002082800058892828, acc: 0.999443841087444\n",
      "Epoch[474/3000] loss: 0.0020668512782081438, acc: 0.9994613579035875\n",
      "Epoch[475/3000] loss: 0.002079401575800213, acc: 0.9994307034753364\n",
      "Epoch[476/3000] loss: 0.0020675281182596056, acc: 0.9994569786995515\n",
      "Epoch[477/3000] loss: 0.002072442293523016, acc: 0.999443841087444\n",
      "Epoch[478/3000] loss: 0.0020702435205520375, acc: 0.999439461883408\n",
      "Epoch[479/3000] loss: 0.002067637762258789, acc: 0.9994482202914798\n",
      "Epoch[480/3000] loss: 0.002073827625379293, acc: 0.9994604820627803\n",
      "Epoch[481/3000] loss: 0.0020776102192438368, acc: 0.9994482202914798\n",
      "Epoch[482/3000] loss: 0.0020694775248312853, acc: 0.9994657371076233\n",
      "Epoch[483/3000] loss: 0.002065736423289631, acc: 0.9994482202914798\n",
      "Epoch[484/3000] loss: 0.002080546089169656, acc: 0.9994648612668161\n",
      "Epoch[485/3000] loss: 0.0020689066800983432, acc: 0.9994482202914798\n",
      "Epoch[486/3000] loss: 0.002052536233117639, acc: 0.9994657371076233\n",
      "Epoch[487/3000] loss: 0.002105501597550005, acc: 0.9994473444506726\n",
      "Epoch[488/3000] loss: 0.0020599693118553085, acc: 0.9994657371076233\n",
      "Epoch[489/3000] loss: 0.0020617888005428965, acc: 0.999443841087444\n",
      "Epoch[490/3000] loss: 0.002055389020058056, acc: 0.9994744955156951\n",
      "Epoch[491/3000] loss: 0.0020522097486641356, acc: 0.9994744955156951\n",
      "Epoch[492/3000] loss: 0.0020483726791502657, acc: 0.9994569786995515\n",
      "Epoch[493/3000] loss: 0.0020469916364383744, acc: 0.9994525994955157\n",
      "Epoch[494/3000] loss: 0.002049094200482218, acc: 0.9994525994955157\n",
      "Epoch[495/3000] loss: 0.0020416209585400876, acc: 0.9994613579035875\n",
      "Epoch[496/3000] loss: 0.0020519981347618265, acc: 0.9994569786995515\n",
      "Epoch[497/3000] loss: 0.0020452317644188375, acc: 0.9994525994955157\n",
      "Epoch[498/3000] loss: 0.0020491012647029403, acc: 0.999478874719731\n",
      "Epoch[499/3000] loss: 0.002052173539681617, acc: 0.9994482202914798\n",
      "Epoch[500/3000] loss: 0.0020466456656326485, acc: 0.9994744955156951\n",
      "Epoch[501/3000] loss: 0.0020352917303733953, acc: 0.9994525994955157\n",
      "Epoch[502/3000] loss: 0.0020498383898480384, acc: 0.9994482202914798\n",
      "Epoch[503/3000] loss: 0.0020372339907637903, acc: 0.9994701163116592\n",
      "Epoch[504/3000] loss: 0.002054280727645466, acc: 0.9994701163116592\n",
      "Epoch[505/3000] loss: 0.0020359360689342736, acc: 0.999443841087444\n",
      "Epoch[506/3000] loss: 0.0020330935169333452, acc: 0.9994701163116592\n",
      "Epoch[507/3000] loss: 0.0020367330837915984, acc: 0.9994744955156951\n",
      "Epoch[508/3000] loss: 0.002039499345027754, acc: 0.9994307034753364\n",
      "Epoch[509/3000] loss: 0.002031067609001967, acc: 0.999443841087444\n",
      "Epoch[510/3000] loss: 0.002040220849148641, acc: 0.9994569786995515\n",
      "Epoch[511/3000] loss: 0.002041188732790343, acc: 0.9994613579035875\n",
      "Epoch[512/3000] loss: 0.0020358925007774144, acc: 0.9994350826793722\n",
      "Epoch[513/3000] loss: 0.0020361354622458454, acc: 0.999443841087444\n",
      "Epoch[514/3000] loss: 0.0020354554591757337, acc: 0.9994569786995515\n",
      "Epoch[515/3000] loss: 0.0020257170951477306, acc: 0.9994613579035875\n",
      "Epoch[516/3000] loss: 0.0020278881771780754, acc: 0.9994569786995515\n",
      "Epoch[517/3000] loss: 0.0020522445384270746, acc: 0.9994429652466367\n",
      "Epoch[518/3000] loss: 0.00202428193619598, acc: 0.9994744955156951\n",
      "Epoch[519/3000] loss: 0.0020248999360676294, acc: 0.9994569786995515\n",
      "Epoch[520/3000] loss: 0.002033586686446656, acc: 0.9994525994955157\n",
      "Epoch[521/3000] loss: 0.002031844501594899, acc: 0.9994525994955157\n",
      "Epoch[522/3000] loss: 0.002034144912939832, acc: 0.9994482202914798\n",
      "Epoch[523/3000] loss: 0.0020228726762926913, acc: 0.9994569786995515\n",
      "Epoch[524/3000] loss: 0.0020587398307664205, acc: 0.9994736196748878\n",
      "Epoch[525/3000] loss: 0.002026997233884286, acc: 0.9994569786995515\n",
      "Epoch[526/3000] loss: 0.0020231348728481, acc: 0.999443841087444\n",
      "Epoch[527/3000] loss: 0.0020233058482595156, acc: 0.9994613579035875\n",
      "Epoch[528/3000] loss: 0.002058407989446222, acc: 0.999434206838565\n",
      "Epoch[529/3000] loss: 0.0020497150174219416, acc: 0.9994166900224215\n",
      "Epoch[530/3000] loss: 0.0020250830309894385, acc: 0.9994350826793722\n",
      "Epoch[531/3000] loss: 0.0020520449710635033, acc: 0.9994604820627803\n",
      "Epoch[532/3000] loss: 0.002021069218399501, acc: 0.9994569786995515\n",
      "Epoch[533/3000] loss: 0.002020616908461788, acc: 0.9994525994955157\n",
      "Epoch[534/3000] loss: 0.0020180679270760116, acc: 0.9994744955156951\n",
      "Epoch[535/3000] loss: 0.002013327882575177, acc: 0.9994525994955157\n",
      "Epoch[536/3000] loss: 0.00200918022323086, acc: 0.9994832539237668\n",
      "Epoch[537/3000] loss: 0.002014744528847921, acc: 0.999439461883408\n",
      "Epoch[538/3000] loss: 0.0020163967786382907, acc: 0.9994701163116592\n",
      "Epoch[539/3000] loss: 0.002002619751150257, acc: 0.9994657371076233\n",
      "Epoch[540/3000] loss: 0.0020146454902258263, acc: 0.9994569786995515\n",
      "Epoch[541/3000] loss: 0.0020095174606687715, acc: 0.9994657371076233\n",
      "Epoch[542/3000] loss: 0.002009027036605925, acc: 0.9994701163116592\n",
      "Epoch[543/3000] loss: 0.0020111875275500755, acc: 0.9994613579035875\n",
      "Epoch[544/3000] loss: 0.0020099386502101164, acc: 0.9994657371076233\n",
      "Epoch[545/3000] loss: 0.0020134104073673826, acc: 0.999443841087444\n",
      "Epoch[546/3000] loss: 0.0020013321487331185, acc: 0.999443841087444\n",
      "Epoch[547/3000] loss: 0.002006944572672325, acc: 0.9994613579035875\n",
      "Epoch[548/3000] loss: 0.0020060576670129695, acc: 0.9994613579035875\n",
      "Epoch[549/3000] loss: 0.001999434284245801, acc: 0.9994613579035875\n",
      "Epoch[550/3000] loss: 0.002002999605960082, acc: 0.9994613579035875\n",
      "Epoch[551/3000] loss: 0.0020307647392888255, acc: 0.9994473444506726\n",
      "Epoch[552/3000] loss: 0.002010677657544304, acc: 0.9994525994955157\n",
      "Epoch[553/3000] loss: 0.0020084466016877466, acc: 0.9994569786995515\n",
      "Epoch[554/3000] loss: 0.0020467039375699587, acc: 0.9994561028587443\n",
      "Epoch[555/3000] loss: 0.002003760329743815, acc: 0.9994569786995515\n",
      "Epoch[556/3000] loss: 0.0019873019676935346, acc: 0.9994657371076233\n",
      "Epoch[557/3000] loss: 0.0019882047283333974, acc: 0.9994744955156951\n",
      "Epoch[558/3000] loss: 0.0019978920362653282, acc: 0.9994613579035875\n",
      "Epoch[559/3000] loss: 0.002024890865558694, acc: 0.9994561028587443\n",
      "Epoch[560/3000] loss: 0.0020023340168393387, acc: 0.9994482202914798\n",
      "Epoch[561/3000] loss: 0.002019371956827238, acc: 0.9994517236547085\n",
      "Epoch[562/3000] loss: 0.0019998641201371178, acc: 0.9994482202914798\n",
      "Epoch[563/3000] loss: 0.001996802783353492, acc: 0.9994569786995515\n",
      "Epoch[564/3000] loss: 0.001996761429942664, acc: 0.9994525994955157\n",
      "Epoch[565/3000] loss: 0.00199285977315604, acc: 0.9994657371076233\n",
      "Epoch[566/3000] loss: 0.001992018212798034, acc: 0.9994613579035875\n",
      "Epoch[567/3000] loss: 0.001997890362191466, acc: 0.9994657371076233\n",
      "Epoch[568/3000] loss: 0.0019943061644897113, acc: 0.9994482202914798\n",
      "Epoch[569/3000] loss: 0.001992487616815099, acc: 0.9994569786995515\n",
      "Epoch[570/3000] loss: 0.0019912056659572527, acc: 0.9994657371076233\n",
      "Epoch[571/3000] loss: 0.0019797805330546148, acc: 0.9994569786995515\n",
      "Epoch[572/3000] loss: 0.0019883639419472497, acc: 0.9994613579035875\n",
      "Epoch[573/3000] loss: 0.0019921946922756184, acc: 0.9994569786995515\n",
      "Epoch[574/3000] loss: 0.0019786502964191156, acc: 0.9994657371076233\n",
      "Epoch[575/3000] loss: 0.0019872782868659436, acc: 0.9994613579035875\n",
      "Epoch[576/3000] loss: 0.0019773723205670007, acc: 0.9994263242713004\n",
      "Epoch[577/3000] loss: 0.0019882349520250883, acc: 0.9994701163116592\n",
      "Epoch[578/3000] loss: 0.0019821413596121033, acc: 0.9994657371076233\n",
      "Epoch[579/3000] loss: 0.001976640288004936, acc: 0.9994701163116592\n",
      "Epoch[580/3000] loss: 0.0019938303266500276, acc: 0.9994736196748878\n",
      "Epoch[581/3000] loss: 0.0019827806983502517, acc: 0.9994569786995515\n",
      "Epoch[582/3000] loss: 0.0019721566177109797, acc: 0.9994744955156951\n",
      "Epoch[583/3000] loss: 0.0019851715862607444, acc: 0.9994744955156951\n",
      "Epoch[584/3000] loss: 0.0019731549338447797, acc: 0.9994569786995515\n",
      "Epoch[585/3000] loss: 0.0019737716918931795, acc: 0.9994701163116592\n",
      "Epoch[586/3000] loss: 0.0019848751239581693, acc: 0.9994569786995515\n",
      "Epoch[587/3000] loss: 0.0019665837868413317, acc: 0.9994832539237668\n",
      "Epoch[588/3000] loss: 0.0019682607825713926, acc: 0.9994701163116592\n",
      "Epoch[589/3000] loss: 0.001979249195596924, acc: 0.9994482202914798\n",
      "Epoch[590/3000] loss: 0.0019698775298812535, acc: 0.9994701163116592\n",
      "Epoch[591/3000] loss: 0.001978066648806747, acc: 0.999478874719731\n",
      "Epoch[592/3000] loss: 0.0019750206576539683, acc: 0.9994744955156951\n",
      "Epoch[593/3000] loss: 0.0019741587209114084, acc: 0.9994613579035875\n",
      "Epoch[594/3000] loss: 0.001976828422571476, acc: 0.9994701163116592\n",
      "Epoch[595/3000] loss: 0.001981768627361172, acc: 0.999439461883408\n",
      "Epoch[596/3000] loss: 0.001972055781454254, acc: 0.9994482202914798\n",
      "Epoch[597/3000] loss: 0.0019695599131887014, acc: 0.9994613579035875\n",
      "Epoch[598/3000] loss: 0.001971261620354753, acc: 0.9994657371076233\n",
      "Epoch[599/3000] loss: 0.0019616964423731166, acc: 0.9994569786995515\n",
      "Epoch[600/3000] loss: 0.0019705431296222846, acc: 0.9994525994955157\n",
      "Epoch[601/3000] loss: 0.0019693200700880327, acc: 0.9994701163116592\n",
      "Epoch[602/3000] loss: 0.001971565799413989, acc: 0.9994657371076233\n",
      "Epoch[603/3000] loss: 0.001968088681996756, acc: 0.9994701163116592\n",
      "Epoch[604/3000] loss: 0.0019672545679614434, acc: 0.9994525994955157\n",
      "Epoch[605/3000] loss: 0.001990203973681237, acc: 0.999469240470852\n",
      "Epoch[606/3000] loss: 0.001970906212860395, acc: 0.9994569786995515\n",
      "Epoch[607/3000] loss: 0.0019529679594014618, acc: 0.9994613579035875\n",
      "Epoch[608/3000] loss: 0.001973223325236144, acc: 0.9994569786995515\n",
      "Epoch[609/3000] loss: 0.0019931618656957627, acc: 0.9994517236547085\n",
      "Epoch[610/3000] loss: 0.0019702319262458966, acc: 0.999443841087444\n",
      "Epoch[611/3000] loss: 0.001956789184440916, acc: 0.999478874719731\n",
      "Epoch[612/3000] loss: 0.001980124720190328, acc: 0.9994604820627803\n",
      "Epoch[613/3000] loss: 0.001963312406669216, acc: 0.999478874719731\n",
      "Epoch[614/3000] loss: 0.0019567706215806407, acc: 0.9994744955156951\n",
      "Epoch[615/3000] loss: 0.001982618761701105, acc: 0.9994473444506726\n",
      "Epoch[616/3000] loss: 0.0019531329126854743, acc: 0.9994569786995515\n",
      "Epoch[617/3000] loss: 0.0019590064694383494, acc: 0.9994657371076233\n",
      "Epoch[618/3000] loss: 0.001961434161573212, acc: 0.9994482202914798\n",
      "Epoch[619/3000] loss: 0.0019635581600498313, acc: 0.999443841087444\n",
      "Epoch[620/3000] loss: 0.0019527867094406362, acc: 0.9994613579035875\n",
      "Epoch[621/3000] loss: 0.0019593781437552554, acc: 0.9994657371076233\n",
      "Epoch[622/3000] loss: 0.0019566608121594615, acc: 0.9994569786995515\n",
      "Epoch[623/3000] loss: 0.0019571166170202145, acc: 0.999443841087444\n",
      "Epoch[624/3000] loss: 0.0019511915225713908, acc: 0.9994569786995515\n",
      "Epoch[625/3000] loss: 0.0019580412355503234, acc: 0.9994525994955157\n",
      "Epoch[626/3000] loss: 0.0019600611505396987, acc: 0.999439461883408\n",
      "Epoch[627/3000] loss: 0.0019463984807620437, acc: 0.9994744955156951\n",
      "Epoch[628/3000] loss: 0.001958409354142634, acc: 0.999443841087444\n",
      "Epoch[629/3000] loss: 0.001954169852701914, acc: 0.9994701163116592\n",
      "Epoch[630/3000] loss: 0.0019501134008143072, acc: 0.9994569786995515\n",
      "Epoch[631/3000] loss: 0.0019387373404936017, acc: 0.9994744955156951\n",
      "Epoch[632/3000] loss: 0.0019574229481641513, acc: 0.9994569786995515\n",
      "Epoch[633/3000] loss: 0.0019482337858643009, acc: 0.9994482202914798\n",
      "Epoch[634/3000] loss: 0.001958019244294711, acc: 0.9994569786995515\n",
      "Epoch[635/3000] loss: 0.0019421033670891112, acc: 0.9994657371076233\n",
      "Epoch[636/3000] loss: 0.0019512613895628498, acc: 0.9994569786995515\n",
      "Epoch[637/3000] loss: 0.0019516835003937783, acc: 0.9994525994955157\n",
      "Epoch[638/3000] loss: 0.0019469207089328816, acc: 0.9994482202914798\n",
      "Epoch[639/3000] loss: 0.001948702525175328, acc: 0.9994525994955157\n",
      "Epoch[640/3000] loss: 0.001950392686212336, acc: 0.9994569786995515\n",
      "Epoch[641/3000] loss: 0.001941453222281429, acc: 0.9994613579035875\n",
      "Epoch[642/3000] loss: 0.0019475721837888783, acc: 0.9994613579035875\n",
      "Epoch[643/3000] loss: 0.0019478992747570725, acc: 0.9994482202914798\n",
      "Epoch[644/3000] loss: 0.0019742852724578823, acc: 0.9994517236547085\n",
      "Epoch[645/3000] loss: 0.00194910988617193, acc: 0.9994569786995515\n",
      "Epoch[646/3000] loss: 0.0019479761961809696, acc: 0.9994657371076233\n",
      "Epoch[647/3000] loss: 0.0019384382250603924, acc: 0.9994744955156951\n",
      "Epoch[648/3000] loss: 0.001946044957691766, acc: 0.9994657371076233\n",
      "Epoch[649/3000] loss: 0.0019514410023679571, acc: 0.999439461883408\n",
      "Epoch[650/3000] loss: 0.00194014058548019, acc: 0.9994701163116592\n",
      "Epoch[651/3000] loss: 0.0019439273663646919, acc: 0.999443841087444\n",
      "Epoch[652/3000] loss: 0.0019393742205943147, acc: 0.9994569786995515\n",
      "Epoch[653/3000] loss: 0.001955783875181962, acc: 0.9994657371076233\n",
      "Epoch[654/3000] loss: 0.001950649529946874, acc: 0.9994657371076233\n",
      "Epoch[655/3000] loss: 0.00194049787328755, acc: 0.9994569786995515\n",
      "Epoch[656/3000] loss: 0.0019361201525682002, acc: 0.9994613579035875\n",
      "Epoch[657/3000] loss: 0.0019401430388972071, acc: 0.9994569786995515\n",
      "Epoch[658/3000] loss: 0.001930210281592442, acc: 0.9994832539237668\n",
      "Epoch[659/3000] loss: 0.0019390798640452301, acc: 0.999443841087444\n",
      "Epoch[660/3000] loss: 0.0019402511930380292, acc: 0.9994350826793722\n",
      "Epoch[661/3000] loss: 0.001934278562588376, acc: 0.9994569786995515\n",
      "Epoch[662/3000] loss: 0.0019379208815368122, acc: 0.9994525994955157\n",
      "Epoch[663/3000] loss: 0.0019356633010462621, acc: 0.9994482202914798\n",
      "Epoch[664/3000] loss: 0.001938252331847701, acc: 0.9994613579035875\n",
      "Epoch[665/3000] loss: 0.0019401932297416205, acc: 0.999443841087444\n",
      "Epoch[666/3000] loss: 0.0019303103805630051, acc: 0.9994613579035875\n",
      "Epoch[667/3000] loss: 0.0019304505400915605, acc: 0.9994482202914798\n",
      "Epoch[668/3000] loss: 0.0019315797581419352, acc: 0.9994701163116592\n",
      "Epoch[669/3000] loss: 0.001935808532129354, acc: 0.9994657371076233\n",
      "Epoch[670/3000] loss: 0.0019304793460156002, acc: 0.9994701163116592\n",
      "Epoch[671/3000] loss: 0.001932242675730071, acc: 0.9994701163116592\n",
      "Epoch[672/3000] loss: 0.0019281974438911476, acc: 0.999478874719731\n",
      "Epoch[673/3000] loss: 0.0019269743283352849, acc: 0.999478874719731\n",
      "Epoch[674/3000] loss: 0.0019480929792323046, acc: 0.9994517236547085\n",
      "Epoch[675/3000] loss: 0.0019302856752446919, acc: 0.9994657371076233\n",
      "Epoch[676/3000] loss: 0.001953686428387626, acc: 0.9994648612668161\n",
      "Epoch[677/3000] loss: 0.001925976387663737, acc: 0.9994307034753364\n",
      "Epoch[678/3000] loss: 0.0019302674616140682, acc: 0.9994482202914798\n",
      "Epoch[679/3000] loss: 0.0019278802551749902, acc: 0.9994569786995515\n",
      "Epoch[680/3000] loss: 0.0019276226495530701, acc: 0.9994613579035875\n",
      "Epoch[681/3000] loss: 0.0019218125475848286, acc: 0.9994701163116592\n",
      "Epoch[682/3000] loss: 0.0019183047065556241, acc: 0.9994744955156951\n",
      "Epoch[683/3000] loss: 0.0019149733489682594, acc: 0.9994744955156951\n",
      "Epoch[684/3000] loss: 0.0019329017070595816, acc: 0.9994525994955157\n",
      "Epoch[685/3000] loss: 0.0019190257844783213, acc: 0.9994701163116592\n",
      "Epoch[686/3000] loss: 0.0019210727128633268, acc: 0.9994701163116592\n",
      "Epoch[687/3000] loss: 0.0019203567829090347, acc: 0.9994701163116592\n",
      "Epoch[688/3000] loss: 0.0019248622862636238, acc: 0.9994569786995515\n",
      "Epoch[689/3000] loss: 0.001925285081801317, acc: 0.9994832539237668\n",
      "Epoch[690/3000] loss: 0.0019192096154627572, acc: 0.9994613579035875\n",
      "Epoch[691/3000] loss: 0.0019228822118731899, acc: 0.9994701163116592\n",
      "Epoch[692/3000] loss: 0.0019186291377860927, acc: 0.9994701163116592\n",
      "Epoch[693/3000] loss: 0.0019192898067117944, acc: 0.9994525994955157\n",
      "Epoch[694/3000] loss: 0.0019245467309999507, acc: 0.9994482202914798\n",
      "Epoch[695/3000] loss: 0.0019228403274969077, acc: 0.9994657371076233\n",
      "Epoch[696/3000] loss: 0.001922589518728738, acc: 0.9994613579035875\n",
      "Epoch[697/3000] loss: 0.0019109027711189071, acc: 0.9994657371076233\n",
      "Epoch[698/3000] loss: 0.001928267573779595, acc: 0.9994525994955157\n",
      "Epoch[699/3000] loss: 0.00191084719735465, acc: 0.9994569786995515\n",
      "Epoch[700/3000] loss: 0.0019385858039612331, acc: 0.9994604820627803\n",
      "Epoch[701/3000] loss: 0.0019285638920475746, acc: 0.9994613579035875\n",
      "Epoch[702/3000] loss: 0.0019140425389571243, acc: 0.9994525994955157\n",
      "Epoch[703/3000] loss: 0.0019101680576764236, acc: 0.9994569786995515\n",
      "Epoch[704/3000] loss: 0.0019089515327841134, acc: 0.9994613579035875\n",
      "Epoch[705/3000] loss: 0.0019480135545853396, acc: 0.9994517236547085\n",
      "Epoch[706/3000] loss: 0.001919880770886841, acc: 0.999439461883408\n",
      "Epoch[707/3000] loss: 0.0019093586129059121, acc: 0.9994525994955157\n",
      "Epoch[708/3000] loss: 0.001916063622634956, acc: 0.9994613579035875\n",
      "Epoch[709/3000] loss: 0.0019538742001514284, acc: 0.9994517236547085\n",
      "Epoch[710/3000] loss: 0.0019104867745476024, acc: 0.9994832539237668\n",
      "Epoch[711/3000] loss: 0.0019136787260187718, acc: 0.9994657371076233\n",
      "Epoch[712/3000] loss: 0.001911017839931789, acc: 0.9994482202914798\n",
      "Epoch[713/3000] loss: 0.0019103681696111632, acc: 0.9994613579035875\n",
      "Epoch[714/3000] loss: 0.0019123495496607714, acc: 0.9994657371076233\n",
      "Epoch[715/3000] loss: 0.0019123928081652727, acc: 0.9994657371076233\n",
      "Epoch[716/3000] loss: 0.001909324641998167, acc: 0.9994657371076233\n",
      "Epoch[717/3000] loss: 0.0019112119771062628, acc: 0.9994569786995515\n",
      "Epoch[718/3000] loss: 0.0019052654016569034, acc: 0.9994701163116592\n",
      "Epoch[719/3000] loss: 0.0019168125496443252, acc: 0.9994613579035875\n",
      "Epoch[720/3000] loss: 0.0019051022454528524, acc: 0.9994657371076233\n",
      "Epoch[721/3000] loss: 0.0019072114793474643, acc: 0.9994525994955157\n",
      "Epoch[722/3000] loss: 0.0019123020944785381, acc: 0.9994701163116592\n",
      "Epoch[723/3000] loss: 0.001909476074823669, acc: 0.9994525994955157\n",
      "Epoch[724/3000] loss: 0.001908251977307177, acc: 0.9994482202914798\n",
      "Epoch[725/3000] loss: 0.0019059681733925968, acc: 0.9994613579035875\n",
      "Epoch[726/3000] loss: 0.0019039754168041188, acc: 0.9994701163116592\n",
      "Epoch[727/3000] loss: 0.0019099840758545344, acc: 0.9994613579035875\n",
      "Epoch[728/3000] loss: 0.0019022905060557091, acc: 0.9994657371076233\n",
      "Epoch[729/3000] loss: 0.001899249042179947, acc: 0.9994613579035875\n",
      "Epoch[730/3000] loss: 0.0019111253079139177, acc: 0.9994613579035875\n",
      "Epoch[731/3000] loss: 0.0019055469991291316, acc: 0.9994657371076233\n",
      "Epoch[732/3000] loss: 0.0018993100282125857, acc: 0.9994832539237668\n",
      "Epoch[733/3000] loss: 0.0019000399274345134, acc: 0.9994876331278026\n",
      "Epoch[734/3000] loss: 0.0019328794390239311, acc: 0.9994517236547085\n",
      "Epoch[735/3000] loss: 0.0019050474778190202, acc: 0.9994657371076233\n",
      "Epoch[736/3000] loss: 0.0018989694151055588, acc: 0.999439461883408\n",
      "Epoch[737/3000] loss: 0.0019056089617062158, acc: 0.9994482202914798\n",
      "Epoch[738/3000] loss: 0.001902128935178414, acc: 0.9994657371076233\n",
      "Epoch[739/3000] loss: 0.0019008225991607032, acc: 0.9994525994955157\n",
      "Epoch[740/3000] loss: 0.0018990860497501978, acc: 0.9994701163116592\n",
      "Epoch[741/3000] loss: 0.0018900161518316221, acc: 0.9994744955156951\n",
      "Epoch[742/3000] loss: 0.0019200966116963655, acc: 0.9994517236547085\n",
      "Epoch[743/3000] loss: 0.0019058660159867377, acc: 0.9994569786995515\n",
      "Epoch[744/3000] loss: 0.0018974844451437468, acc: 0.9994569786995515\n",
      "Epoch[745/3000] loss: 0.0018972242869984362, acc: 0.9994525994955157\n",
      "Epoch[746/3000] loss: 0.001903035286462575, acc: 0.9994657371076233\n",
      "Epoch[747/3000] loss: 0.0018977566334732047, acc: 0.9994482202914798\n",
      "Epoch[748/3000] loss: 0.0019014330384314463, acc: 0.9994569786995515\n",
      "Epoch[749/3000] loss: 0.0018992364058644832, acc: 0.9994613579035875\n",
      "Epoch[750/3000] loss: 0.0018989811122188795, acc: 0.9994657371076233\n",
      "Epoch[751/3000] loss: 0.0019204432960842931, acc: 0.9994517236547085\n",
      "Epoch[752/3000] loss: 0.001896444582851497, acc: 0.9994701163116592\n",
      "Epoch[753/3000] loss: 0.0018941343935657317, acc: 0.9994701163116592\n",
      "Epoch[754/3000] loss: 0.0019186547422235269, acc: 0.9994429652466367\n",
      "Epoch[755/3000] loss: 0.0018981625977388089, acc: 0.999439461883408\n",
      "Epoch[756/3000] loss: 0.0018889642656441973, acc: 0.9994657371076233\n",
      "Epoch[757/3000] loss: 0.001891313617543428, acc: 0.9994657371076233\n",
      "Epoch[758/3000] loss: 0.0018976859235636876, acc: 0.9994613579035875\n",
      "Epoch[759/3000] loss: 0.001886405930354199, acc: 0.9994744955156951\n",
      "Epoch[760/3000] loss: 0.0019225222812292077, acc: 0.999434206838565\n",
      "Epoch[761/3000] loss: 0.0018954814156943373, acc: 0.9994525994955157\n",
      "Epoch[762/3000] loss: 0.0019156048987285263, acc: 0.9994517236547085\n",
      "Epoch[763/3000] loss: 0.0018902632741526662, acc: 0.9994657371076233\n",
      "Epoch[764/3000] loss: 0.001897001785637013, acc: 0.9994744955156951\n",
      "Epoch[765/3000] loss: 0.0019140764324395445, acc: 0.9994648612668161\n",
      "Epoch[766/3000] loss: 0.0018957591946918053, acc: 0.9994525994955157\n",
      "Epoch[767/3000] loss: 0.0018953848049173684, acc: 0.9994613579035875\n",
      "Epoch[768/3000] loss: 0.0018854300789189846, acc: 0.9994657371076233\n",
      "Epoch[769/3000] loss: 0.0019193193044554184, acc: 0.9994429652466367\n",
      "Epoch[770/3000] loss: 0.0018886567182652156, acc: 0.9994613579035875\n",
      "Epoch[771/3000] loss: 0.0018893851551746441, acc: 0.9994657371076233\n",
      "Epoch[772/3000] loss: 0.0018902628900359397, acc: 0.9994613579035875\n",
      "Epoch[773/3000] loss: 0.0018881220663967517, acc: 0.9994482202914798\n",
      "Epoch[774/3000] loss: 0.0018926447556855787, acc: 0.9994613579035875\n",
      "Epoch[775/3000] loss: 0.001888769965290876, acc: 0.9994657371076233\n",
      "Epoch[776/3000] loss: 0.001888190749389247, acc: 0.9994744955156951\n",
      "Epoch[777/3000] loss: 0.0018860619353298688, acc: 0.9994569786995515\n",
      "Epoch[778/3000] loss: 0.001882248686395811, acc: 0.9994744955156951\n",
      "Epoch[779/3000] loss: 0.0018884692174737365, acc: 0.9994744955156951\n",
      "Epoch[780/3000] loss: 0.0018851288439879024, acc: 0.9994657371076233\n",
      "Epoch[781/3000] loss: 0.001886097262852962, acc: 0.9994832539237668\n",
      "Epoch[782/3000] loss: 0.0018861990273652668, acc: 0.9994569786995515\n",
      "Epoch[783/3000] loss: 0.0018845551467614563, acc: 0.9994569786995515\n",
      "Epoch[784/3000] loss: 0.0018868167472104313, acc: 0.9994657371076233\n",
      "Epoch[785/3000] loss: 0.0018810800862460212, acc: 0.9994613579035875\n",
      "Epoch[786/3000] loss: 0.0018828077673082786, acc: 0.999478874719731\n",
      "Epoch[787/3000] loss: 0.0018824045087363923, acc: 0.9994525994955157\n",
      "Epoch[788/3000] loss: 0.0018862265416763194, acc: 0.9994613579035875\n",
      "Epoch[789/3000] loss: 0.0018833509448698028, acc: 0.9994613579035875\n",
      "Epoch[790/3000] loss: 0.001881621494093994, acc: 0.9994482202914798\n",
      "Epoch[791/3000] loss: 0.0019063953172203808, acc: 0.9994473444506726\n",
      "Epoch[792/3000] loss: 0.0018725275879868332, acc: 0.9994701163116592\n",
      "Epoch[793/3000] loss: 0.0018853784090077927, acc: 0.9994613579035875\n",
      "Epoch[794/3000] loss: 0.0018770419502276625, acc: 0.9994744955156951\n",
      "Epoch[795/3000] loss: 0.001879723428751093, acc: 0.9994525994955157\n",
      "Epoch[796/3000] loss: 0.0018825310673059596, acc: 0.9994701163116592\n",
      "Epoch[797/3000] loss: 0.0018783637917025803, acc: 0.9994569786995515\n",
      "Epoch[798/3000] loss: 0.0018879887638872646, acc: 0.9994429652466367\n",
      "Epoch[799/3000] loss: 0.0018757579833667307, acc: 0.999478874719731\n",
      "Epoch[800/3000] loss: 0.001882172997186332, acc: 0.9994525994955157\n",
      "Epoch[801/3000] loss: 0.001879105293956108, acc: 0.9994569786995515\n",
      "Epoch[802/3000] loss: 0.0018684750815117137, acc: 0.9994744955156951\n",
      "Epoch[803/3000] loss: 0.001879293376758941, acc: 0.9994657371076233\n",
      "Epoch[804/3000] loss: 0.0018819497728785823, acc: 0.9994525994955157\n",
      "Epoch[805/3000] loss: 0.0019067057743543266, acc: 0.9994561028587443\n",
      "Epoch[806/3000] loss: 0.0018801952344296247, acc: 0.9994657371076233\n",
      "Epoch[807/3000] loss: 0.001877399277555218, acc: 0.9994613579035875\n",
      "Epoch[808/3000] loss: 0.0018744288486007793, acc: 0.9994525994955157\n",
      "Epoch[809/3000] loss: 0.0018752910550431851, acc: 0.9994744955156951\n",
      "Epoch[810/3000] loss: 0.00187232525443795, acc: 0.9994613579035875\n",
      "Epoch[811/3000] loss: 0.001875635607575532, acc: 0.9994744955156951\n",
      "Epoch[812/3000] loss: 0.001878879443827036, acc: 0.9994657371076233\n",
      "Epoch[813/3000] loss: 0.0019112125154129296, acc: 0.9994517236547085\n",
      "Epoch[814/3000] loss: 0.0018767424136127587, acc: 0.9994569786995515\n",
      "Epoch[815/3000] loss: 0.0018724802455166876, acc: 0.9994701163116592\n",
      "Epoch[816/3000] loss: 0.001875742022118089, acc: 0.9994569786995515\n",
      "Epoch[817/3000] loss: 0.0018812725332530628, acc: 0.9994701163116592\n",
      "Epoch[818/3000] loss: 0.0018757445295582756, acc: 0.9994613579035875\n",
      "Epoch[819/3000] loss: 0.0018703025443551982, acc: 0.9994701163116592\n",
      "Epoch[820/3000] loss: 0.0018722303669291263, acc: 0.9994613579035875\n",
      "Epoch[821/3000] loss: 0.0018735796015850851, acc: 0.9994701163116592\n",
      "Epoch[822/3000] loss: 0.0018644948791475776, acc: 0.9994744955156951\n",
      "Epoch[823/3000] loss: 0.001874898040921737, acc: 0.9994569786995515\n",
      "Epoch[824/3000] loss: 0.0018803664000273939, acc: 0.9994701163116592\n",
      "Epoch[825/3000] loss: 0.0018721575703858993, acc: 0.9994350826793722\n",
      "Epoch[826/3000] loss: 0.0018738919791150546, acc: 0.9994613579035875\n",
      "Epoch[827/3000] loss: 0.0018650768449941244, acc: 0.9994569786995515\n",
      "Epoch[828/3000] loss: 0.001865152672022639, acc: 0.9994701163116592\n",
      "Epoch[829/3000] loss: 0.0018708616695922774, acc: 0.9994701163116592\n",
      "Epoch[830/3000] loss: 0.0018883314208450442, acc: 0.9994569786995515\n",
      "Epoch[831/3000] loss: 0.0018627081748727587, acc: 0.9994613579035875\n",
      "Epoch[832/3000] loss: 0.0018704804683958608, acc: 0.9994744955156951\n",
      "Epoch[833/3000] loss: 0.001870093947177481, acc: 0.999478874719731\n",
      "Epoch[834/3000] loss: 0.0018703543535047269, acc: 0.999478874719731\n",
      "Epoch[835/3000] loss: 0.0018697327449375522, acc: 0.9994569786995515\n",
      "Epoch[836/3000] loss: 0.0018707521242426809, acc: 0.9994613579035875\n",
      "Epoch[837/3000] loss: 0.0018707869685426744, acc: 0.9994525994955157\n",
      "Epoch[838/3000] loss: 0.0018681955068943696, acc: 0.999443841087444\n",
      "Epoch[839/3000] loss: 0.0018622734235500319, acc: 0.9994613579035875\n",
      "Epoch[840/3000] loss: 0.0018702567586055493, acc: 0.9994613579035875\n",
      "Epoch[841/3000] loss: 0.0018661652945407255, acc: 0.9994657371076233\n",
      "Epoch[842/3000] loss: 0.0018712739571463055, acc: 0.9994657371076233\n",
      "Epoch[843/3000] loss: 0.001869054057037564, acc: 0.9994569786995515\n",
      "Epoch[844/3000] loss: 0.001863868217346271, acc: 0.9994657371076233\n",
      "Epoch[845/3000] loss: 0.0018650548953713453, acc: 0.9994613579035875\n",
      "Epoch[846/3000] loss: 0.001864148504183614, acc: 0.9994701163116592\n",
      "Epoch[847/3000] loss: 0.001861310689214467, acc: 0.9994832539237668\n",
      "Epoch[848/3000] loss: 0.001858953358346767, acc: 0.9994613579035875\n",
      "Epoch[849/3000] loss: 0.0018630035769900035, acc: 0.9994657371076233\n",
      "Epoch[850/3000] loss: 0.001867111221960714, acc: 0.9994307034753364\n",
      "Epoch[851/3000] loss: 0.0018619619124544723, acc: 0.9994482202914798\n",
      "Epoch[852/3000] loss: 0.0018648182512775837, acc: 0.9994569786995515\n",
      "Epoch[853/3000] loss: 0.0018537897280103636, acc: 0.9994832539237668\n",
      "Epoch[854/3000] loss: 0.0018643087012752799, acc: 0.9994701163116592\n",
      "Epoch[855/3000] loss: 0.0018885694344175903, acc: 0.9994473444506726\n",
      "Epoch[856/3000] loss: 0.0018680246467889373, acc: 0.9994569786995515\n",
      "Epoch[857/3000] loss: 0.0018596432148009088, acc: 0.9994744955156951\n",
      "Epoch[858/3000] loss: 0.0018648949813242191, acc: 0.9994744955156951\n",
      "Epoch[859/3000] loss: 0.0018598439007206098, acc: 0.9994657371076233\n",
      "Epoch[860/3000] loss: 0.0018599792596669512, acc: 0.9994525994955157\n",
      "Epoch[861/3000] loss: 0.0018618872610151542, acc: 0.999478874719731\n",
      "Epoch[862/3000] loss: 0.0018620177768084044, acc: 0.9994744955156951\n",
      "Epoch[863/3000] loss: 0.0018567612520045801, acc: 0.999439461883408\n",
      "Epoch[864/3000] loss: 0.0018574672724145638, acc: 0.999478874719731\n",
      "Epoch[865/3000] loss: 0.0019001709639807748, acc: 0.999434206838565\n",
      "Epoch[866/3000] loss: 0.001857277410574839, acc: 0.9994613579035875\n",
      "Epoch[867/3000] loss: 0.0018587356944938583, acc: 0.9994701163116592\n",
      "Epoch[868/3000] loss: 0.0018615413107729811, acc: 0.9994613579035875\n",
      "Epoch[869/3000] loss: 0.0018845980061242156, acc: 0.9994561028587443\n",
      "Epoch[870/3000] loss: 0.0018563360010677423, acc: 0.9994657371076233\n",
      "Epoch[871/3000] loss: 0.0018605630360115654, acc: 0.9994744955156951\n",
      "Epoch[872/3000] loss: 0.0018575226094818052, acc: 0.9994701163116592\n",
      "Epoch[873/3000] loss: 0.0018568006256616005, acc: 0.9994657371076233\n",
      "Epoch[874/3000] loss: 0.0018543746742506557, acc: 0.9994657371076233\n",
      "Epoch[875/3000] loss: 0.0018614869152815416, acc: 0.9994482202914798\n",
      "Epoch[876/3000] loss: 0.0018572594965798957, acc: 0.9994657371076233\n",
      "Epoch[877/3000] loss: 0.0018562306195317147, acc: 0.9994657371076233\n",
      "Epoch[878/3000] loss: 0.001855540900186958, acc: 0.9994613579035875\n",
      "Epoch[879/3000] loss: 0.0018587884915014573, acc: 0.9994525994955157\n",
      "Epoch[880/3000] loss: 0.0018546701681412178, acc: 0.9994657371076233\n",
      "Epoch[881/3000] loss: 0.0018597804046620462, acc: 0.9994350826793722\n",
      "Epoch[882/3000] loss: 0.001850821958907847, acc: 0.9994613579035875\n",
      "Epoch[883/3000] loss: 0.0018564044197276338, acc: 0.9994832539237668\n",
      "Epoch[884/3000] loss: 0.001848977955055672, acc: 0.9994613579035875\n",
      "Epoch[885/3000] loss: 0.0018559878618961368, acc: 0.9994613579035875\n",
      "Epoch[886/3000] loss: 0.0018575343519393506, acc: 0.9994657371076233\n",
      "Epoch[887/3000] loss: 0.001853177376916502, acc: 0.9994613579035875\n",
      "Epoch[888/3000] loss: 0.0018912732447799677, acc: 0.9994473444506726\n",
      "Epoch[889/3000] loss: 0.001856088977486011, acc: 0.9994569786995515\n",
      "Epoch[890/3000] loss: 0.0018539917713203185, acc: 0.9994482202914798\n",
      "Epoch[891/3000] loss: 0.0018525081696708539, acc: 0.9994701163116592\n",
      "Epoch[892/3000] loss: 0.0018532008322138776, acc: 0.9994482202914798\n",
      "Epoch[893/3000] loss: 0.0018562169955294754, acc: 0.9994701163116592\n",
      "Epoch[894/3000] loss: 0.0018544267097950313, acc: 0.9994525994955157\n",
      "Epoch[895/3000] loss: 0.0018770628093459018, acc: 0.9994473444506726\n",
      "Epoch[896/3000] loss: 0.0018521088871814107, acc: 0.9994525994955157\n",
      "Epoch[897/3000] loss: 0.0018531620443040728, acc: 0.9994525994955157\n",
      "Epoch[898/3000] loss: 0.001877653201909646, acc: 0.9994473444506726\n",
      "Epoch[899/3000] loss: 0.0018534291990330645, acc: 0.9994657371076233\n",
      "Epoch[900/3000] loss: 0.0018926781582937241, acc: 0.9994736196748878\n",
      "Epoch[901/3000] loss: 0.001849820233376137, acc: 0.9994657371076233\n",
      "Epoch[902/3000] loss: 0.0018496344456974587, acc: 0.9994701163116592\n",
      "Epoch[903/3000] loss: 0.001854718954693184, acc: 0.9994569786995515\n",
      "Epoch[904/3000] loss: 0.0018453072048623858, acc: 0.999478874719731\n",
      "Epoch[905/3000] loss: 0.001851858136531176, acc: 0.9994701163116592\n",
      "Epoch[906/3000] loss: 0.001846271953749107, acc: 0.9994613579035875\n",
      "Epoch[907/3000] loss: 0.0018503954931471086, acc: 0.9994613579035875\n",
      "Epoch[908/3000] loss: 0.001846014636090339, acc: 0.9994569786995515\n",
      "Epoch[909/3000] loss: 0.0018495250724095771, acc: 0.999478874719731\n",
      "Epoch[910/3000] loss: 0.0018452156840174563, acc: 0.9994482202914798\n",
      "Epoch[911/3000] loss: 0.0018473299154916501, acc: 0.9994569786995515\n",
      "Epoch[912/3000] loss: 0.0018475075499214662, acc: 0.9994744955156951\n",
      "Epoch[913/3000] loss: 0.0018456751702925255, acc: 0.9994525994955157\n",
      "Epoch[914/3000] loss: 0.0018487145752598914, acc: 0.9994569786995515\n",
      "Epoch[915/3000] loss: 0.0018658022364177204, acc: 0.9994569786995515\n",
      "Epoch[916/3000] loss: 0.0018713474697269622, acc: 0.9994604820627803\n",
      "Epoch[917/3000] loss: 0.0018432313831194958, acc: 0.9994525994955157\n",
      "Epoch[918/3000] loss: 0.0018475654130021629, acc: 0.9994613579035875\n",
      "Epoch[919/3000] loss: 0.0018693188061816184, acc: 0.999469240470852\n",
      "Epoch[920/3000] loss: 0.0018463100170322633, acc: 0.9994569786995515\n",
      "Epoch[921/3000] loss: 0.0018445930126302712, acc: 0.999478874719731\n",
      "Epoch[922/3000] loss: 0.001847647586346039, acc: 0.9994657371076233\n",
      "Epoch[923/3000] loss: 0.0018637548924792839, acc: 0.9994482202914798\n",
      "Epoch[924/3000] loss: 0.0018420327707457293, acc: 0.999478874719731\n",
      "Epoch[925/3000] loss: 0.0018418994801534777, acc: 0.9994613579035875\n",
      "Epoch[926/3000] loss: 0.0018440994469280175, acc: 0.9994744955156951\n",
      "Epoch[927/3000] loss: 0.001845545210579834, acc: 0.9994307034753364\n",
      "Epoch[928/3000] loss: 0.0018434820328688383, acc: 0.9994744955156951\n",
      "Epoch[929/3000] loss: 0.0018406491714727808, acc: 0.9994701163116592\n",
      "Epoch[930/3000] loss: 0.0018412730157492273, acc: 0.9994569786995515\n",
      "Epoch[931/3000] loss: 0.0018382749465003666, acc: 0.9994613579035875\n",
      "Epoch[932/3000] loss: 0.0018461268311528271, acc: 0.9994832539237668\n",
      "Epoch[933/3000] loss: 0.0018425749735654835, acc: 0.9994657371076233\n",
      "Epoch[934/3000] loss: 0.0018390003883907223, acc: 0.9994482202914798\n",
      "Epoch[935/3000] loss: 0.0018430401964518743, acc: 0.9994657371076233\n",
      "Epoch[936/3000] loss: 0.001841476350851621, acc: 0.9994701163116592\n",
      "Epoch[937/3000] loss: 0.0018411913072346343, acc: 0.9994876331278026\n",
      "Epoch[938/3000] loss: 0.0018411919964847938, acc: 0.9994657371076233\n",
      "Epoch[939/3000] loss: 0.0018385186688447237, acc: 0.9994613579035875\n",
      "Epoch[940/3000] loss: 0.001837505047522003, acc: 0.9994744955156951\n",
      "Epoch[941/3000] loss: 0.001838869004417485, acc: 0.9994657371076233\n",
      "Epoch[942/3000] loss: 0.0018384315297413447, acc: 0.999478874719731\n",
      "Epoch[943/3000] loss: 0.0018361600077411002, acc: 0.9994613579035875\n",
      "Epoch[944/3000] loss: 0.0018383675553619804, acc: 0.9994701163116592\n",
      "Epoch[945/3000] loss: 0.0018404482893224607, acc: 0.9994701163116592\n",
      "Epoch[946/3000] loss: 0.0018370886177375716, acc: 0.9994657371076233\n",
      "Epoch[947/3000] loss: 0.0018413530775400547, acc: 0.9994613579035875\n",
      "Epoch[948/3000] loss: 0.0018407066073356747, acc: 0.9994482202914798\n",
      "Epoch[949/3000] loss: 0.001837270144812868, acc: 0.9994613579035875\n",
      "Epoch[950/3000] loss: 0.001834243537977611, acc: 0.9994701163116592\n",
      "Epoch[951/3000] loss: 0.001830239021165804, acc: 0.9994613579035875\n",
      "Epoch[952/3000] loss: 0.0018353873495678909, acc: 0.9994920123318386\n",
      "Epoch[953/3000] loss: 0.0018369165987238097, acc: 0.999478874719731\n",
      "Epoch[954/3000] loss: 0.001836359063136645, acc: 0.999478874719731\n",
      "Epoch[955/3000] loss: 0.0018395071741735282, acc: 0.999443841087444\n",
      "Epoch[956/3000] loss: 0.0018347871739897203, acc: 0.9994569786995515\n",
      "Epoch[957/3000] loss: 0.0018349413213836507, acc: 0.999478874719731\n",
      "Epoch[958/3000] loss: 0.0018390952884774908, acc: 0.999478874719731\n",
      "Epoch[959/3000] loss: 0.0018353031381401921, acc: 0.9994482202914798\n",
      "Epoch[960/3000] loss: 0.0018382468809702694, acc: 0.9994657371076233\n",
      "Epoch[961/3000] loss: 0.0018318621629738535, acc: 0.9994657371076233\n",
      "Epoch[962/3000] loss: 0.0018579419134734168, acc: 0.9994517236547085\n",
      "Epoch[963/3000] loss: 0.0018339560189689096, acc: 0.9994657371076233\n",
      "Epoch[964/3000] loss: 0.0018380577830489956, acc: 0.9994657371076233\n",
      "Epoch[965/3000] loss: 0.0018312046850531365, acc: 0.9994744955156951\n",
      "Epoch[966/3000] loss: 0.0018327972306355713, acc: 0.9994657371076233\n",
      "Epoch[967/3000] loss: 0.0018357008202207659, acc: 0.999478874719731\n",
      "Epoch[968/3000] loss: 0.0018338840544216478, acc: 0.9994701163116592\n",
      "Epoch[969/3000] loss: 0.0018848923471180858, acc: 0.9994736196748878\n",
      "Epoch[970/3000] loss: 0.0018344942118713869, acc: 0.9994569786995515\n",
      "Epoch[971/3000] loss: 0.0018297270108620567, acc: 0.9994525994955157\n",
      "Epoch[972/3000] loss: 0.0018320882664073959, acc: 0.9994701163116592\n",
      "Epoch[973/3000] loss: 0.0018296922316623073, acc: 0.9994701163116592\n",
      "Epoch[974/3000] loss: 0.0018316058886319402, acc: 0.9994613579035875\n",
      "Epoch[975/3000] loss: 0.0018319018043730629, acc: 0.9994657371076233\n",
      "Epoch[976/3000] loss: 0.0018327446553162863, acc: 0.9994525994955157\n",
      "Epoch[977/3000] loss: 0.001831679921386468, acc: 0.999478874719731\n",
      "Epoch[978/3000] loss: 0.0018581735840667746, acc: 0.9994517236547085\n",
      "Epoch[979/3000] loss: 0.0018361117575544578, acc: 0.999439461883408\n",
      "Epoch[980/3000] loss: 0.0018326178859641845, acc: 0.999478874719731\n",
      "Epoch[981/3000] loss: 0.0018317922506789544, acc: 0.9994657371076233\n",
      "Epoch[982/3000] loss: 0.0018314268874477452, acc: 0.9994482202914798\n",
      "Epoch[983/3000] loss: 0.0018515316629997662, acc: 0.9994657371076233\n",
      "Epoch[984/3000] loss: 0.0018305429884963047, acc: 0.9994744955156951\n",
      "Epoch[985/3000] loss: 0.0018257124197691252, acc: 0.9994701163116592\n",
      "Epoch[986/3000] loss: 0.001831808151028157, acc: 0.9994657371076233\n",
      "Epoch[987/3000] loss: 0.00182748634180964, acc: 0.9994613579035875\n",
      "Epoch[988/3000] loss: 0.0018298783847131488, acc: 0.9994701163116592\n",
      "Epoch[989/3000] loss: 0.00186560185037025, acc: 0.9994561028587443\n",
      "Epoch[990/3000] loss: 0.0018303233980970675, acc: 0.9994701163116592\n",
      "Epoch[991/3000] loss: 0.0018301624150306843, acc: 0.9994613579035875\n",
      "Epoch[992/3000] loss: 0.0018295655765231634, acc: 0.9994832539237668\n",
      "Epoch[993/3000] loss: 0.001829316838939529, acc: 0.9994569786995515\n",
      "Epoch[994/3000] loss: 0.0018301942093014501, acc: 0.9994701163116592\n",
      "Epoch[995/3000] loss: 0.0018277093202846424, acc: 0.9994613579035875\n",
      "Epoch[996/3000] loss: 0.0018255088592507129, acc: 0.999478874719731\n",
      "Epoch[997/3000] loss: 0.0018251489981198595, acc: 0.9994657371076233\n",
      "Epoch[998/3000] loss: 0.0018272846909201388, acc: 0.9994657371076233\n",
      "Epoch[999/3000] loss: 0.0018268388541050034, acc: 0.999478874719731\n",
      "Epoch[1000/3000] loss: 0.001876723407000638, acc: 0.9994648612668161\n",
      "Epoch[1001/3000] loss: 0.0018252297768698465, acc: 0.9994657371076233\n",
      "Epoch[1002/3000] loss: 0.0018502700839178232, acc: 0.9994604820627803\n",
      "Epoch[1003/3000] loss: 0.0018285081131266187, acc: 0.9994657371076233\n",
      "Epoch[1004/3000] loss: 0.0018285096907940423, acc: 0.9994701163116592\n",
      "Epoch[1005/3000] loss: 0.0018248536236708302, acc: 0.999478874719731\n",
      "Epoch[1006/3000] loss: 0.0018245215715680975, acc: 0.9994569786995515\n",
      "Epoch[1007/3000] loss: 0.0018244569717663405, acc: 0.999478874719731\n",
      "Epoch[1008/3000] loss: 0.0018248546951697856, acc: 0.9994744955156951\n",
      "Epoch[1009/3000] loss: 0.0018228803441275488, acc: 0.9994701163116592\n",
      "Epoch[1010/3000] loss: 0.0018273712759821847, acc: 0.9994657371076233\n",
      "Epoch[1011/3000] loss: 0.0018263522402866014, acc: 0.9994701163116592\n",
      "Epoch[1012/3000] loss: 0.0018232743876175474, acc: 0.9994525994955157\n",
      "Epoch[1013/3000] loss: 0.001823876933167616, acc: 0.9994701163116592\n",
      "Epoch[1014/3000] loss: 0.0018237291417141927, acc: 0.9994613579035875\n",
      "Epoch[1015/3000] loss: 0.0018239264592570497, acc: 0.9994613579035875\n",
      "Epoch[1016/3000] loss: 0.0018243473226283082, acc: 0.9994744955156951\n",
      "Epoch[1017/3000] loss: 0.0018221088941711484, acc: 0.999478874719731\n",
      "Epoch[1018/3000] loss: 0.0018198118040480024, acc: 0.9994744955156951\n",
      "Epoch[1019/3000] loss: 0.0018237296609010787, acc: 0.9994657371076233\n",
      "Epoch[1020/3000] loss: 0.0018231800394431603, acc: 0.9994657371076233\n",
      "Epoch[1021/3000] loss: 0.0018192764324551885, acc: 0.999478874719731\n",
      "Epoch[1022/3000] loss: 0.0018185881191647702, acc: 0.9994657371076233\n",
      "Epoch[1023/3000] loss: 0.0018233006358875047, acc: 0.9994613579035875\n",
      "Epoch[1024/3000] loss: 0.0018226827989092109, acc: 0.9994657371076233\n",
      "Epoch[1025/3000] loss: 0.0018500899932650152, acc: 0.9994648612668161\n",
      "Epoch[1026/3000] loss: 0.001819214377085174, acc: 0.9994701163116592\n",
      "Epoch[1027/3000] loss: 0.0018201050915778302, acc: 0.999478874719731\n",
      "Epoch[1028/3000] loss: 0.0018194144497852096, acc: 0.9994701163116592\n",
      "Epoch[1029/3000] loss: 0.001822396304236251, acc: 0.9994701163116592\n",
      "Epoch[1030/3000] loss: 0.0018205573264883483, acc: 0.9994876331278026\n",
      "Epoch[1031/3000] loss: 0.0018192416532309683, acc: 0.9994569786995515\n",
      "Epoch[1032/3000] loss: 0.0018193710781281476, acc: 0.9994569786995515\n",
      "Epoch[1033/3000] loss: 0.0018215741929126358, acc: 0.9994701163116592\n",
      "Epoch[1034/3000] loss: 0.0018199620810403273, acc: 0.9994701163116592\n",
      "Epoch[1035/3000] loss: 0.0018208537477617007, acc: 0.999478874719731\n",
      "Epoch[1036/3000] loss: 0.0018212016970487578, acc: 0.9994569786995515\n",
      "Epoch[1037/3000] loss: 0.001820367340957344, acc: 0.9994701163116592\n",
      "Epoch[1038/3000] loss: 0.0018169836851325374, acc: 0.9994525994955157\n",
      "Epoch[1039/3000] loss: 0.0018144114503979938, acc: 0.9994920123318386\n",
      "Epoch[1040/3000] loss: 0.0018205963379299272, acc: 0.9994657371076233\n",
      "Epoch[1041/3000] loss: 0.0018165609803403918, acc: 0.9994920123318386\n",
      "Epoch[1042/3000] loss: 0.0018415366642823394, acc: 0.9994561028587443\n",
      "Epoch[1043/3000] loss: 0.001819083368459992, acc: 0.9994701163116592\n",
      "Epoch[1044/3000] loss: 0.0018192185039479717, acc: 0.9994613579035875\n",
      "Epoch[1045/3000] loss: 0.0018464354651862903, acc: 0.9994648612668161\n",
      "Epoch[1046/3000] loss: 0.0018147290608749856, acc: 0.9994744955156951\n",
      "Epoch[1047/3000] loss: 0.0018122709153010979, acc: 0.999478874719731\n",
      "Epoch[1048/3000] loss: 0.0018175815361156625, acc: 0.9994832539237668\n",
      "Epoch[1049/3000] loss: 0.0018477587875877605, acc: 0.999469240470852\n",
      "Epoch[1050/3000] loss: 0.001817977867543584, acc: 0.9994657371076233\n",
      "Epoch[1051/3000] loss: 0.0018171517397363847, acc: 0.9994701163116592\n",
      "Epoch[1052/3000] loss: 0.0018140313014828689, acc: 0.9994876331278026\n",
      "Epoch[1053/3000] loss: 0.0018179307393879122, acc: 0.9994744955156951\n",
      "Epoch[1054/3000] loss: 0.0018165448378943034, acc: 0.9994744955156951\n",
      "Epoch[1055/3000] loss: 0.0018128835200909613, acc: 0.9994701163116592\n",
      "Epoch[1056/3000] loss: 0.001818858024613585, acc: 0.9994569786995515\n",
      "Epoch[1057/3000] loss: 0.0018165855062802479, acc: 0.9994876331278026\n",
      "Epoch[1058/3000] loss: 0.0018158712149662307, acc: 0.9994613579035875\n",
      "Epoch[1059/3000] loss: 0.0018141997092235244, acc: 0.9994832539237668\n",
      "Epoch[1060/3000] loss: 0.0018139524731299952, acc: 0.9994613579035875\n",
      "Epoch[1061/3000] loss: 0.001815763431483245, acc: 0.9994701163116592\n",
      "Epoch[1062/3000] loss: 0.0018128086701126205, acc: 0.9994832539237668\n",
      "Epoch[1063/3000] loss: 0.0018132302676459472, acc: 0.9994657371076233\n",
      "Epoch[1064/3000] loss: 0.001815726549605157, acc: 0.9994744955156951\n",
      "Epoch[1065/3000] loss: 0.0018147929314903852, acc: 0.9994832539237668\n",
      "Epoch[1066/3000] loss: 0.0018137480492196976, acc: 0.9994876331278026\n",
      "Epoch[1067/3000] loss: 0.0018323253730465078, acc: 0.9994701163116592\n",
      "Epoch[1068/3000] loss: 0.0018142488908143104, acc: 0.9994525994955157\n",
      "Epoch[1069/3000] loss: 0.0018159938572597692, acc: 0.999478874719731\n",
      "Epoch[1070/3000] loss: 0.0018386522838043863, acc: 0.9994736196748878\n",
      "Epoch[1071/3000] loss: 0.001810580245183607, acc: 0.9994569786995515\n",
      "Epoch[1072/3000] loss: 0.0018131186251810203, acc: 0.9994657371076233\n",
      "Epoch[1073/3000] loss: 0.00180947197944409, acc: 0.9994482202914798\n",
      "Epoch[1074/3000] loss: 0.001812779905699061, acc: 0.9994701163116592\n",
      "Epoch[1075/3000] loss: 0.0018112208708771504, acc: 0.9994613579035875\n",
      "Epoch[1076/3000] loss: 0.001813492849549775, acc: 0.999478874719731\n",
      "Epoch[1077/3000] loss: 0.0018113718266543283, acc: 0.999478874719731\n",
      "Epoch[1078/3000] loss: 0.0018148456741322245, acc: 0.9994569786995515\n",
      "Epoch[1079/3000] loss: 0.0018096298827819611, acc: 0.9994657371076233\n",
      "Epoch[1080/3000] loss: 0.0018109227015060396, acc: 0.9994613579035875\n",
      "Epoch[1081/3000] loss: 0.0018115837562451037, acc: 0.9994744955156951\n",
      "Epoch[1082/3000] loss: 0.0018087136091553108, acc: 0.9994701163116592\n",
      "Epoch[1083/3000] loss: 0.0018081987181351746, acc: 0.9994744955156951\n",
      "Epoch[1084/3000] loss: 0.0018107385402812226, acc: 0.9994701163116592\n",
      "Epoch[1085/3000] loss: 0.0018091750437230125, acc: 0.9994744955156951\n",
      "Epoch[1086/3000] loss: 0.0018134341511858816, acc: 0.9994832539237668\n",
      "Epoch[1087/3000] loss: 0.0018105954222112654, acc: 0.9994569786995515\n",
      "Epoch[1088/3000] loss: 0.0018118577418013474, acc: 0.9994701163116592\n",
      "Epoch[1089/3000] loss: 0.001807652856544667, acc: 0.9994744955156951\n",
      "Epoch[1090/3000] loss: 0.0018063490818516453, acc: 0.9994744955156951\n",
      "Epoch[1091/3000] loss: 0.001808092823162878, acc: 0.9994701163116592\n",
      "Epoch[1092/3000] loss: 0.001807161886638175, acc: 0.9994701163116592\n",
      "Epoch[1093/3000] loss: 0.0018122756453505702, acc: 0.9994876331278026\n",
      "Epoch[1094/3000] loss: 0.001803600556471825, acc: 0.9994832539237668\n",
      "Epoch[1095/3000] loss: 0.001806157412221945, acc: 0.9994657371076233\n",
      "Epoch[1096/3000] loss: 0.0018116484202498455, acc: 0.9994744955156951\n",
      "Epoch[1097/3000] loss: 0.0018248033673427984, acc: 0.9994648612668161\n",
      "Epoch[1098/3000] loss: 0.001810012652991327, acc: 0.9994657371076233\n",
      "Epoch[1099/3000] loss: 0.0018085199308392672, acc: 0.9994657371076233\n",
      "Epoch[1100/3000] loss: 0.0018063136540068003, acc: 0.9994744955156951\n",
      "Epoch[1101/3000] loss: 0.0018091987060670606, acc: 0.9994744955156951\n",
      "Epoch[1102/3000] loss: 0.0018067114941436864, acc: 0.9994832539237668\n",
      "Epoch[1103/3000] loss: 0.0018075905553011208, acc: 0.9994701163116592\n",
      "Epoch[1104/3000] loss: 0.0018059089294191807, acc: 0.9994744955156951\n",
      "Epoch[1105/3000] loss: 0.0018094985632401016, acc: 0.9994657371076233\n",
      "Epoch[1106/3000] loss: 0.001803156430371361, acc: 0.999478874719731\n",
      "Epoch[1107/3000] loss: 0.0018089423785666097, acc: 0.9994876331278026\n",
      "Epoch[1108/3000] loss: 0.0018056390671366972, acc: 0.9994832539237668\n",
      "Epoch[1109/3000] loss: 0.0018055456420464123, acc: 0.9994744955156951\n",
      "Epoch[1110/3000] loss: 0.0018077695951571938, acc: 0.9994613579035875\n",
      "Epoch[1111/3000] loss: 0.0018054048139454469, acc: 0.9994701163116592\n",
      "Epoch[1112/3000] loss: 0.0018070669684104538, acc: 0.999478874719731\n",
      "Epoch[1113/3000] loss: 0.0018040853698659352, acc: 0.999478874719731\n",
      "Epoch[1114/3000] loss: 0.0018045711736712263, acc: 0.9994657371076233\n",
      "Epoch[1115/3000] loss: 0.001803315457624447, acc: 0.9994920123318386\n",
      "Epoch[1116/3000] loss: 0.0018042686182761908, acc: 0.999478874719731\n",
      "Epoch[1117/3000] loss: 0.0018048334960223957, acc: 0.9994657371076233\n",
      "Epoch[1118/3000] loss: 0.0018066395483409877, acc: 0.9994832539237668\n",
      "Epoch[1119/3000] loss: 0.0018054971906289693, acc: 0.9994832539237668\n",
      "Epoch[1120/3000] loss: 0.0018048486017260932, acc: 0.9994832539237668\n",
      "Epoch[1121/3000] loss: 0.0018305720085064247, acc: 0.9994561028587443\n",
      "Epoch[1122/3000] loss: 0.0018078930908340973, acc: 0.9994701163116592\n",
      "Epoch[1123/3000] loss: 0.0018045052653961635, acc: 0.9994876331278026\n",
      "Epoch[1124/3000] loss: 0.001801687800362812, acc: 0.9994744955156951\n",
      "Epoch[1125/3000] loss: 0.0018043276238943124, acc: 0.9994744955156951\n",
      "Epoch[1126/3000] loss: 0.001804075584722557, acc: 0.9994657371076233\n",
      "Epoch[1127/3000] loss: 0.0018053882055852073, acc: 0.9994832539237668\n",
      "Epoch[1128/3000] loss: 0.001804312943956489, acc: 0.9994701163116592\n",
      "Epoch[1129/3000] loss: 0.0018036956163441696, acc: 0.9994657371076233\n",
      "Epoch[1130/3000] loss: 0.001853798073028019, acc: 0.9994727438340807\n",
      "Epoch[1131/3000] loss: 0.001824953206211541, acc: 0.9994657371076233\n",
      "Epoch[1132/3000] loss: 0.001805787525865583, acc: 0.9994744955156951\n",
      "Epoch[1133/3000] loss: 0.001804880780554058, acc: 0.9994832539237668\n",
      "Epoch[1134/3000] loss: 0.0018025651605897807, acc: 0.9994701163116592\n",
      "Epoch[1135/3000] loss: 0.0018045317021803096, acc: 0.9994876331278026\n",
      "Epoch[1136/3000] loss: 0.0017991243270354376, acc: 0.9994744955156951\n",
      "Epoch[1137/3000] loss: 0.0018022208165152508, acc: 0.9994525994955157\n",
      "Epoch[1138/3000] loss: 0.001804522078780164, acc: 0.9994876331278026\n",
      "Epoch[1139/3000] loss: 0.0018017961344000547, acc: 0.999478874719731\n",
      "Epoch[1140/3000] loss: 0.001797908723791615, acc: 0.9994744955156951\n",
      "Epoch[1141/3000] loss: 0.0018023275443563012, acc: 0.999478874719731\n",
      "Epoch[1142/3000] loss: 0.0018021701438833528, acc: 0.9994701163116592\n",
      "Epoch[1143/3000] loss: 0.0018021278976913173, acc: 0.999478874719731\n",
      "Epoch[1144/3000] loss: 0.001801353796211193, acc: 0.9994613579035875\n",
      "Epoch[1145/3000] loss: 0.0017983349543154233, acc: 0.9994657371076233\n",
      "Epoch[1146/3000] loss: 0.0018027357557834692, acc: 0.999478874719731\n",
      "Epoch[1147/3000] loss: 0.0017968173969137398, acc: 0.9994525994955157\n",
      "Epoch[1148/3000] loss: 0.001799559006544789, acc: 0.9994701163116592\n",
      "Epoch[1149/3000] loss: 0.0017984062947535543, acc: 0.9994701163116592\n",
      "Epoch[1150/3000] loss: 0.0018023298112086292, acc: 0.999478874719731\n",
      "Epoch[1151/3000] loss: 0.0017986833588474022, acc: 0.9994832539237668\n",
      "Epoch[1152/3000] loss: 0.0018267551189180956, acc: 0.9994604820627803\n",
      "Epoch[1153/3000] loss: 0.0018032744003663016, acc: 0.9994701163116592\n",
      "Epoch[1154/3000] loss: 0.0017994464203343239, acc: 0.999478874719731\n",
      "Epoch[1155/3000] loss: 0.0017998429068168177, acc: 0.9994832539237668\n",
      "Epoch[1156/3000] loss: 0.0018007707823820428, acc: 0.9994657371076233\n",
      "Epoch[1157/3000] loss: 0.0017988171454201502, acc: 0.999478874719731\n",
      "Epoch[1158/3000] loss: 0.0018596332179043247, acc: 0.999463985426009\n",
      "Epoch[1159/3000] loss: 0.0018018007723580901, acc: 0.9994657371076233\n",
      "Epoch[1160/3000] loss: 0.001799028758955399, acc: 0.9994744955156951\n",
      "Epoch[1161/3000] loss: 0.0018023342717948057, acc: 0.9994876331278026\n",
      "Epoch[1162/3000] loss: 0.0017989341437158455, acc: 0.9994657371076233\n",
      "Epoch[1163/3000] loss: 0.001793589629728823, acc: 0.9994657371076233\n",
      "Epoch[1164/3000] loss: 0.0017972808960833558, acc: 0.9994613579035875\n",
      "Epoch[1165/3000] loss: 0.00179617787577908, acc: 0.9994832539237668\n",
      "Epoch[1166/3000] loss: 0.001798597806245373, acc: 0.9994657371076233\n",
      "Epoch[1167/3000] loss: 0.0017985263709343014, acc: 0.999478874719731\n",
      "Epoch[1168/3000] loss: 0.001796328539033397, acc: 0.9994701163116592\n",
      "Epoch[1169/3000] loss: 0.0017975932550074291, acc: 0.9994569786995515\n",
      "Epoch[1170/3000] loss: 0.0018481556672900847, acc: 0.9994823780829596\n",
      "Epoch[1171/3000] loss: 0.0017970967168725674, acc: 0.999478874719731\n",
      "Epoch[1172/3000] loss: 0.0017970774500226084, acc: 0.9994701163116592\n",
      "Epoch[1173/3000] loss: 0.0017960047700537386, acc: 0.9994876331278026\n",
      "Epoch[1174/3000] loss: 0.0017965341918056254, acc: 0.9994613579035875\n",
      "Epoch[1175/3000] loss: 0.0018461344133858411, acc: 0.999469240470852\n",
      "Epoch[1176/3000] loss: 0.0017966059453745876, acc: 0.9994613579035875\n",
      "Epoch[1177/3000] loss: 0.001798767847356944, acc: 0.9994920123318386\n",
      "Epoch[1178/3000] loss: 0.0017944442923898565, acc: 0.9994657371076233\n",
      "Epoch[1179/3000] loss: 0.001796510373501589, acc: 0.999478874719731\n",
      "Epoch[1180/3000] loss: 0.0017951446481324046, acc: 0.9994832539237668\n",
      "Epoch[1181/3000] loss: 0.0017945877001114448, acc: 0.9994744955156951\n",
      "Epoch[1182/3000] loss: 0.0017956204342328517, acc: 0.9994744955156951\n",
      "Epoch[1183/3000] loss: 0.0017912662386665964, acc: 0.9994701163116592\n",
      "Epoch[1184/3000] loss: 0.0017925410326005162, acc: 0.9994876331278026\n",
      "Epoch[1185/3000] loss: 0.0017973743965556612, acc: 0.9994701163116592\n",
      "Epoch[1186/3000] loss: 0.0017954837710874216, acc: 0.9994525994955157\n",
      "Epoch[1187/3000] loss: 0.0017914103700360049, acc: 0.9994657371076233\n",
      "Epoch[1188/3000] loss: 0.0017940108610090414, acc: 0.9994701163116592\n",
      "Epoch[1189/3000] loss: 0.0017943788589607542, acc: 0.9994876331278026\n",
      "Epoch[1190/3000] loss: 0.0017962135099446335, acc: 0.9994744955156951\n",
      "Epoch[1191/3000] loss: 0.0017949484948569627, acc: 0.9994657371076233\n",
      "Epoch[1192/3000] loss: 0.001794098965457438, acc: 0.9994744955156951\n",
      "Epoch[1193/3000] loss: 0.0017938653076793336, acc: 0.9994920123318386\n",
      "Epoch[1194/3000] loss: 0.0017944079802388268, acc: 0.9994701163116592\n",
      "Epoch[1195/3000] loss: 0.0017931748839615188, acc: 0.9994744955156951\n",
      "Epoch[1196/3000] loss: 0.0017905308363654843, acc: 0.999478874719731\n",
      "Epoch[1197/3000] loss: 0.0017918905676180714, acc: 0.999478874719731\n",
      "Epoch[1198/3000] loss: 0.0017949036562216526, acc: 0.9994744955156951\n",
      "Epoch[1199/3000] loss: 0.0017942002555234708, acc: 0.9994876331278026\n",
      "Epoch[1200/3000] loss: 0.0017897672630492033, acc: 0.9994876331278026\n",
      "Epoch[1201/3000] loss: 0.0017931442445614156, acc: 0.9994744955156951\n",
      "Epoch[1202/3000] loss: 0.00179114501693037, acc: 0.9994832539237668\n",
      "Epoch[1203/3000] loss: 0.0017905375101805077, acc: 0.9994744955156951\n",
      "Epoch[1204/3000] loss: 0.0017898960169187444, acc: 0.9994701163116592\n",
      "Epoch[1205/3000] loss: 0.0017922951833599752, acc: 0.999478874719731\n",
      "Epoch[1206/3000] loss: 0.0017873225659980974, acc: 0.9994920123318386\n",
      "Epoch[1207/3000] loss: 0.0017926959621965962, acc: 0.9994744955156951\n",
      "Epoch[1208/3000] loss: 0.001792469942326448, acc: 0.9994525994955157\n",
      "Epoch[1209/3000] loss: 0.0017919421004688194, acc: 0.9994832539237668\n",
      "Epoch[1210/3000] loss: 0.0017905523314122073, acc: 0.9994832539237668\n",
      "Epoch[1211/3000] loss: 0.0018234278259645143, acc: 0.999469240470852\n",
      "Epoch[1212/3000] loss: 0.0017921599552585172, acc: 0.9994744955156951\n",
      "Epoch[1213/3000] loss: 0.0017906008661279457, acc: 0.9994701163116592\n",
      "Epoch[1214/3000] loss: 0.0017915260569708507, acc: 0.9994701163116592\n",
      "Epoch[1215/3000] loss: 0.001787736049117578, acc: 0.9994657371076233\n",
      "Epoch[1216/3000] loss: 0.0017907799377429208, acc: 0.9994744955156951\n",
      "Epoch[1217/3000] loss: 0.0017889244911708424, acc: 0.9994920123318386\n",
      "Epoch[1218/3000] loss: 0.0017876905692844247, acc: 0.999478874719731\n",
      "Epoch[1219/3000] loss: 0.0017895984962824556, acc: 0.9994744955156951\n",
      "Epoch[1220/3000] loss: 0.0018112511639025181, acc: 0.9994744955156951\n",
      "Epoch[1221/3000] loss: 0.0017937293639238203, acc: 0.9994744955156951\n",
      "Epoch[1222/3000] loss: 0.0017919159802785556, acc: 0.999478874719731\n",
      "Epoch[1223/3000] loss: 0.0017868831889526898, acc: 0.9994744955156951\n",
      "Epoch[1224/3000] loss: 0.0017905757626385617, acc: 0.999478874719731\n",
      "Epoch[1225/3000] loss: 0.001790449520956525, acc: 0.999478874719731\n",
      "Epoch[1226/3000] loss: 0.0017875712663944102, acc: 0.9994657371076233\n",
      "Epoch[1227/3000] loss: 0.0017889474453488725, acc: 0.9994701163116592\n",
      "Epoch[1228/3000] loss: 0.0017899389223761635, acc: 0.9994832539237668\n",
      "Epoch[1229/3000] loss: 0.001787649517034618, acc: 0.999478874719731\n",
      "Epoch[1230/3000] loss: 0.0017876166989012113, acc: 0.9994657371076233\n",
      "Epoch[1231/3000] loss: 0.0017865811737730363, acc: 0.9994876331278026\n",
      "Epoch[1232/3000] loss: 0.0017890864287825033, acc: 0.999478874719731\n",
      "Epoch[1233/3000] loss: 0.0017890274670893053, acc: 0.999478874719731\n",
      "Epoch[1234/3000] loss: 0.0017879859521350643, acc: 0.9994701163116592\n",
      "Epoch[1235/3000] loss: 0.0017858761321302545, acc: 0.999478874719731\n",
      "Epoch[1236/3000] loss: 0.0017862115406722034, acc: 0.9994832539237668\n",
      "Epoch[1237/3000] loss: 0.001788548862876701, acc: 0.9994701163116592\n",
      "Epoch[1238/3000] loss: 0.0017898285197605161, acc: 0.999478874719731\n",
      "Epoch[1239/3000] loss: 0.0017893100812379427, acc: 0.9994920123318386\n",
      "Epoch[1240/3000] loss: 0.001787388017453956, acc: 0.9994832539237668\n",
      "Epoch[1241/3000] loss: 0.0017891143336705498, acc: 0.9994744955156951\n",
      "Epoch[1242/3000] loss: 0.001800421958968616, acc: 0.9994473444506726\n",
      "Epoch[1243/3000] loss: 0.0017866612966101268, acc: 0.9994876331278026\n",
      "Epoch[1244/3000] loss: 0.0017850108076667547, acc: 0.9994744955156951\n",
      "Epoch[1245/3000] loss: 0.0017883719753989775, acc: 0.9994744955156951\n",
      "Epoch[1246/3000] loss: 0.0017844066724667718, acc: 0.9994701163116592\n",
      "Epoch[1247/3000] loss: 0.0017882168975471042, acc: 0.9994832539237668\n",
      "Epoch[1248/3000] loss: 0.0017830716653423166, acc: 0.999478874719731\n",
      "Epoch[1249/3000] loss: 0.001786842441428471, acc: 0.9994832539237668\n",
      "Epoch[1250/3000] loss: 0.0017879799489153633, acc: 0.9994657371076233\n",
      "Epoch[1251/3000] loss: 0.0017842866121022597, acc: 0.999478874719731\n",
      "Epoch[1252/3000] loss: 0.0018140430013532229, acc: 0.9994648612668161\n",
      "Epoch[1253/3000] loss: 0.0017881513059311786, acc: 0.9994657371076233\n",
      "Epoch[1254/3000] loss: 0.0017863978715598476, acc: 0.9994876331278026\n",
      "Epoch[1255/3000] loss: 0.001786935652065615, acc: 0.999478874719731\n",
      "Epoch[1256/3000] loss: 0.001786663513697189, acc: 0.9994920123318386\n",
      "Epoch[1257/3000] loss: 0.0017852788562781356, acc: 0.9994657371076233\n",
      "Epoch[1258/3000] loss: 0.0017847166724960218, acc: 0.999478874719731\n",
      "Epoch[1259/3000] loss: 0.001784844634542346, acc: 0.9994832539237668\n",
      "Epoch[1260/3000] loss: 0.0017841782775429751, acc: 0.9994613579035875\n",
      "Epoch[1261/3000] loss: 0.0017850899204263707, acc: 0.9994920123318386\n",
      "Epoch[1262/3000] loss: 0.0017849317004783036, acc: 0.9994744955156951\n",
      "Epoch[1263/3000] loss: 0.0017866157291555201, acc: 0.9994832539237668\n",
      "Epoch[1264/3000] loss: 0.0017824752798447263, acc: 0.9994876331278026\n",
      "Epoch[1265/3000] loss: 0.0017837105471796915, acc: 0.9994613579035875\n",
      "Epoch[1266/3000] loss: 0.0017818192977205962, acc: 0.9994832539237668\n",
      "Epoch[1267/3000] loss: 0.0017853370195981236, acc: 0.9994744955156951\n",
      "Epoch[1268/3000] loss: 0.0017838487802190166, acc: 0.9994876331278026\n",
      "Epoch[1269/3000] loss: 0.001781840485655837, acc: 0.9994701163116592\n",
      "Epoch[1270/3000] loss: 0.0017866105826308866, acc: 0.9994876331278026\n",
      "Epoch[1271/3000] loss: 0.0017829260265364206, acc: 0.9994832539237668\n",
      "Epoch[1272/3000] loss: 0.0017841228476379405, acc: 0.999478874719731\n",
      "Epoch[1273/3000] loss: 0.0017839240167285273, acc: 0.9994876331278026\n",
      "Epoch[1274/3000] loss: 0.0017816390832541553, acc: 0.999478874719731\n",
      "Epoch[1275/3000] loss: 0.001780131022881795, acc: 0.999478874719731\n",
      "Epoch[1276/3000] loss: 0.0018100362627722914, acc: 0.9994736196748878\n",
      "Epoch[1277/3000] loss: 0.0017831995686589381, acc: 0.9994613579035875\n",
      "Epoch[1278/3000] loss: 0.0017815419533881835, acc: 0.9994832539237668\n",
      "Epoch[1279/3000] loss: 0.0017818040720410887, acc: 0.9994569786995515\n",
      "Epoch[1280/3000] loss: 0.0017842956272258231, acc: 0.9994876331278026\n",
      "Epoch[1281/3000] loss: 0.00177934878068589, acc: 0.9994876331278026\n",
      "Epoch[1282/3000] loss: 0.0017826131097984465, acc: 0.9994832539237668\n",
      "Epoch[1283/3000] loss: 0.0017804717999711243, acc: 0.9994832539237668\n",
      "Epoch[1284/3000] loss: 0.0017829420456582901, acc: 0.9994701163116592\n",
      "Epoch[1285/3000] loss: 0.001780399248232875, acc: 0.9994832539237668\n",
      "Epoch[1286/3000] loss: 0.0017809034298019951, acc: 0.999478874719731\n",
      "Epoch[1287/3000] loss: 0.001781249506782051, acc: 0.999478874719731\n",
      "Epoch[1288/3000] loss: 0.0017820410869354324, acc: 0.9994744955156951\n",
      "Epoch[1289/3000] loss: 0.0018069201505568344, acc: 0.9994779988789237\n",
      "Epoch[1290/3000] loss: 0.0017790873156619483, acc: 0.999478874719731\n",
      "Epoch[1291/3000] loss: 0.001783163628633987, acc: 0.9994920123318386\n",
      "Epoch[1292/3000] loss: 0.0018017222139660833, acc: 0.9994876331278026\n",
      "Epoch[1293/3000] loss: 0.0017794242388592442, acc: 0.9994920123318386\n",
      "Epoch[1294/3000] loss: 0.001780266682854569, acc: 0.9994701163116592\n",
      "Epoch[1295/3000] loss: 0.0017823107860472252, acc: 0.9994832539237668\n",
      "Epoch[1296/3000] loss: 0.0017814002877935002, acc: 0.9994920123318386\n",
      "Epoch[1297/3000] loss: 0.001826540933339987, acc: 0.9994604820627803\n",
      "Epoch[1298/3000] loss: 0.0018287489780384669, acc: 0.9994648612668161\n",
      "Epoch[1299/3000] loss: 0.0017815447911829479, acc: 0.9994832539237668\n",
      "Epoch[1300/3000] loss: 0.0017774291937210755, acc: 0.9994832539237668\n",
      "Epoch[1301/3000] loss: 0.0017802565370210902, acc: 0.9994876331278026\n",
      "Epoch[1302/3000] loss: 0.0017805036734046353, acc: 0.9994876331278026\n",
      "Epoch[1303/3000] loss: 0.0017766256816293669, acc: 0.9994920123318386\n",
      "Epoch[1304/3000] loss: 0.001777239932563856, acc: 0.999478874719731\n",
      "Epoch[1305/3000] loss: 0.0017786384912546005, acc: 0.999478874719731\n",
      "Epoch[1306/3000] loss: 0.0017780092054101766, acc: 0.9994920123318386\n",
      "Epoch[1307/3000] loss: 0.0017802733111948572, acc: 0.9994744955156951\n",
      "Epoch[1308/3000] loss: 0.0017785021338380578, acc: 0.9994701163116592\n",
      "Epoch[1309/3000] loss: 0.0017785080366625947, acc: 0.9994876331278026\n",
      "Epoch[1310/3000] loss: 0.0017797122253196728, acc: 0.9994920123318386\n",
      "Epoch[1311/3000] loss: 0.0017789911364871176, acc: 0.9994744955156951\n",
      "Epoch[1312/3000] loss: 0.0017772796698311483, acc: 0.9994701163116592\n",
      "Epoch[1313/3000] loss: 0.0017802888084013568, acc: 0.9994832539237668\n",
      "Epoch[1314/3000] loss: 0.0017788818483573051, acc: 0.9994744955156951\n",
      "Epoch[1315/3000] loss: 0.0017778925569151161, acc: 0.9994876331278026\n",
      "Epoch[1316/3000] loss: 0.0017788903798336099, acc: 0.999478874719731\n",
      "Epoch[1317/3000] loss: 0.0017778270508447124, acc: 0.999478874719731\n",
      "Epoch[1318/3000] loss: 0.0017780398226316602, acc: 0.9994876331278026\n",
      "Epoch[1319/3000] loss: 0.0017767741880557438, acc: 0.9994920123318386\n",
      "Epoch[1320/3000] loss: 0.0017800618412537315, acc: 0.9994744955156951\n",
      "Epoch[1321/3000] loss: 0.001776534244160223, acc: 0.9994876331278026\n",
      "Epoch[1322/3000] loss: 0.001779418333126771, acc: 0.9994920123318386\n",
      "Epoch[1323/3000] loss: 0.001778512176193063, acc: 0.9994876331278026\n",
      "Epoch[1324/3000] loss: 0.0017763537028181234, acc: 0.9994832539237668\n",
      "Epoch[1325/3000] loss: 0.0017771141280632792, acc: 0.9994832539237668\n",
      "Epoch[1326/3000] loss: 0.0017750790437830678, acc: 0.9994701163116592\n",
      "Epoch[1327/3000] loss: 0.0017762485972546002, acc: 0.999478874719731\n",
      "Epoch[1328/3000] loss: 0.0018058438724556296, acc: 0.9994736196748878\n",
      "Epoch[1329/3000] loss: 0.0017771639394852637, acc: 0.9994876331278026\n",
      "Epoch[1330/3000] loss: 0.0017742381162131377, acc: 0.9994613579035875\n",
      "Epoch[1331/3000] loss: 0.0017772279287672904, acc: 0.9994876331278026\n",
      "Epoch[1332/3000] loss: 0.0018023978728286343, acc: 0.9994779988789237\n",
      "Epoch[1333/3000] loss: 0.0018025235278702747, acc: 0.9994823780829596\n",
      "Epoch[1334/3000] loss: 0.0017762152293703916, acc: 0.9994876331278026\n",
      "Epoch[1335/3000] loss: 0.0017763816043944673, acc: 0.999478874719731\n",
      "Epoch[1336/3000] loss: 0.0017741037256992017, acc: 0.9994920123318386\n",
      "Epoch[1337/3000] loss: 0.001773639192462659, acc: 0.9994832539237668\n",
      "Epoch[1338/3000] loss: 0.0017771961192431158, acc: 0.9994876331278026\n",
      "Epoch[1339/3000] loss: 0.001777134479377591, acc: 0.999478874719731\n",
      "Epoch[1340/3000] loss: 0.0017763873624298948, acc: 0.9994744955156951\n",
      "Epoch[1341/3000] loss: 0.0017735090307850455, acc: 0.9994701163116592\n",
      "Epoch[1342/3000] loss: 0.001776539675691131, acc: 0.9994920123318386\n",
      "Epoch[1343/3000] loss: 0.00180143898025171, acc: 0.9994736196748878\n",
      "Epoch[1344/3000] loss: 0.0017762302504421403, acc: 0.999478874719731\n",
      "Epoch[1345/3000] loss: 0.0017756583900401665, acc: 0.9994701163116592\n",
      "Epoch[1346/3000] loss: 0.0017748539306791373, acc: 0.9994876331278026\n",
      "Epoch[1347/3000] loss: 0.0017766389397210875, acc: 0.999478874719731\n",
      "Epoch[1348/3000] loss: 0.001816095325843706, acc: 0.9994779988789237\n",
      "Epoch[1349/3000] loss: 0.0017726246657644724, acc: 0.999478874719731\n",
      "Epoch[1350/3000] loss: 0.0017748610876112208, acc: 0.9994832539237668\n",
      "Epoch[1351/3000] loss: 0.0017747397350056401, acc: 0.9994876331278026\n",
      "Epoch[1352/3000] loss: 0.0017754720811639256, acc: 0.9994744955156951\n",
      "Epoch[1353/3000] loss: 0.0017726808624958942, acc: 0.9994701163116592\n",
      "Epoch[1354/3000] loss: 0.0017724515925403115, acc: 0.9994876331278026\n",
      "Epoch[1355/3000] loss: 0.0017740486169396013, acc: 0.9994832539237668\n",
      "Epoch[1356/3000] loss: 0.0017750959646872873, acc: 0.999478874719731\n",
      "Epoch[1357/3000] loss: 0.0017745572537031847, acc: 0.9994876331278026\n",
      "Epoch[1358/3000] loss: 0.0017723491375053703, acc: 0.9994876331278026\n",
      "Epoch[1359/3000] loss: 0.0017726258749234038, acc: 0.9994876331278026\n",
      "Epoch[1360/3000] loss: 0.0017744658747475856, acc: 0.999478874719731\n",
      "Epoch[1361/3000] loss: 0.0017734130927201152, acc: 0.9994876331278026\n",
      "Epoch[1362/3000] loss: 0.0017735738101773391, acc: 0.9994920123318386\n",
      "Epoch[1363/3000] loss: 0.001773717028683245, acc: 0.9994744955156951\n",
      "Epoch[1364/3000] loss: 0.0017723964385302907, acc: 0.999478874719731\n",
      "Epoch[1365/3000] loss: 0.0017708815490069221, acc: 0.9994832539237668\n",
      "Epoch[1366/3000] loss: 0.0017745752540756538, acc: 0.9994876331278026\n",
      "Epoch[1367/3000] loss: 0.001774127165551481, acc: 0.9994744955156951\n",
      "Epoch[1368/3000] loss: 0.0017721337599834743, acc: 0.999478874719731\n",
      "Epoch[1369/3000] loss: 0.0017716404503060183, acc: 0.9994920123318386\n",
      "Epoch[1370/3000] loss: 0.0017739236361802033, acc: 0.9994920123318386\n",
      "Epoch[1371/3000] loss: 0.0017706271783067536, acc: 0.999478874719731\n",
      "Epoch[1372/3000] loss: 0.0017741465847805479, acc: 0.9994832539237668\n",
      "Epoch[1373/3000] loss: 0.0017725764786592406, acc: 0.9994832539237668\n",
      "Epoch[1374/3000] loss: 0.0017711897716388178, acc: 0.9994876331278026\n",
      "Epoch[1375/3000] loss: 0.0017723654235374249, acc: 0.9994876331278026\n",
      "Epoch[1376/3000] loss: 0.0017713555383251345, acc: 0.9994920123318386\n",
      "Epoch[1377/3000] loss: 0.0017717429406888994, acc: 0.9994832539237668\n",
      "Epoch[1378/3000] loss: 0.001771473043988363, acc: 0.9994832539237668\n",
      "Epoch[1379/3000] loss: 0.0017720782413109411, acc: 0.9994920123318386\n",
      "Epoch[1380/3000] loss: 0.0017708669065215196, acc: 0.9994876331278026\n",
      "Epoch[1381/3000] loss: 0.0017710844889685347, acc: 0.9994832539237668\n",
      "Epoch[1382/3000] loss: 0.0017704263529203224, acc: 0.9994920123318386\n",
      "Epoch[1383/3000] loss: 0.0017702924740970781, acc: 0.999478874719731\n",
      "Epoch[1384/3000] loss: 0.0017702019297150253, acc: 0.999478874719731\n",
      "Epoch[1385/3000] loss: 0.0018201579196183671, acc: 0.9994823780829596\n",
      "Epoch[1386/3000] loss: 0.0017701228523318985, acc: 0.9994832539237668\n",
      "Epoch[1387/3000] loss: 0.0017707148065796146, acc: 0.9994876331278026\n",
      "Epoch[1388/3000] loss: 0.0017715548311599731, acc: 0.9994876331278026\n",
      "Epoch[1389/3000] loss: 0.001770602784908417, acc: 0.9994920123318386\n",
      "Epoch[1390/3000] loss: 0.0017715266952072527, acc: 0.9994876331278026\n",
      "Epoch[1391/3000] loss: 0.001771075397977615, acc: 0.9994832539237668\n",
      "Epoch[1392/3000] loss: 0.0017709138293026839, acc: 0.9994832539237668\n",
      "Epoch[1393/3000] loss: 0.0017698865016827277, acc: 0.9994832539237668\n",
      "Epoch[1394/3000] loss: 0.0018057141859910367, acc: 0.9994779988789237\n",
      "Epoch[1395/3000] loss: 0.0017963081951133178, acc: 0.9994736196748878\n",
      "Epoch[1396/3000] loss: 0.00176956639166551, acc: 0.9994920123318386\n",
      "Epoch[1397/3000] loss: 0.001771241866002622, acc: 0.9994876331278026\n",
      "Epoch[1398/3000] loss: 0.0017689621251067585, acc: 0.999478874719731\n",
      "Epoch[1399/3000] loss: 0.0017679645321232078, acc: 0.9994832539237668\n",
      "Epoch[1400/3000] loss: 0.001769807101167961, acc: 0.999478874719731\n",
      "Epoch[1401/3000] loss: 0.001768674396310665, acc: 0.999478874719731\n",
      "Epoch[1402/3000] loss: 0.001766981435155628, acc: 0.9994657371076233\n",
      "Epoch[1403/3000] loss: 0.0017682821069953175, acc: 0.9994744955156951\n",
      "Epoch[1404/3000] loss: 0.001769560749033948, acc: 0.9994920123318386\n",
      "Epoch[1405/3000] loss: 0.001765370844888348, acc: 0.9994832539237668\n",
      "Epoch[1406/3000] loss: 0.001769652371416008, acc: 0.9994657371076233\n",
      "Epoch[1407/3000] loss: 0.0017666200951823112, acc: 0.999478874719731\n",
      "Epoch[1408/3000] loss: 0.0018060789923953304, acc: 0.999469240470852\n",
      "Epoch[1409/3000] loss: 0.0017935558951102488, acc: 0.9994779988789237\n",
      "Epoch[1410/3000] loss: 0.0017693053473787112, acc: 0.9994876331278026\n",
      "Epoch[1411/3000] loss: 0.001769302208618691, acc: 0.9994832539237668\n",
      "Epoch[1412/3000] loss: 0.0017679185852584075, acc: 0.9994832539237668\n",
      "Epoch[1413/3000] loss: 0.0017686940590599973, acc: 0.9994920123318386\n",
      "Epoch[1414/3000] loss: 0.0017677750170375145, acc: 0.9994701163116592\n",
      "Epoch[1415/3000] loss: 0.0017689190025370481, acc: 0.9994876331278026\n",
      "Epoch[1416/3000] loss: 0.0017675731662345523, acc: 0.9994920123318386\n",
      "Epoch[1417/3000] loss: 0.0017671357043198873, acc: 0.9994832539237668\n",
      "Epoch[1418/3000] loss: 0.001766629741454413, acc: 0.9994744955156951\n",
      "Epoch[1419/3000] loss: 0.001766962940658358, acc: 0.9994744955156951\n",
      "Epoch[1420/3000] loss: 0.0018149811354109945, acc: 0.9994779988789237\n",
      "Epoch[1421/3000] loss: 0.001766356979319446, acc: 0.999478874719731\n",
      "Epoch[1422/3000] loss: 0.0017680536740399598, acc: 0.9994920123318386\n",
      "Epoch[1423/3000] loss: 0.0017668267139398744, acc: 0.9994920123318386\n",
      "Epoch[1424/3000] loss: 0.0017672751315216772, acc: 0.9994920123318386\n",
      "Epoch[1425/3000] loss: 0.0017657751951923877, acc: 0.9994920123318386\n",
      "Epoch[1426/3000] loss: 0.0017643492084824688, acc: 0.9994963915358744\n",
      "Epoch[1427/3000] loss: 0.001766460416090702, acc: 0.9994832539237668\n",
      "Epoch[1428/3000] loss: 0.0017921848103416508, acc: 0.999469240470852\n",
      "Epoch[1429/3000] loss: 0.0017657839014752054, acc: 0.999478874719731\n",
      "Epoch[1430/3000] loss: 0.0017663299101142846, acc: 0.9994876331278026\n",
      "Epoch[1431/3000] loss: 0.0017664386464459124, acc: 0.9994920123318386\n",
      "Epoch[1432/3000] loss: 0.0017671288053498662, acc: 0.9994701163116592\n",
      "Epoch[1433/3000] loss: 0.0017657388456338, acc: 0.9994876331278026\n",
      "Epoch[1434/3000] loss: 0.0017663728147600919, acc: 0.999478874719731\n",
      "Epoch[1435/3000] loss: 0.0017630168433871024, acc: 0.9994876331278026\n",
      "Epoch[1436/3000] loss: 0.001768642716293642, acc: 0.999478874719731\n",
      "Epoch[1437/3000] loss: 0.001765965703686924, acc: 0.9994701163116592\n",
      "Epoch[1438/3000] loss: 0.001763850777403641, acc: 0.9994920123318386\n",
      "Epoch[1439/3000] loss: 0.001765576620935854, acc: 0.9994701163116592\n",
      "Epoch[1440/3000] loss: 0.0017659026795541495, acc: 0.9994876331278026\n",
      "Epoch[1441/3000] loss: 0.0017622719459368331, acc: 0.9994657371076233\n",
      "Epoch[1442/3000] loss: 0.0017646301619264084, acc: 0.9994876331278026\n",
      "Epoch[1443/3000] loss: 0.0017657383546371714, acc: 0.9994876331278026\n",
      "Epoch[1444/3000] loss: 0.0017648305507227552, acc: 0.9994920123318386\n",
      "Epoch[1445/3000] loss: 0.001763773412819274, acc: 0.999478874719731\n",
      "Epoch[1446/3000] loss: 0.0017658753482880818, acc: 0.9994876331278026\n",
      "Epoch[1447/3000] loss: 0.0017634361916584627, acc: 0.9994701163116592\n",
      "Epoch[1448/3000] loss: 0.0017655542541690475, acc: 0.9994920123318386\n",
      "Epoch[1449/3000] loss: 0.001760781461496565, acc: 0.9994920123318386\n",
      "Epoch[1450/3000] loss: 0.0017638074988739422, acc: 0.9994701163116592\n",
      "Epoch[1451/3000] loss: 0.0017657574815880845, acc: 0.9994920123318386\n",
      "Epoch[1452/3000] loss: 0.0017941183542775699, acc: 0.9994779988789237\n",
      "Epoch[1453/3000] loss: 0.0017645357365183836, acc: 0.999478874719731\n",
      "Epoch[1454/3000] loss: 0.0017634185938046937, acc: 0.999478874719731\n",
      "Epoch[1455/3000] loss: 0.0017635636175311505, acc: 0.9994876331278026\n",
      "Epoch[1456/3000] loss: 0.00176394116713081, acc: 0.9994920123318386\n",
      "Epoch[1457/3000] loss: 0.0017646632197585712, acc: 0.9994876331278026\n",
      "Epoch[1458/3000] loss: 0.0017655595810795158, acc: 0.9994876331278026\n",
      "Epoch[1459/3000] loss: 0.001762501503794391, acc: 0.9994832539237668\n",
      "Epoch[1460/3000] loss: 0.0017632640342220545, acc: 0.999478874719731\n",
      "Epoch[1461/3000] loss: 0.001764585325852366, acc: 0.9994832539237668\n",
      "Epoch[1462/3000] loss: 0.0017638008140430214, acc: 0.999478874719731\n",
      "Epoch[1463/3000] loss: 0.0017641576040794446, acc: 0.9994876331278026\n",
      "Epoch[1464/3000] loss: 0.0017627554762256062, acc: 0.9994832539237668\n",
      "Epoch[1465/3000] loss: 0.0017636312100354197, acc: 0.9994920123318386\n",
      "Epoch[1466/3000] loss: 0.0017638304547608437, acc: 0.9994920123318386\n",
      "Epoch[1467/3000] loss: 0.0017635515020951297, acc: 0.9994832539237668\n",
      "Epoch[1468/3000] loss: 0.0017615351872936292, acc: 0.999478874719731\n",
      "Epoch[1469/3000] loss: 0.001764019443848637, acc: 0.9994920123318386\n",
      "Epoch[1470/3000] loss: 0.0017626091258644988, acc: 0.9994876331278026\n",
      "Epoch[1471/3000] loss: 0.0017635768419267041, acc: 0.9994876331278026\n",
      "Epoch[1472/3000] loss: 0.0017729288609906904, acc: 0.999478874719731\n",
      "Epoch[1473/3000] loss: 0.0017619106824793465, acc: 0.9994876331278026\n",
      "Epoch[1474/3000] loss: 0.0017617381271614375, acc: 0.9994832539237668\n",
      "Epoch[1475/3000] loss: 0.001760364183010831, acc: 0.9994920123318386\n",
      "Epoch[1476/3000] loss: 0.0017727421795315842, acc: 0.999478874719731\n",
      "Epoch[1477/3000] loss: 0.001757677371948669, acc: 0.9994832539237668\n",
      "Epoch[1478/3000] loss: 0.0017630901920874177, acc: 0.9994569786995515\n",
      "Epoch[1479/3000] loss: 0.0017627135116914368, acc: 0.9994832539237668\n",
      "Epoch[1480/3000] loss: 0.0017618754610694064, acc: 0.9994920123318386\n",
      "Epoch[1481/3000] loss: 0.001761194295314319, acc: 0.9994920123318386\n",
      "Epoch[1482/3000] loss: 0.0017610755492389846, acc: 0.9994744955156951\n",
      "Epoch[1483/3000] loss: 0.0017602479180452, acc: 0.9994832539237668\n",
      "Epoch[1484/3000] loss: 0.001789400872403022, acc: 0.9994779988789237\n",
      "Epoch[1485/3000] loss: 0.0017618448122684721, acc: 0.9994920123318386\n",
      "Epoch[1486/3000] loss: 0.001762346181957309, acc: 0.9994657371076233\n",
      "Epoch[1487/3000] loss: 0.0017616145842519863, acc: 0.9994744955156951\n",
      "Epoch[1488/3000] loss: 0.0017608248070189092, acc: 0.9994920123318386\n",
      "Epoch[1489/3000] loss: 0.0017569635241988886, acc: 0.9994920123318386\n",
      "Epoch[1490/3000] loss: 0.0017870276118171894, acc: 0.999469240470852\n",
      "Epoch[1491/3000] loss: 0.0017613721443277862, acc: 0.9994832539237668\n",
      "Epoch[1492/3000] loss: 0.0017606442432616398, acc: 0.9994832539237668\n",
      "Epoch[1493/3000] loss: 0.0017599419088297598, acc: 0.9994920123318386\n",
      "Epoch[1494/3000] loss: 0.0017602357016715813, acc: 0.9994832539237668\n",
      "Epoch[1495/3000] loss: 0.0017606115223748327, acc: 0.9994832539237668\n",
      "Epoch[1496/3000] loss: 0.001759377039277164, acc: 0.9994832539237668\n",
      "Epoch[1497/3000] loss: 0.001757373280441553, acc: 0.9994920123318386\n",
      "Epoch[1498/3000] loss: 0.0017618342869106034, acc: 0.9994569786995515\n",
      "Epoch[1499/3000] loss: 0.0017826638542314847, acc: 0.9994823780829596\n",
      "Epoch[1500/3000] loss: 0.0017602871212841211, acc: 0.999478874719731\n",
      "Epoch[1501/3000] loss: 0.0017843917284492087, acc: 0.9994779988789237\n",
      "Epoch[1502/3000] loss: 0.0017607454059739437, acc: 0.9994832539237668\n",
      "Epoch[1503/3000] loss: 0.001758251421936706, acc: 0.9994876331278026\n",
      "Epoch[1504/3000] loss: 0.0017594471335852001, acc: 0.9994832539237668\n",
      "Epoch[1505/3000] loss: 0.0017600488753091688, acc: 0.9994876331278026\n",
      "Epoch[1506/3000] loss: 0.0017582478997410606, acc: 0.9994876331278026\n",
      "Epoch[1507/3000] loss: 0.0017743035041348776, acc: 0.999469240470852\n",
      "Epoch[1508/3000] loss: 0.0017596066065151607, acc: 0.9994832539237668\n",
      "Epoch[1509/3000] loss: 0.0017583061640761373, acc: 0.9994876331278026\n",
      "Epoch[1510/3000] loss: 0.0017585650293237486, acc: 0.999478874719731\n",
      "Epoch[1511/3000] loss: 0.0017582974231468735, acc: 0.9994876331278026\n",
      "Epoch[1512/3000] loss: 0.001756950772063557, acc: 0.9994832539237668\n",
      "Epoch[1513/3000] loss: 0.001783620228201422, acc: 0.9994779988789237\n",
      "Epoch[1514/3000] loss: 0.001759295763229341, acc: 0.9994832539237668\n",
      "Epoch[1515/3000] loss: 0.0017583682215995741, acc: 0.9994876331278026\n",
      "Epoch[1516/3000] loss: 0.0017584491245180194, acc: 0.9994832539237668\n",
      "Epoch[1517/3000] loss: 0.0017600613937777743, acc: 0.9994876331278026\n",
      "Epoch[1518/3000] loss: 0.0017573923967069227, acc: 0.9994701163116592\n",
      "Epoch[1519/3000] loss: 0.0017941404381140334, acc: 0.9994736196748878\n",
      "Epoch[1520/3000] loss: 0.0017599375094040433, acc: 0.9994832539237668\n",
      "Epoch[1521/3000] loss: 0.0017568010392345552, acc: 0.9994744955156951\n",
      "Epoch[1522/3000] loss: 0.0017581246650891366, acc: 0.9994963915358744\n",
      "Epoch[1523/3000] loss: 0.001807897983882965, acc: 0.9994596062219732\n",
      "Epoch[1524/3000] loss: 0.0017590622382333844, acc: 0.9994832539237668\n",
      "Epoch[1525/3000] loss: 0.001759004482440435, acc: 0.999478874719731\n",
      "Epoch[1526/3000] loss: 0.0017847969354523948, acc: 0.9994736196748878\n",
      "Epoch[1527/3000] loss: 0.001758784463864959, acc: 0.9994920123318386\n",
      "Epoch[1528/3000] loss: 0.001757232558241661, acc: 0.9994876331278026\n",
      "Epoch[1529/3000] loss: 0.0017572843740105166, acc: 0.999478874719731\n",
      "Epoch[1530/3000] loss: 0.0017578924086955434, acc: 0.9994920123318386\n",
      "Epoch[1531/3000] loss: 0.0017926244150299117, acc: 0.9994823780829596\n",
      "Epoch[1532/3000] loss: 0.0017573223269511653, acc: 0.9994832539237668\n",
      "Epoch[1533/3000] loss: 0.0017561429861298124, acc: 0.999478874719731\n",
      "Epoch[1534/3000] loss: 0.0017541274822019761, acc: 0.9994876331278026\n",
      "Epoch[1535/3000] loss: 0.0017563988800256055, acc: 0.999478874719731\n",
      "Epoch[1536/3000] loss: 0.0017842738863789803, acc: 0.9994823780829596\n",
      "Epoch[1537/3000] loss: 0.0017587716070398536, acc: 0.9994920123318386\n",
      "Epoch[1538/3000] loss: 0.0017558423288910041, acc: 0.9994832539237668\n",
      "Epoch[1539/3000] loss: 0.0017569439445847136, acc: 0.999478874719731\n",
      "Epoch[1540/3000] loss: 0.0017563913236866807, acc: 0.9994920123318386\n",
      "Epoch[1541/3000] loss: 0.0017567579522493348, acc: 0.9994920123318386\n",
      "Epoch[1542/3000] loss: 0.001756281190155033, acc: 0.9994920123318386\n",
      "Epoch[1543/3000] loss: 0.0017567435368072532, acc: 0.9994920123318386\n",
      "Epoch[1544/3000] loss: 0.00175468203962223, acc: 0.9994701163116592\n",
      "Epoch[1545/3000] loss: 0.0017825413320157176, acc: 0.9994823780829596\n",
      "Epoch[1546/3000] loss: 0.0017558534738513149, acc: 0.9994832539237668\n",
      "Epoch[1547/3000] loss: 0.0017564302971293264, acc: 0.9994832539237668\n",
      "Epoch[1548/3000] loss: 0.0017574455910168822, acc: 0.9994876331278026\n",
      "Epoch[1549/3000] loss: 0.0017568364408916627, acc: 0.9994920123318386\n",
      "Epoch[1550/3000] loss: 0.0017843844749434706, acc: 0.9994648612668161\n",
      "Epoch[1551/3000] loss: 0.0017569515397219482, acc: 0.9994920123318386\n",
      "Epoch[1552/3000] loss: 0.0017547841560355722, acc: 0.9994876331278026\n",
      "Epoch[1553/3000] loss: 0.0017559142946985077, acc: 0.9994920123318386\n",
      "Epoch[1554/3000] loss: 0.0017555590351907649, acc: 0.9994876331278026\n",
      "Epoch[1555/3000] loss: 0.0017765396789457355, acc: 0.9994832539237668\n",
      "Epoch[1556/3000] loss: 0.001756624064804244, acc: 0.9994920123318386\n",
      "Epoch[1557/3000] loss: 0.0017558429600803272, acc: 0.9994920123318386\n",
      "Epoch[1558/3000] loss: 0.0017544174924675681, acc: 0.9994920123318386\n",
      "Epoch[1559/3000] loss: 0.001755072189761176, acc: 0.9994920123318386\n",
      "Epoch[1560/3000] loss: 0.0017542844456973873, acc: 0.999478874719731\n",
      "Epoch[1561/3000] loss: 0.0017545896497612307, acc: 0.9994920123318386\n",
      "Epoch[1562/3000] loss: 0.001755478650011355, acc: 0.9994920123318386\n",
      "Epoch[1563/3000] loss: 0.0017548008238759496, acc: 0.9994920123318386\n",
      "Epoch[1564/3000] loss: 0.0017534777554478159, acc: 0.9994832539237668\n",
      "Epoch[1565/3000] loss: 0.0017570073790260305, acc: 0.999478874719731\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m train_acc \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain(mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m i_batch, (image, label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      8\u001b[0m     image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\C109154330\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\C109154330\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\C109154330\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\C109154330\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    169\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\C109154330\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    169\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\C109154330\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:138\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    136\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel)\n\u001b[0;32m    137\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m    139\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    140\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    142\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if training_choice == True:\n",
    "    metric = {'loss': [], 'acc': []}\n",
    "    for i_epoch in range(epoch):\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        model.train(mode=True)\n",
    "        for i_batch, (image, label) in enumerate(dataloader):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            pred = model.forward(image)  # inference\n",
    "\n",
    "            loss = criterion(pred, label)  # calculate loss\n",
    "            optimizer.zero_grad()  # reset gradient to zero\n",
    "            loss.backward()  # calculate gradient\n",
    "            optimizer.step()  # optimize weight (using gradient)\n",
    "\n",
    "            train_loss += [loss.item()]\n",
    "            train_acc += [accuracy(pred, label)]\n",
    "\n",
    "        metric['loss'] += [sum(train_loss)/ len(dataloader)]\n",
    "        metric['acc'] += [sum(train_acc)/ len(dataloader)]\n",
    "        print(f'Epoch[{i_epoch+1}/{epoch}] loss: {metric[\"loss\"][-1]}, acc: {metric[\"acc\"][-1]}')\n",
    "        if metric[\"acc\"][-1] >= 1.0:  # 设定一个接近1的阈值\n",
    "            print(f'Reached desired accuracy. Stopping training.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if training_choice == True:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax1.plot(range(len(metric[\"loss\"])), metric[\"loss\"])\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax2.plot(range(len(metric[\"acc\"])), metric[\"acc\"])\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(output_path)\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.jit.script(model).save(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = Path('./model/hira.pt')  # TODO: adjust path\n",
    "# assert model_path.exists() is True\n",
    "# image_path = Path(f'./{DATASET_PATH}/99.jpg')  # TODO: adjust path\n",
    "# assert image_path.exists() is True\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((83, 84)),  # TODO: adjust img_size\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# trained_model = torch.jit.load(model_path)\n",
    "# test_image = Image.open(image_path)\n",
    "# trans_image = img_convert(test_image)\n",
    "# # trans_image = transform(test_image)\n",
    "\n",
    "# pred = trained_model(trans_image.unsqueeze(0))\n",
    "# pred_label = pred.max(1)[1]\n",
    "\n",
    "# print(pred_label)\n",
    "# test_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ttttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def img_convert(input: np.ndarray) -> torch.Tensor: #回傳的是torch.Tensor，用於測試\n",
    "    # 将图像转换为灰度影像\n",
    "    gray_image = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY)\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(gray_image, (83, 84))\n",
    "\n",
    "    equalized_image = cv2.equalizeHist(resized_image)\n",
    "\n",
    "    # 多次侵蚀后膨胀\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    eroded_image = cv2.erode(equalized_image, kernel, iterations=2)\n",
    "    dilated_image = cv2.dilate(eroded_image, kernel, iterations=2)\n",
    "\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(dilated_image, (83, 84))\n",
    "\n",
    "    # 将图像转换为张量并调整形状\n",
    "    tensor_image = transforms.ToTensor()(resized_image)\n",
    "    # tensor_image = torch.unsqueeze(tensor_image, 0)\n",
    "\n",
    "    return tensor_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=CNN\n",
       "  (cnn1): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (relu1): RecursiveScriptModule(original_name=ReLU)\n",
       "  (maxpool1): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (cnn2): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (relu2): RecursiveScriptModule(original_name=ReLU)\n",
       "  (maxpool2): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (hidden_layer): RecursiveScriptModule(original_name=Linear)\n",
       "  (fc1): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\",device)\n",
    "\n",
    "model_path = Path(output_path, map_location=device)  # 請調整路徑\n",
    "assert model_path.exists() is True\n",
    "\n",
    "trained_model = torch.jit.load(model_path, map_location=device).to(device)\n",
    "\n",
    "label_dict = {\n",
    "    0: 'A', 1: 'BA', 2: 'CHI', 3: 'DA', 4: 'E', 5: 'FU', 6: 'HA', 7: 'HE', 8: 'HI', 9: 'HO', 10: 'I', 11: 'JI',\n",
    "    12: 'KA', 13: 'KE', 14: 'KI', 15: 'KO', 16: 'KU', 17: 'MA', 18: 'ME', 19: 'MI', 20: 'MO', 21: 'MU', 22: 'N',\n",
    "    23: 'NA', 24: 'NE', 25: 'NI', 26: 'NO', 27: 'NU', 28: 'O', 29: 'PI', 30: 'RA', 31: 'RE', 32: 'RI', 33: 'RO',\n",
    "    34: 'RU', 35: 'SA', 36: 'SE', 37: 'SHI', 38: 'SO', 39: 'SU', 40: 'TA', 41: 'TE', 42: 'TO', 43: 'TSU', 44: 'U',\n",
    "    45: 'WA', 46: 'WO', 47: 'YA', 48: 'YO', 49: 'YU'\n",
    "}\n",
    "\n",
    "trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test path set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = 'fdata'\n",
    "# test_data_path = 'data_new'\n",
    "# test_data_path = 'data_color'\n",
    "# test_data_path = 'data_color2'\n",
    "# test_data_path = 'test_color'\n",
    "test_data_path = 'testdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测错误的图像：testdata/145.jpg\t真实标签：HE\t预测标签：WA\n",
      "-------------------------------------\n",
      "预测错误的图像：testdata/219.jpg\t真实标签：I\t预测标签：SHI\n",
      "-------------------------------------\n",
      "Accuracy: 99.80%\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(f'{test_data_path}/label.txt', mode='r') as f:\n",
    "    labels = f.read().split('\\n')\n",
    "labels = [label_dict[int(label)] for label in labels]\n",
    "\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    image_path = f'{test_data_path}/{i}.jpg'  # 依據預設命名規則為每個圖片建立對應的路徑\n",
    "    if os.path.exists(image_path):\n",
    "        test_image = cv2.imread(image_path)  # 使用 cv2 讀取圖片\n",
    "        converted_image = img_convert(test_image)  # 將圖片轉換成您需要的格式\n",
    "        # trans_image = transform(converted_image)  # 應用其他轉換（如果需要）\n",
    "        # pred = trained_model(trans_image.unsqueeze(0))\n",
    "        pred = trained_model(converted_image.unsqueeze(0).to(device))\n",
    "        pred_label = label_dict[pred.max(1)[1].item()]\n",
    "        predictions.append((label, pred_label))\n",
    "\n",
    "        if label != pred_label:\n",
    "            print(f\"预测错误的图像：{image_path}\",end='\\t')\n",
    "            # plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "            print(f\"真实标签：{label}\",end='\\t')\n",
    "            print(f\"预测标签：{pred_label}\")\n",
    "            print(\"-------------------------------------\")\n",
    "total = len(predictions)\n",
    "correct = sum(1 for label, pred_label in predictions if label == pred_label)\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A  BA  CHI  DA   E  FU  HA  HE  HI  HO   I  JI  KA  KE  KI  KO  KU  MA  \\\n",
      "A    20   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "BA    0  20    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "CHI   0   0   20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "DA    0   0    0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "E     0   0    0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "FU    0   0    0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HA    0   0    0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HE    0   0    0   0   0   0   0  19   0   0   0   0   0   0   0   0   0   0   \n",
      "HI    0   0    0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   \n",
      "HO    0   0    0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   \n",
      "I     0   0    0   0   0   0   0   0   0   0  19   0   0   0   0   0   0   0   \n",
      "JI    0   0    0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   \n",
      "KA    0   0    0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   \n",
      "KE    0   0    0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   \n",
      "KI    0   0    0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   \n",
      "KO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   \n",
      "KU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   \n",
      "MA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   \n",
      "ME    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "N     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "O     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "PI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SHI   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TSU   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "U     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "WA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "WO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "     ME  MI  MO  MU   N  NA  NE  NI  NO  NU   O  PI  RA  RE  RI  RO  RU  SA  \\\n",
      "A     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "BA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "CHI   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "DA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "E     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "FU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HI    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "I     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "JI    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KI    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "ME   20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MI    0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MO    0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MU    0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "N     0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NA    0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NE    0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NI    0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   \n",
      "NO    0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   \n",
      "NU    0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   \n",
      "O     0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   \n",
      "PI    0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   \n",
      "RA    0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   \n",
      "RE    0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   \n",
      "RI    0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   \n",
      "RO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   \n",
      "RU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   \n",
      "SA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   \n",
      "SE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SHI   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TSU   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "U     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "WA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "WO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "     SE  SHI  SO  SU  TA  TE  TO  TSU   U  WA  WO  YA  YO  YU  \n",
      "A     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "BA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "CHI   0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "DA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "E     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "FU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "HA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "HE    0    0   0   0   0   0   0    0   0   1   0   0   0   0  \n",
      "HI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "HO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "I     0    1   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "JI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KE    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "ME    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "N     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NE    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "O     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "PI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RE    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SE   20    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SHI   0   20   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SO    0    0  20   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SU    0    0   0  20   0   0   0    0   0   0   0   0   0   0  \n",
      "TA    0    0   0   0  20   0   0    0   0   0   0   0   0   0  \n",
      "TE    0    0   0   0   0  20   0    0   0   0   0   0   0   0  \n",
      "TO    0    0   0   0   0   0  20    0   0   0   0   0   0   0  \n",
      "TSU   0    0   0   0   0   0   0   20   0   0   0   0   0   0  \n",
      "U     0    0   0   0   0   0   0    0  20   0   0   0   0   0  \n",
      "WA    0    0   0   0   0   0   0    0   0  20   0   0   0   0  \n",
      "WO    0    0   0   0   0   0   0    0   0   0  20   0   0   0  \n",
      "YA    0    0   0   0   0   0   0    0   0   0   0  20   0   0  \n",
      "YO    0    0   0   0   0   0   0    0   0   0   0   0  20   0  \n",
      "YU    0    0   0   0   0   0   0    0   0   0   0   0   0  20  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建一个空的数据帧\n",
    "df = pd.DataFrame(index=label_dict.values(), columns=label_dict.values())\n",
    "df = df.fillna(0)  # 将所有值初始化为0\n",
    "\n",
    "# 计算正確值和預測值的交叉计数\n",
    "for label, pred_label in predictions:\n",
    "    df.loc[label, pred_label] += 1\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.80%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 比對預測結果和真實標籤\n",
    "# for label, pred_label in predictions:\n",
    "#     if label == pred_label:\n",
    "#         print(f'Label: {label}\\tPrediction: {pred_label}\\tCorrect')\n",
    "total = len(predictions)\n",
    "correct = sum(1 for label, pred_label in predictions if label == pred_label)\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('testing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22c60e53b9b57bc2d5950e7c6d3f647d37bca2c21c4934a06f07d6484f6bc0f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

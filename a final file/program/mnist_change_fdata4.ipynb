{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 777\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "```text\n",
    ".\n",
    "<!-- ├── dataset\n",
    "│   └── mnist -->\n",
    "│       └── data_new\n",
    "│           ├── \n",
    "│           │   ├── 0.jpg\n",
    "│           │   ├── 1.jpg\n",
    "│           │   ├── ...\n",
    "│           │   └── 59999.jpg\n",
    "│           └── label.txt\n",
    "└── mnist_change.ipynb  (訓練 MNIST)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\",device)\n",
    "DATASET_PATH = './fdata4'  # TODO: adjust path\n",
    "# DATASET_PATH = './data_new'  # TODO: adjust path\n",
    "# DATASET_PATH = './data_color'  # TODO: adjust path\n",
    "# DATASET_PATH = './data_color2'  # TODO: adjust path\n",
    "\n",
    "output_path = './model/hira.pt'  # TODO: adjust path\n",
    "epoch = 1000\n",
    "batch_size = 1024\n",
    "learning_rate = 0.01\n",
    "img_size=(83, 84)  # TODO: adjust img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def img_convert(input: np.ndarray) -> torch.Tensor: #回傳的是torch.Tensor，用於測試\n",
    "    # 将图像转换为灰度影像\n",
    "    gray_image = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY)\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(gray_image, (83, 84))\n",
    "\n",
    "    equalized_image = cv2.equalizeHist(resized_image)\n",
    "\n",
    "    # 多次侵蚀后膨胀\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    eroded_image = cv2.erode(equalized_image, kernel, iterations=2)\n",
    "    dilated_image = cv2.dilate(eroded_image, kernel, iterations=2)\n",
    "\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(dilated_image, (83, 84))\n",
    "\n",
    "    # 将图像转换为张量并调整形状\n",
    "    tensor_image = transforms.ToTensor()(resized_image)\n",
    "    # tensor_image = torch.unsqueeze(tensor_image, 0)\n",
    "\n",
    "    return tensor_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, path: str=f'/{DATASET_PATH}', data_transform: Optional[Callable]=None) -> None:\n",
    "        super().__init__()\n",
    "        self.dataset_path = Path(path)\n",
    "        # self.data_transform = data_transform\n",
    "        self.image, self.label = self.read_dataset()\n",
    "\n",
    "        assert len(self.image) == len(self.label)\n",
    "        self.length = len(self.image)\n",
    "\n",
    "    def read_dataset(self):  # TODO: adjust this function\n",
    "        image = dict()\n",
    "        for path in self.dataset_path.joinpath('').glob('**/*'):\n",
    "            if path.suffix in ['.jpg']:\n",
    "                test_image = cv2.imread(str(path))  # 使用 cv2 讀取圖片\n",
    "                converted_image = img_convert(test_image)  # 將圖片轉換成您需要的格式\n",
    "                image[int(path.stem)] = np.array(converted_image)\n",
    "\n",
    "                # image[int(path.stem)] = np.array(Image.open(path).copy())\n",
    "        with open(self.dataset_path.joinpath('label.txt'), mode='r') as f:\n",
    "            label = f.read().split(\"\\n\")  # 使用換行符分隔標籤\n",
    "            label = [int(label) for label in label]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.image[index], self.label[index]\n",
    "        # 假设您直接在此处对图像进行转换\n",
    "        # image = img_convert(image)\n",
    "        image = torch.Tensor(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, in_channels=1, out_num=50):  # adjust out_num to 50\n",
    "#         super(CNN, self).__init__()\n",
    "\n",
    "#         self.cnn1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=5, stride=1, padding=0) # out_shape=(32, 79, 80)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2) # out_shape=(32, 39, 40)\n",
    "\n",
    "#         self.cnn2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0) # out_shape=(64, 35, 36)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2) # out_shape=(64, 17, 18)\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_features=64 * 17 * 18, out_features=out_num)  # adjust in_features based on the output shape of the last convolutional layer\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.cnn1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.maxpool1(x)\n",
    "\n",
    "#         x = self.cnn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.maxpool2(x)\n",
    "\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_num=50):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Add a new hidden layer\n",
    "        self.hidden_layer = nn.Linear(in_features=64 * 17 * 18, out_features=256)  # Adjust the number of output features as needed\n",
    "        self.dropout = nn.Dropout(0.05)  # Adjust the dropout rate as needed\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=out_num)  # Adjust the input features based on the output shape of the hidden layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.cnn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Pass through the hidden layer and apply dropout\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build\n",
    "(1)model, (2)loss-function, (3)optimizer, (4)dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('./model/hira.pt')  # TODO: adjust path\n",
    "use_pretrained_model = True  # TODO: set True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set new model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (hidden_layer): Linear(in_features=19584, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       "  (fc1): Linear(in_features=256, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢查模型檔案是否存在\n",
    "if model_path.exists() and use_pretrained_model:\n",
    "    # 載入預訓練模型\n",
    "    model = torch.jit.load(model_path, map_location=device).to(device)\n",
    "    print('load model')\n",
    "else:\n",
    "    # 建立新模型\n",
    "    model = CNN().to(device)\n",
    "    print('set new model')\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# data_transform = transforms.Compose([\n",
    "#     transforms.Resize(img_size),\n",
    "#     transforms.ToTensor(),   # [0, 1]\n",
    "#     # transforms.Normalize((0.5), (0.5))  # [-1, 1]\n",
    "# ])\n",
    "dataset = MnistDataset(path=DATASET_PATH)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred: torch.Tensor, label: torch.Tensor):\n",
    "    _, pred_label = pred.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    acc = num_correct / label.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_choice = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/1000] loss: 3.775537222623825, acc: 0.2451761466305272\n",
      "Epoch[2/1000] loss: 1.5041558103902, acc: 0.7160171197385204\n",
      "Epoch[3/1000] loss: 0.4086578016036323, acc: 0.8910617958120748\n",
      "Epoch[4/1000] loss: 0.26258421157087597, acc: 0.9242603535554847\n",
      "Epoch[5/1000] loss: 0.19931561978799955, acc: 0.9418521803252551\n",
      "Epoch[6/1000] loss: 0.17710666917264462, acc: 0.9459942668473639\n",
      "Epoch[7/1000] loss: 0.15686677629128098, acc: 0.9513570564944728\n",
      "Epoch[8/1000] loss: 0.14318437541701964, acc: 0.955326002471301\n",
      "Epoch[9/1000] loss: 0.13395184245226638, acc: 0.9579833153964711\n",
      "Epoch[10/1000] loss: 0.12763295542182668, acc: 0.9600261247076955\n",
      "Epoch[11/1000] loss: 0.12121754425710865, acc: 0.9613248863998723\n",
      "Epoch[12/1000] loss: 0.11308071576058865, acc: 0.9639128600659014\n",
      "Epoch[13/1000] loss: 0.11072506123621549, acc: 0.9651277503188777\n",
      "Epoch[14/1000] loss: 0.10597678679706794, acc: 0.9662699796715561\n",
      "Epoch[15/1000] loss: 0.1016795517477606, acc: 0.9677775895514455\n",
      "Epoch[16/1000] loss: 0.09865918216694679, acc: 0.9684215727306548\n",
      "Epoch[17/1000] loss: 0.09467798943764397, acc: 0.9698536152742347\n",
      "Epoch[18/1000] loss: 0.09110137311342571, acc: 0.971265727970876\n",
      "Epoch[19/1000] loss: 0.08861026680096984, acc: 0.972032611872874\n",
      "Epoch[20/1000] loss: 0.08939487723234509, acc: 0.9711494705303997\n",
      "Epoch[21/1000] loss: 0.08480342444298523, acc: 0.9733234846673045\n",
      "Epoch[22/1000] loss: 0.08393265706087862, acc: 0.9731495137117347\n",
      "Epoch[23/1000] loss: 0.08143990826127785, acc: 0.9741356259300595\n",
      "Epoch[24/1000] loss: 0.07920597513605442, acc: 0.9747285388764881\n",
      "Epoch[25/1000] loss: 0.0791038304367768, acc: 0.9749357262436223\n",
      "Epoch[26/1000] loss: 0.07687053263985685, acc: 0.9753027675914117\n",
      "Epoch[27/1000] loss: 0.07455991268424052, acc: 0.976200025908801\n",
      "Epoch[28/1000] loss: 0.07398198651415962, acc: 0.9762195405505952\n",
      "Epoch[29/1000] loss: 0.07150823060822274, acc: 0.9774747057025935\n",
      "Epoch[30/1000] loss: 0.07095902416455958, acc: 0.9771159684576955\n",
      "Epoch[31/1000] loss: 0.0705717136817319, acc: 0.9772135416666667\n",
      "Epoch[32/1000] loss: 0.06789903469117624, acc: 0.9785828882334184\n",
      "Epoch[33/1000] loss: 0.06715935874464256, acc: 0.978171004730017\n",
      "Epoch[34/1000] loss: 0.06512020940759353, acc: 0.9796034624787415\n",
      "Epoch[35/1000] loss: 0.0650415699928999, acc: 0.9791330350499575\n",
      "Epoch[36/1000] loss: 0.06315174263103732, acc: 0.9796296204028486\n",
      "Epoch[37/1000] loss: 0.06266872228921525, acc: 0.9802553345556974\n",
      "Epoch[38/1000] loss: 0.06262468423561326, acc: 0.9801257905505952\n",
      "Epoch[39/1000] loss: 0.06073922507597932, acc: 0.9809283820950255\n",
      "Epoch[40/1000] loss: 0.059678219757708054, acc: 0.9810404874840561\n",
      "Epoch[41/1000] loss: 0.058539174596912096, acc: 0.9810849144345238\n",
      "Epoch[42/1000] loss: 0.05810857534275523, acc: 0.9818447398490646\n",
      "Epoch[43/1000] loss: 0.05614433474173503, acc: 0.9821324770142432\n",
      "Epoch[44/1000] loss: 0.05634820946891393, acc: 0.9815225406568878\n",
      "Epoch[45/1000] loss: 0.05498915754391679, acc: 0.9828291912468112\n",
      "Epoch[46/1000] loss: 0.05440689576789737, acc: 0.9828653140943878\n",
      "Epoch[47/1000] loss: 0.0540160841108965, acc: 0.9830384546396684\n",
      "Epoch[48/1000] loss: 0.0532962868622105, acc: 0.9829620568930697\n",
      "Epoch[49/1000] loss: 0.05305518043626632, acc: 0.9832983730601617\n",
      "Epoch[50/1000] loss: 0.051370659693410356, acc: 0.9837804262329932\n",
      "Epoch[51/1000] loss: 0.05148515431210399, acc: 0.9836816074085883\n",
      "Epoch[52/1000] loss: 0.04991544151146497, acc: 0.9843309882546769\n",
      "Epoch[53/1000] loss: 0.04943922122142145, acc: 0.9847490998352466\n",
      "Epoch[54/1000] loss: 0.04883327434903809, acc: 0.9845298715189201\n",
      "Epoch[55/1000] loss: 0.0477650182853852, acc: 0.9853415975765306\n",
      "Epoch[56/1000] loss: 0.04741873400884548, acc: 0.9849728954081634\n",
      "Epoch[57/1000] loss: 0.04733192797617188, acc: 0.9849567024075255\n",
      "Epoch[58/1000] loss: 0.04657059462208833, acc: 0.9854113520408163\n",
      "Epoch[59/1000] loss: 0.04597487975843251, acc: 0.9855944575095663\n",
      "Epoch[60/1000] loss: 0.04529316069757832, acc: 0.9858635104432398\n",
      "Epoch[61/1000] loss: 0.04461931169498712, acc: 0.986220171662415\n",
      "Epoch[62/1000] loss: 0.04436055281465607, acc: 0.9861346394026361\n",
      "Epoch[63/1000] loss: 0.043950922886974046, acc: 0.9864568385948129\n",
      "Epoch[64/1000] loss: 0.0437308891144182, acc: 0.9860291772959184\n",
      "Epoch[65/1000] loss: 0.04335119703318924, acc: 0.986464312287415\n",
      "Epoch[66/1000] loss: 0.0419764341287581, acc: 0.9869276812287415\n",
      "Epoch[67/1000] loss: 0.041829085775784085, acc: 0.986708452912415\n",
      "Epoch[68/1000] loss: 0.04149251534337444, acc: 0.9870065702062075\n",
      "Epoch[69/1000] loss: 0.04077274545228907, acc: 0.9871726522640306\n",
      "Epoch[70/1000] loss: 0.040354448021389544, acc: 0.987373196348852\n",
      "Epoch[71/1000] loss: 0.03920049123865153, acc: 0.9882102499202806\n",
      "Epoch[72/1000] loss: 0.040136641007848084, acc: 0.9874325706845238\n",
      "Epoch[73/1000] loss: 0.038934445939958096, acc: 0.9878266003667092\n",
      "Epoch[74/1000] loss: 0.038321149801569324, acc: 0.9880362789647109\n",
      "Epoch[75/1000] loss: 0.03747378025270466, acc: 0.9885914082429847\n",
      "Epoch[76/1000] loss: 0.0373836356281702, acc: 0.9884460864423895\n",
      "Epoch[77/1000] loss: 0.03781722955006574, acc: 0.9884888525722789\n",
      "Epoch[78/1000] loss: 0.037152366895627766, acc: 0.9884539753401361\n",
      "Epoch[79/1000] loss: 0.03676249203272164, acc: 0.9884186829028486\n",
      "Epoch[80/1000] loss: 0.036964113025793006, acc: 0.9881919808939201\n",
      "Epoch[81/1000] loss: 0.03572014236955771, acc: 0.9892054966517857\n",
      "Epoch[82/1000] loss: 0.03502830352434622, acc: 0.9892640405771684\n",
      "Epoch[83/1000] loss: 0.035127946632980765, acc: 0.9891876428305697\n",
      "Epoch[84/1000] loss: 0.0343084869507168, acc: 0.9894056255314626\n",
      "Epoch[85/1000] loss: 0.03458979566182409, acc: 0.9891951165231718\n",
      "Epoch[86/1000] loss: 0.034349445769164176, acc: 0.9892914441167092\n",
      "Epoch[87/1000] loss: 0.033832956022316854, acc: 0.9896663743622449\n",
      "Epoch[88/1000] loss: 0.03281964318427656, acc: 0.9900595736341412\n",
      "Epoch[89/1000] loss: 0.032902976913776784, acc: 0.9899283688084609\n",
      "Epoch[90/1000] loss: 0.03226222887003262, acc: 0.9899005500637755\n",
      "Epoch[91/1000] loss: 0.03170349615226899, acc: 0.9903987962372449\n",
      "Epoch[92/1000] loss: 0.031680742744356394, acc: 0.9904772700095663\n",
      "Epoch[93/1000] loss: 0.03110397479031235, acc: 0.9905295858577806\n",
      "Epoch[94/1000] loss: 0.03164580846039046, acc: 0.9903286265678146\n",
      "Epoch[95/1000] loss: 0.030954958987422287, acc: 0.9905283402423469\n",
      "Epoch[96/1000] loss: 0.03068852042945634, acc: 0.9907475685586735\n",
      "Epoch[97/1000] loss: 0.03008591860998422, acc: 0.9911391070099915\n",
      "Epoch[98/1000] loss: 0.02967742388968223, acc: 0.9911486567283163\n",
      "Epoch[99/1000] loss: 0.02989189441515399, acc: 0.9908530306653912\n",
      "Epoch[100/1000] loss: 0.029105232828961953, acc: 0.9913408967102466\n",
      "Epoch[101/1000] loss: 0.028977653959632983, acc: 0.9912271305006378\n",
      "Epoch[102/1000] loss: 0.028683920823303715, acc: 0.9911843643707483\n",
      "Epoch[103/1000] loss: 0.02814732646636133, acc: 0.9915580490008503\n",
      "Epoch[104/1000] loss: 0.028107395861297846, acc: 0.9915318910767432\n",
      "Epoch[105/1000] loss: 0.02804759670315044, acc: 0.9917324351615646\n",
      "Epoch[106/1000] loss: 0.027482973877340555, acc: 0.9918374820631378\n",
      "Epoch[107/1000] loss: 0.02751943951339594, acc: 0.991498259460034\n",
      "Epoch[108/1000] loss: 0.027378338621929288, acc: 0.9915754876169218\n",
      "Epoch[109/1000] loss: 0.02687602464409013, acc: 0.9921692309736395\n",
      "Epoch[110/1000] loss: 0.026423811579921415, acc: 0.9921684005633503\n",
      "Epoch[111/1000] loss: 0.026375460272122706, acc: 0.9920812074829932\n",
      "Epoch[112/1000] loss: 0.025780353844831034, acc: 0.992456968138818\n",
      "Epoch[113/1000] loss: 0.025916442266731923, acc: 0.9923523364423895\n",
      "Epoch[114/1000] loss: 0.025888636857936426, acc: 0.9920895115858844\n",
      "Epoch[115/1000] loss: 0.025461801668695574, acc: 0.9922394006430697\n",
      "Epoch[116/1000] loss: 0.025100491674883024, acc: 0.9925703191432823\n",
      "Epoch[117/1000] loss: 0.024652989855634848, acc: 0.9925449916294643\n",
      "Epoch[118/1000] loss: 0.024129576224368066, acc: 0.9928393720769558\n",
      "Epoch[119/1000] loss: 0.02437944370155622, acc: 0.9927708632281037\n",
      "Epoch[120/1000] loss: 0.0237830014001312, acc: 0.9927791673309949\n",
      "Epoch[121/1000] loss: 0.02383341799057754, acc: 0.9926840853528912\n",
      "Epoch[122/1000] loss: 0.023371894477999637, acc: 0.9931366589604592\n",
      "Epoch[123/1000] loss: 0.023187129691775357, acc: 0.9933816299957483\n",
      "Epoch[124/1000] loss: 0.02339916834275105, acc: 0.993110501036352\n",
      "Epoch[125/1000] loss: 0.02258868226116257, acc: 0.993232571348852\n",
      "Epoch[126/1000] loss: 0.02230747163412161, acc: 0.9935294430272109\n",
      "Epoch[127/1000] loss: 0.022566319326870143, acc: 0.9932765830941752\n",
      "Epoch[128/1000] loss: 0.02247880049149639, acc: 0.9933467527636054\n",
      "Epoch[129/1000] loss: 0.021539141066438918, acc: 0.9937391216252126\n",
      "Epoch[130/1000] loss: 0.021338725204779103, acc: 0.993860776732568\n",
      "Epoch[131/1000] loss: 0.021591436392295042, acc: 0.9936689519557823\n",
      "Epoch[132/1000] loss: 0.021935112948995084, acc: 0.9936768408535289\n",
      "Epoch[133/1000] loss: 0.020885025009712472, acc: 0.9942278180803571\n",
      "Epoch[134/1000] loss: 0.020982757767861977, acc: 0.9936510981345663\n",
      "Epoch[135/1000] loss: 0.020987986021542122, acc: 0.9939222270939626\n",
      "Epoch[136/1000] loss: 0.02030320222755628, acc: 0.9942867772108844\n",
      "Epoch[137/1000] loss: 0.020197048658571606, acc: 0.9941140518707483\n",
      "Epoch[138/1000] loss: 0.020501674676779658, acc: 0.9942348765678146\n",
      "Epoch[139/1000] loss: 0.020302439923398197, acc: 0.9940177242772109\n",
      "Epoch[140/1000] loss: 0.019960592338715548, acc: 0.9942622801073554\n",
      "Epoch[141/1000] loss: 0.01921665443972285, acc: 0.9944632393973214\n",
      "Epoch[142/1000] loss: 0.019277702335135212, acc: 0.9944624089870323\n",
      "Epoch[143/1000] loss: 0.019389258560425202, acc: 0.99456787109375\n",
      "Epoch[144/1000] loss: 0.01901311764959246, acc: 0.9945412979644983\n",
      "Epoch[145/1000] loss: 0.01892774863102074, acc: 0.9943918241124575\n",
      "Epoch[146/1000] loss: 0.018139646505005658, acc: 0.9949241171077806\n",
      "Epoch[147/1000] loss: 0.01899574131571821, acc: 0.9944362510629252\n",
      "Epoch[148/1000] loss: 0.01846840902830341, acc: 0.9947854385894983\n",
      "Epoch[149/1000] loss: 0.01827147272082844, acc: 0.9948115965136054\n",
      "Epoch[150/1000] loss: 0.017455323524440507, acc: 0.9952566964285714\n",
      "Epoch[151/1000] loss: 0.017762371596680687, acc: 0.99493408203125\n",
      "Epoch[152/1000] loss: 0.017194829307430024, acc: 0.9953696322278912\n",
      "Epoch[153/1000] loss: 0.017238819056988826, acc: 0.9950117253932823\n",
      "Epoch[154/1000] loss: 0.01711912903036656, acc: 0.9950453570099915\n",
      "Epoch[155/1000] loss: 0.016969913176061318, acc: 0.9953351702008929\n",
      "Epoch[156/1000] loss: 0.01670425368190211, acc: 0.9953862404336735\n",
      "Epoch[157/1000] loss: 0.016476875661672757, acc: 0.9954917025403912\n",
      "Epoch[158/1000] loss: 0.016536749820391248, acc: 0.9953521936118197\n",
      "Epoch[159/1000] loss: 0.016655728046316653, acc: 0.9951076377816752\n",
      "Epoch[160/1000] loss: 0.01607530040616569, acc: 0.9956569541879252\n",
      "Epoch[161/1000] loss: 0.015910764973211502, acc: 0.9957711356026786\n",
      "Epoch[162/1000] loss: 0.016026846511522308, acc: 0.995325620482568\n",
      "Epoch[163/1000] loss: 0.015529827464238874, acc: 0.9958579134778912\n",
      "Epoch[164/1000] loss: 0.015454402596723022, acc: 0.9956303810586735\n",
      "Epoch[165/1000] loss: 0.015284557959863119, acc: 0.9958579134778912\n",
      "Epoch[166/1000] loss: 0.015062312691173117, acc: 0.9961016388977466\n",
      "Epoch[167/1000] loss: 0.01471482151620356, acc: 0.9961194927189626\n",
      "Epoch[168/1000] loss: 0.014539328736386128, acc: 0.9960754809736395\n",
      "Epoch[169/1000] loss: 0.014489709528528951, acc: 0.996058042357568\n",
      "Epoch[170/1000] loss: 0.014621557044197939, acc: 0.9959355568399235\n",
      "Epoch[171/1000] loss: 0.014328595121956564, acc: 0.9962328437234269\n",
      "Epoch[172/1000] loss: 0.013996520624329736, acc: 0.9964250837053571\n",
      "Epoch[173/1000] loss: 0.01408235342075516, acc: 0.9963287561118197\n",
      "Epoch[174/1000] loss: 0.01406894309911877, acc: 0.9961286272321429\n",
      "Epoch[175/1000] loss: 0.013647866959217936, acc: 0.9963030133928571\n",
      "Epoch[176/1000] loss: 0.013907328996408199, acc: 0.9962502823394983\n",
      "Epoch[177/1000] loss: 0.013542140992025711, acc: 0.9963544988307823\n",
      "Epoch[178/1000] loss: 0.013363240532011591, acc: 0.9965812008397109\n",
      "Epoch[179/1000] loss: 0.013250021089334041, acc: 0.9964587153220663\n",
      "Epoch[180/1000] loss: 0.013258365845623692, acc: 0.9965122767857143\n",
      "Epoch[181/1000] loss: 0.012652027379122696, acc: 0.9966866629464286\n",
      "Epoch[182/1000] loss: 0.012584940014806176, acc: 0.9967298442814626\n",
      "Epoch[183/1000] loss: 0.012521371000080503, acc: 0.9966692243303571\n",
      "Epoch[184/1000] loss: 0.012679496006707527, acc: 0.99676513671875\n",
      "Epoch[185/1000] loss: 0.01277878212775769, acc: 0.9964857036564626\n",
      "Epoch[186/1000] loss: 0.01262416722602211, acc: 0.9966339318930697\n",
      "Epoch[187/1000] loss: 0.012376527277021003, acc: 0.9968697684151786\n",
      "Epoch[188/1000] loss: 0.012380975390052689, acc: 0.9967995987457483\n",
      "Epoch[189/1000] loss: 0.012097937524751095, acc: 0.9968784877232143\n",
      "Epoch[190/1000] loss: 0.012247061298694462, acc: 0.9968079028486395\n",
      "Epoch[191/1000] loss: 0.01163319543203605, acc: 0.9972446986607143\n",
      "Epoch[192/1000] loss: 0.011714972035926101, acc: 0.9970703125\n",
      "Epoch[193/1000] loss: 0.01168773029348813, acc: 0.9970790318080357\n",
      "Epoch[194/1000] loss: 0.011500444313112115, acc: 0.9971309324511054\n",
      "Epoch[195/1000] loss: 0.011217669272030304, acc: 0.9970175814466412\n",
      "Epoch[196/1000] loss: 0.011127056287867683, acc: 0.9972613068664966\n",
      "Epoch[197/1000] loss: 0.011329823381467057, acc: 0.9972011021205357\n",
      "Epoch[198/1000] loss: 0.010841456179540339, acc: 0.9973314765359269\n",
      "Epoch[199/1000] loss: 0.010785772090977324, acc: 0.9971309324511054\n",
      "Epoch[200/1000] loss: 0.0109250731308878, acc: 0.9971139090401786\n",
      "Epoch[201/1000] loss: 0.010491090602174933, acc: 0.9974182544111395\n",
      "Epoch[202/1000] loss: 0.01057262105418236, acc: 0.9975233013127126\n",
      "Epoch[203/1000] loss: 0.010530793785749535, acc: 0.9974531316432823\n",
      "Epoch[204/1000] loss: 0.01035587809747085, acc: 0.9975320206207483\n",
      "Epoch[205/1000] loss: 0.010419747480357597, acc: 0.9973401958439626\n",
      "Epoch[206/1000] loss: 0.010342800927381697, acc: 0.9974190848214286\n",
      "Epoch[207/1000] loss: 0.010039358096297033, acc: 0.9976453716252126\n",
      "Epoch[208/1000] loss: 0.009657511296349444, acc: 0.9977500033216412\n",
      "Epoch[209/1000] loss: 0.009863088959329096, acc: 0.9975752019557823\n",
      "Epoch[210/1000] loss: 0.009771048230634603, acc: 0.9975934709821429\n",
      "Epoch[211/1000] loss: 0.009621888461489496, acc: 0.9977321495004252\n",
      "Epoch[212/1000] loss: 0.009412770086782984, acc: 0.997802734375\n",
      "Epoch[213/1000] loss: 0.00941185889366482, acc: 0.9978633543261054\n",
      "Epoch[214/1000] loss: 0.009306099141083126, acc: 0.9978376116071429\n",
      "Epoch[215/1000] loss: 0.009223072232478964, acc: 0.997802734375\n",
      "Epoch[216/1000] loss: 0.009172080918298369, acc: 0.997767026732568\n",
      "Epoch[217/1000] loss: 0.009123551268463157, acc: 0.997802734375\n",
      "Epoch[218/1000] loss: 0.008978166095662996, acc: 0.9979326935852466\n",
      "Epoch[219/1000] loss: 0.008752346209283652, acc: 0.9978982315582483\n",
      "Epoch[220/1000] loss: 0.00893795924951389, acc: 0.9978288922991071\n",
      "Epoch[221/1000] loss: 0.008667054278443434, acc: 0.9980464597948554\n",
      "Epoch[222/1000] loss: 0.008481013138537361, acc: 0.99798583984375\n",
      "Epoch[223/1000] loss: 0.008735449535639159, acc: 0.9979248046875\n",
      "Epoch[224/1000] loss: 0.00863298911385105, acc: 0.9979335239955357\n",
      "Epoch[225/1000] loss: 0.008199518213846855, acc: 0.9982029921343537\n",
      "Epoch[226/1000] loss: 0.008460308356526574, acc: 0.997845915710034\n",
      "Epoch[227/1000] loss: 0.008298974536176371, acc: 0.9981415417729592\n",
      "Epoch[228/1000] loss: 0.008381874543764363, acc: 0.9980817522321429\n",
      "Epoch[229/1000] loss: 0.007903110489548584, acc: 0.9983084542410714\n",
      "Epoch[230/1000] loss: 0.007670236605918035, acc: 0.9983171735491071\n",
      "Epoch[231/1000] loss: 0.007618157958079662, acc: 0.9983429162680697\n",
      "Epoch[232/1000] loss: 0.010503728661985536, acc: 0.9977674419377126\n",
      "Epoch[233/1000] loss: 0.007844377442130021, acc: 0.9983080390359269\n",
      "Epoch[234/1000] loss: 0.007965449913172051, acc: 0.9980817522321429\n",
      "Epoch[235/1000] loss: 0.007714605881899063, acc: 0.9982474190848214\n",
      "Epoch[236/1000] loss: 0.007364286232456964, acc: 0.998578337585034\n",
      "Epoch[237/1000] loss: 0.007407557481201366, acc: 0.9983694893973214\n",
      "Epoch[238/1000] loss: 0.007157656698836945, acc: 0.9984654017857143\n",
      "Epoch[239/1000] loss: 0.007108817157651564, acc: 0.9984649865805697\n",
      "Epoch[240/1000] loss: 0.006982583486075912, acc: 0.9985260217368197\n",
      "Epoch[241/1000] loss: 0.007143722119508311, acc: 0.9985874720982143\n",
      "Epoch[242/1000] loss: 0.007057107375918089, acc: 0.9984213900403912\n",
      "Epoch[243/1000] loss: 0.006878563051161889, acc: 0.9985957762011054\n",
      "Epoch[244/1000] loss: 0.006867000845626795, acc: 0.9985957762011054\n",
      "Epoch[245/1000] loss: 0.0067342469680755, acc: 0.9986746651785714\n",
      "Epoch[246/1000] loss: 0.00657146774548372, acc: 0.9985525948660714\n",
      "Epoch[247/1000] loss: 0.00660273214037131, acc: 0.9986746651785714\n",
      "Epoch[248/1000] loss: 0.006767988988680632, acc: 0.9986044955091412\n",
      "Epoch[249/1000] loss: 0.006493195687653497, acc: 0.9986485072544643\n",
      "Epoch[250/1000] loss: 0.006462805773480795, acc: 0.9987531389508929\n",
      "Epoch[251/1000] loss: 0.006348550012002566, acc: 0.9988054547991071\n",
      "Epoch[252/1000] loss: 0.006160642500617541, acc: 0.9988228934151786\n",
      "Epoch[253/1000] loss: 0.006404687286703847, acc: 0.9987440044377126\n",
      "Epoch[254/1000] loss: 0.006130485545976886, acc: 0.9988490513392857\n",
      "Epoch[255/1000] loss: 0.006063173455394073, acc: 0.9988316127232143\n",
      "Epoch[256/1000] loss: 0.006289271411203247, acc: 0.9987357003348214\n",
      "Epoch[257/1000] loss: 0.005963853169565222, acc: 0.9988573554421769\n",
      "Epoch[258/1000] loss: 0.006086653252298545, acc: 0.9987876009778912\n",
      "Epoch[259/1000] loss: 0.0060075882335825425, acc: 0.9987967354910714\n",
      "Epoch[260/1000] loss: 0.005847610130980944, acc: 0.9988490513392857\n",
      "Epoch[261/1000] loss: 0.006125434987812436, acc: 0.9986397879464286\n",
      "Epoch[262/1000] loss: 0.005903875415762221, acc: 0.99884033203125\n",
      "Epoch[263/1000] loss: 0.005643521888747013, acc: 0.9989441333173895\n",
      "Epoch[264/1000] loss: 0.005763677447768194, acc: 0.9987523085406037\n",
      "Epoch[265/1000] loss: 0.005834545926648259, acc: 0.998865659545068\n",
      "Epoch[266/1000] loss: 0.005474022456577846, acc: 0.9989449637276786\n",
      "Epoch[267/1000] loss: 0.005476004491876145, acc: 0.9989711216517857\n",
      "Epoch[268/1000] loss: 0.005574313566155199, acc: 0.9989188058035714\n",
      "Epoch[269/1000] loss: 0.005360406731987106, acc: 0.9990143029868197\n",
      "Epoch[270/1000] loss: 0.005453279559982808, acc: 0.9989188058035714\n",
      "Epoch[271/1000] loss: 0.00519266957625015, acc: 0.9991538119153912\n",
      "Epoch[272/1000] loss: 0.005085540638122309, acc: 0.99908447265625\n",
      "Epoch[273/1000] loss: 0.005085054217488505, acc: 0.99920654296875\n",
      "Epoch[274/1000] loss: 0.005392359386730407, acc: 0.9990147181919643\n",
      "Epoch[275/1000] loss: 0.0052652483427664265, acc: 0.9989885602678571\n",
      "Epoch[276/1000] loss: 0.0052468636100197075, acc: 0.9989536830357143\n",
      "Epoch[277/1000] loss: 0.00483595603145659, acc: 0.9991629464285714\n",
      "Epoch[278/1000] loss: 0.004904031092467319, acc: 0.9991455078125\n",
      "Epoch[279/1000] loss: 0.005082557456002438, acc: 0.99896240234375\n",
      "Epoch[280/1000] loss: 0.00478051852925481, acc: 0.9991712505314626\n",
      "Epoch[281/1000] loss: 0.005022627636208199, acc: 0.9989798409598214\n",
      "Epoch[282/1000] loss: 0.004770226167498289, acc: 0.9991629464285714\n",
      "Epoch[283/1000] loss: 0.004627856756477351, acc: 0.9992414202008929\n",
      "Epoch[284/1000] loss: 0.00479525734928237, acc: 0.9991629464285714\n",
      "Epoch[285/1000] loss: 0.004760228043388841, acc: 0.9992148470716412\n",
      "Epoch[286/1000] loss: 0.004657967908964825, acc: 0.9991629464285714\n",
      "Epoch[287/1000] loss: 0.004638371276087128, acc: 0.9992152622767857\n",
      "Epoch[288/1000] loss: 0.0044054444255639934, acc: 0.9992152622767857\n",
      "Epoch[289/1000] loss: 0.00442669313010161, acc: 0.9993020401519983\n",
      "Epoch[290/1000] loss: 0.004318551303835453, acc: 0.9993722098214286\n",
      "Epoch[291/1000] loss: 0.004440320317891227, acc: 0.99920654296875\n",
      "Epoch[292/1000] loss: 0.0043502917916547245, acc: 0.9993373325892857\n",
      "Epoch[293/1000] loss: 0.004637713329949682, acc: 0.9991106305803571\n",
      "Epoch[294/1000] loss: 0.004407899940685768, acc: 0.9992588588169643\n",
      "Epoch[295/1000] loss: 0.004327165170772267, acc: 0.9992762974330357\n",
      "Epoch[296/1000] loss: 0.003961337121186911, acc: 0.9993809291294643\n",
      "Epoch[297/1000] loss: 0.004360197636872597, acc: 0.9992239815848214\n",
      "Epoch[298/1000] loss: 0.0040913731780684815, acc: 0.9993373325892857\n",
      "Epoch[299/1000] loss: 0.0040098193078717615, acc: 0.9994245256696429\n",
      "Epoch[300/1000] loss: 0.004155022090084718, acc: 0.9993373325892857\n",
      "Epoch[301/1000] loss: 0.0041121143471432985, acc: 0.9993020401519983\n",
      "Epoch[302/1000] loss: 0.00398284916134019, acc: 0.99932861328125\n",
      "Epoch[303/1000] loss: 0.0040487629407185265, acc: 0.9992937360491071\n",
      "Epoch[304/1000] loss: 0.004002115688178621, acc: 0.9992152622767857\n",
      "Epoch[305/1000] loss: 0.0038774396660820848, acc: 0.9993892332323554\n",
      "Epoch[306/1000] loss: 0.0037680998939322308, acc: 0.9994594029017857\n",
      "Epoch[307/1000] loss: 0.0037266233983765623, acc: 0.9994594029017857\n",
      "Epoch[308/1000] loss: 0.0039962223568831435, acc: 0.9992762974330357\n",
      "Epoch[309/1000] loss: 0.0038344540080288425, acc: 0.9993896484375\n",
      "Epoch[310/1000] loss: 0.003797580952648007, acc: 0.99932861328125\n",
      "Epoch[311/1000] loss: 0.0036738382321865565, acc: 0.9995029994419643\n",
      "Epoch[312/1000] loss: 0.003592231680938442, acc: 0.9994768415178571\n",
      "Epoch[313/1000] loss: 0.003486137264449748, acc: 0.9994158063616071\n",
      "Epoch[314/1000] loss: 0.0035612547269140904, acc: 0.999493864928784\n",
      "Epoch[315/1000] loss: 0.003604035353678877, acc: 0.9994594029017857\n",
      "Epoch[316/1000] loss: 0.0034250129319843836, acc: 0.9994855608258929\n",
      "Epoch[317/1000] loss: 0.0033478719986825517, acc: 0.9996425083705357\n",
      "Epoch[318/1000] loss: 0.0037250650556026293, acc: 0.9994158063616071\n",
      "Epoch[319/1000] loss: 0.0033137277569039725, acc: 0.9995814732142857\n",
      "Epoch[320/1000] loss: 0.0035311576159853886, acc: 0.99945068359375\n",
      "Epoch[321/1000] loss: 0.003431205684527023, acc: 0.9995279117506378\n",
      "Epoch[322/1000] loss: 0.003415509328728409, acc: 0.9995465959821429\n",
      "Epoch[323/1000] loss: 0.0033167957579800194, acc: 0.9995204380580357\n",
      "Epoch[324/1000] loss: 0.003316795702890626, acc: 0.9995291573660714\n",
      "Epoch[325/1000] loss: 0.0033592342744149, acc: 0.99951171875\n",
      "Epoch[326/1000] loss: 0.003079607600479254, acc: 0.9996163504464286\n",
      "Epoch[327/1000] loss: 0.0029811553634187605, acc: 0.9996425083705357\n",
      "Epoch[328/1000] loss: 0.0030664135078300853, acc: 0.9995814732142857\n",
      "Epoch[329/1000] loss: 0.0031227633725003606, acc: 0.9996425083705357\n",
      "Epoch[330/1000] loss: 0.0031579878517991994, acc: 0.9995291573660714\n",
      "Epoch[331/1000] loss: 0.0032700997565240997, acc: 0.99951171875\n",
      "Epoch[332/1000] loss: 0.003262789805635943, acc: 0.9995461807769983\n",
      "Epoch[333/1000] loss: 0.002943195473302954, acc: 0.9996599469866071\n",
      "Epoch[334/1000] loss: 0.0031037624534552117, acc: 0.9996076311383929\n",
      "Epoch[335/1000] loss: 0.0029201456771781003, acc: 0.9997732979910714\n",
      "Epoch[336/1000] loss: 0.0031483976999879815, acc: 0.9995291573660714\n",
      "Epoch[337/1000] loss: 0.0030038151307962835, acc: 0.9995901925223214\n",
      "Epoch[338/1000] loss: 0.0028827274519634166, acc: 0.9996250697544643\n",
      "Epoch[339/1000] loss: 0.0029690270759082133, acc: 0.9996250697544643\n",
      "Epoch[340/1000] loss: 0.002906899113440886, acc: 0.9996861049107143\n",
      "Epoch[341/1000] loss: 0.003091998230437249, acc: 0.9995810580091412\n",
      "Epoch[342/1000] loss: 0.0028857499471216996, acc: 0.9996250697544643\n",
      "Epoch[343/1000] loss: 0.002742801871915747, acc: 0.9996250697544643\n",
      "Epoch[344/1000] loss: 0.002814962588932498, acc: 0.9996163504464286\n",
      "Epoch[345/1000] loss: 0.0027666981882898006, acc: 0.9996773856026786\n",
      "Epoch[346/1000] loss: 0.0027273369365242877, acc: 0.9996856897055697\n",
      "Epoch[347/1000] loss: 0.0028798262376637596, acc: 0.9995465959821429\n",
      "Epoch[348/1000] loss: 0.002691340839321908, acc: 0.9996861049107143\n",
      "Epoch[349/1000] loss: 0.002767055888918029, acc: 0.9996076311383929\n",
      "Epoch[350/1000] loss: 0.002687532315446463, acc: 0.9996686662946429\n",
      "Epoch[351/1000] loss: 0.0026452644924574997, acc: 0.9996599469866071\n",
      "Epoch[352/1000] loss: 0.0026966222784332266, acc: 0.9996250697544643\n",
      "Epoch[353/1000] loss: 0.0024825039914243723, acc: 0.9997297014508929\n",
      "Epoch[354/1000] loss: 0.0028228125665918924, acc: 0.9996250697544643\n",
      "Epoch[355/1000] loss: 0.0027068571788342005, acc: 0.9995984966252126\n",
      "Epoch[356/1000] loss: 0.00260260294557416, acc: 0.9997035435267857\n",
      "Epoch[357/1000] loss: 0.002705489642851587, acc: 0.999615935241284\n",
      "Epoch[358/1000] loss: 0.0025845026689889244, acc: 0.9996425083705357\n",
      "Epoch[359/1000] loss: 0.0024138470450582516, acc: 0.9997209821428571\n",
      "Epoch[360/1000] loss: 0.0023870790942705105, acc: 0.9997384207589286\n",
      "Epoch[361/1000] loss: 0.002322277121753099, acc: 0.9997994559151786\n",
      "Epoch[362/1000] loss: 0.0025479980327613055, acc: 0.99969482421875\n",
      "Epoch[363/1000] loss: 0.0022383440607102656, acc: 0.9997907366071429\n",
      "Epoch[364/1000] loss: 0.002383055959009133, acc: 0.9997820172991071\n",
      "Epoch[365/1000] loss: 0.0024977939590046716, acc: 0.9996686662946429\n",
      "Epoch[366/1000] loss: 0.002331828128912353, acc: 0.9997816020939626\n",
      "Epoch[367/1000] loss: 0.0024530984275250895, acc: 0.9996861049107143\n",
      "Epoch[368/1000] loss: 0.0022937480945464423, acc: 0.9997732979910714\n",
      "Epoch[369/1000] loss: 0.002378654469794128, acc: 0.9996861049107143\n",
      "Epoch[370/1000] loss: 0.0024097245929754407, acc: 0.9997209821428571\n",
      "Epoch[371/1000] loss: 0.0022555183812593377, acc: 0.9997122628348214\n",
      "Epoch[372/1000] loss: 0.0023790229060978163, acc: 0.9997209821428571\n",
      "Epoch[373/1000] loss: 0.0023060889808610746, acc: 0.9997384207589286\n",
      "Epoch[374/1000] loss: 0.0022987189440755174, acc: 0.9996773856026786\n",
      "Epoch[375/1000] loss: 0.002061347532747147, acc: 0.9998517717633929\n",
      "Epoch[376/1000] loss: 0.002257490381972665, acc: 0.99969482421875\n",
      "Epoch[377/1000] loss: 0.002206299870782199, acc: 0.9997820172991071\n",
      "Epoch[378/1000] loss: 0.002339209730312827, acc: 0.99969482421875\n",
      "Epoch[379/1000] loss: 0.0021965540254313964, acc: 0.99981689453125\n",
      "Epoch[380/1000] loss: 0.002055524006469308, acc: 0.9998517717633929\n",
      "Epoch[381/1000] loss: 0.0023477405042545535, acc: 0.9996686662946429\n",
      "Epoch[382/1000] loss: 0.0021250161303864195, acc: 0.9997994559151786\n",
      "Epoch[383/1000] loss: 0.0021131918620085344, acc: 0.9997645786830357\n",
      "Epoch[384/1000] loss: 0.002232043412992165, acc: 0.9998081752232143\n",
      "Epoch[385/1000] loss: 0.0021338877460428713, acc: 0.9997732979910714\n",
      "Epoch[386/1000] loss: 0.0020924150714043727, acc: 0.999755859375\n",
      "Epoch[387/1000] loss: 0.002162843642249105, acc: 0.9997820172991071\n",
      "Epoch[388/1000] loss: 0.0021258428397621693, acc: 0.9997471400669643\n",
      "Epoch[389/1000] loss: 0.0020568654768534805, acc: 0.9998081752232143\n",
      "Epoch[390/1000] loss: 0.0020034656679074813, acc: 0.9997994559151786\n",
      "Epoch[391/1000] loss: 0.0020885514693093554, acc: 0.9997907366071429\n",
      "Epoch[392/1000] loss: 0.002039397145771155, acc: 0.9997645786830357\n",
      "Epoch[393/1000] loss: 0.0019431919208727777, acc: 0.9998517717633929\n",
      "Epoch[394/1000] loss: 0.0019133188533097772, acc: 0.9998692103794643\n",
      "Epoch[395/1000] loss: 0.0020688171997400267, acc: 0.9997297014508929\n",
      "Epoch[396/1000] loss: 0.0017366553599588639, acc: 0.9999215262276786\n",
      "Epoch[397/1000] loss: 0.0019469141282440563, acc: 0.9997820172991071\n",
      "Epoch[398/1000] loss: 0.0019127604656595004, acc: 0.9998517717633929\n",
      "Epoch[399/1000] loss: 0.001955052638680042, acc: 0.9997384207589286\n",
      "Epoch[400/1000] loss: 0.0021121683535706586, acc: 0.999755859375\n",
      "Epoch[401/1000] loss: 0.001732323771908081, acc: 0.9998866489955357\n",
      "Epoch[402/1000] loss: 0.002009303683215486, acc: 0.9997384207589286\n",
      "Epoch[403/1000] loss: 0.0018541482136892487, acc: 0.9998779296875\n",
      "Epoch[404/1000] loss: 0.00181848064260391, acc: 0.9998692103794643\n",
      "Epoch[405/1000] loss: 0.0019281986988062272, acc: 0.99981689453125\n",
      "Epoch[406/1000] loss: 0.0017669608174141363, acc: 0.9999128069196429\n",
      "Epoch[407/1000] loss: 0.0018399421252459952, acc: 0.9998430524553571\n",
      "Epoch[408/1000] loss: 0.0018345063513801765, acc: 0.9997732979910714\n",
      "Epoch[409/1000] loss: 0.001797227679032533, acc: 0.9998081752232143\n",
      "Epoch[410/1000] loss: 0.001646047547962683, acc: 0.9998692103794643\n",
      "Epoch[411/1000] loss: 0.0018257874487192436, acc: 0.9998081752232143\n",
      "Epoch[412/1000] loss: 0.0016937170195368318, acc: 0.9998604910714286\n",
      "Epoch[413/1000] loss: 0.00173563571869246, acc: 0.9998430524553571\n",
      "Epoch[414/1000] loss: 0.00168105400812887, acc: 0.9998692103794643\n",
      "Epoch[415/1000] loss: 0.0017261116330960899, acc: 0.9998779296875\n",
      "Epoch[416/1000] loss: 0.0017366927864454088, acc: 0.9998692103794643\n",
      "Epoch[417/1000] loss: 0.0017390946387812229, acc: 0.9998604910714286\n",
      "Epoch[418/1000] loss: 0.0016669057606902374, acc: 0.9998692103794643\n",
      "Epoch[419/1000] loss: 0.0017822960921226436, acc: 0.9998430524553571\n",
      "Epoch[420/1000] loss: 0.0016372219501395843, acc: 0.9998692103794643\n",
      "Epoch[421/1000] loss: 0.0016821224967965723, acc: 0.9998081752232143\n",
      "Epoch[422/1000] loss: 0.0016409020229600304, acc: 0.9998430524553571\n",
      "Epoch[423/1000] loss: 0.0016482266733614129, acc: 0.9998517717633929\n",
      "Epoch[424/1000] loss: 0.001619225459892602, acc: 0.9998779296875\n",
      "Epoch[425/1000] loss: 0.0017894130864338617, acc: 0.9998343331473214\n",
      "Epoch[426/1000] loss: 0.0015555024048288552, acc: 0.9998866489955357\n",
      "Epoch[427/1000] loss: 0.0015578398545455588, acc: 0.9998692103794643\n",
      "Epoch[428/1000] loss: 0.001656816504172249, acc: 0.99981689453125\n",
      "Epoch[429/1000] loss: 0.0016930762446593559, acc: 0.9997994559151786\n",
      "Epoch[430/1000] loss: 0.0015827549941604957, acc: 0.9998430524553571\n",
      "Epoch[431/1000] loss: 0.0017518818604003172, acc: 0.9997645786830357\n",
      "Epoch[432/1000] loss: 0.0015970562736973598, acc: 0.9998779296875\n",
      "Epoch[433/1000] loss: 0.001689330297397516, acc: 0.99981689453125\n",
      "Epoch[434/1000] loss: 0.0014737803854326817, acc: 0.9998604910714286\n",
      "Epoch[435/1000] loss: 0.0016151684119124962, acc: 0.9997820172991071\n",
      "Epoch[436/1000] loss: 0.0015034876076762366, acc: 0.999860075866284\n",
      "Epoch[437/1000] loss: 0.0016680746775818989, acc: 0.9997816020939626\n",
      "Epoch[438/1000] loss: 0.0015042214420515978, acc: 0.9998953683035714\n",
      "Epoch[439/1000] loss: 0.0014954460935509165, acc: 0.9998866489955357\n",
      "Epoch[440/1000] loss: 0.001429392060734764, acc: 0.9998953683035714\n",
      "Epoch[441/1000] loss: 0.0015550876430227487, acc: 0.9998779296875\n",
      "Epoch[442/1000] loss: 0.0014268490116852003, acc: 0.9998779296875\n",
      "Epoch[443/1000] loss: 0.0015329704011881923, acc: 0.9998604910714286\n",
      "Epoch[444/1000] loss: 0.0013573217997873144, acc: 0.99993896484375\n",
      "Epoch[445/1000] loss: 0.001533590557820779, acc: 0.9998256138392857\n",
      "Epoch[446/1000] loss: 0.001465279820487402, acc: 0.9998692103794643\n",
      "Epoch[447/1000] loss: 0.0015409844563691877, acc: 0.9998692103794643\n",
      "Epoch[448/1000] loss: 0.001489198381542727, acc: 0.9998604910714286\n",
      "Epoch[449/1000] loss: 0.0014759664453387294, acc: 0.9999128069196429\n",
      "Epoch[450/1000] loss: 0.0014728433249859205, acc: 0.9998430524553571\n",
      "Epoch[451/1000] loss: 0.0013755737922370567, acc: 0.9998604910714286\n",
      "Epoch[452/1000] loss: 0.0014628363886523793, acc: 0.9998779296875\n",
      "Epoch[453/1000] loss: 0.0012898636593620591, acc: 0.9999040876116071\n",
      "Epoch[454/1000] loss: 0.0014910441072320932, acc: 0.9997994559151786\n",
      "Epoch[455/1000] loss: 0.001332801094966791, acc: 0.9998953683035714\n",
      "Epoch[456/1000] loss: 0.0013817261954370355, acc: 0.9998779296875\n",
      "Epoch[457/1000] loss: 0.0013646148598646895, acc: 0.9998866489955357\n",
      "Epoch[458/1000] loss: 0.0013098818921467423, acc: 0.9998953683035714\n",
      "Epoch[459/1000] loss: 0.0014486105077854258, acc: 0.99981689453125\n",
      "Epoch[460/1000] loss: 0.0013606367137981579, acc: 0.9998604910714286\n",
      "Epoch[461/1000] loss: 0.0013257761506143392, acc: 0.9998604910714286\n",
      "Epoch[462/1000] loss: 0.0012807257647052342, acc: 0.9999215262276786\n",
      "Epoch[463/1000] loss: 0.0013846853222016112, acc: 0.9998692103794643\n",
      "Epoch[464/1000] loss: 0.0013449251418933272, acc: 0.9998953683035714\n",
      "Epoch[465/1000] loss: 0.0013118096960949646, acc: 0.9998692103794643\n",
      "Epoch[466/1000] loss: 0.0012801687033580883, acc: 0.9999215262276786\n",
      "Epoch[467/1000] loss: 0.0014604355715813913, acc: 0.9997820172991071\n",
      "Epoch[468/1000] loss: 0.0013759086844272264, acc: 0.9998430524553571\n",
      "Epoch[469/1000] loss: 0.0012851777184031171, acc: 0.9999128069196429\n",
      "Epoch[470/1000] loss: 0.0012819416453047389, acc: 0.9999128069196429\n",
      "Epoch[471/1000] loss: 0.0013574927693948016, acc: 0.9998866489955357\n",
      "Epoch[472/1000] loss: 0.00122247697540193, acc: 0.9999040876116071\n",
      "Epoch[473/1000] loss: 0.0011658661099188197, acc: 0.99993896484375\n",
      "Epoch[474/1000] loss: 0.0013167877202379583, acc: 0.9999215262276786\n",
      "Epoch[475/1000] loss: 0.0013389904582124604, acc: 0.9998953683035714\n",
      "Epoch[476/1000] loss: 0.0013033668786062793, acc: 0.9998779296875\n",
      "Epoch[477/1000] loss: 0.001327458596019174, acc: 0.9998517717633929\n",
      "Epoch[478/1000] loss: 0.0012951279157797607, acc: 0.9998692103794643\n",
      "Epoch[479/1000] loss: 0.0011958090349902964, acc: 0.9999215262276786\n",
      "Epoch[480/1000] loss: 0.0012541074523012088, acc: 0.9998953683035714\n",
      "Epoch[481/1000] loss: 0.0013651224640593032, acc: 0.9998256138392857\n",
      "Epoch[482/1000] loss: 0.0012220834996696794, acc: 0.9999128069196429\n",
      "Epoch[483/1000] loss: 0.001288032347344727, acc: 0.999860075866284\n",
      "Epoch[484/1000] loss: 0.0011583719089165762, acc: 0.9999128069196429\n",
      "Epoch[485/1000] loss: 0.001218275612960237, acc: 0.9998862337903912\n",
      "Epoch[486/1000] loss: 0.0011734786556709359, acc: 0.9999302455357143\n",
      "Epoch[487/1000] loss: 0.0012366566770651843, acc: 0.9999040876116071\n",
      "Epoch[488/1000] loss: 0.001208772416118466, acc: 0.9998866489955357\n",
      "Epoch[489/1000] loss: 0.0011463642832885462, acc: 0.9999215262276786\n",
      "Epoch[490/1000] loss: 0.0011417385711476008, acc: 0.9999128069196429\n",
      "Epoch[491/1000] loss: 0.0012718574021814025, acc: 0.9998517717633929\n",
      "Epoch[492/1000] loss: 0.0012616711373993894, acc: 0.9998779296875\n",
      "Epoch[493/1000] loss: 0.0011717383204086218, acc: 0.9998604910714286\n",
      "Epoch[494/1000] loss: 0.0011052725224414775, acc: 0.99993896484375\n",
      "Epoch[495/1000] loss: 0.001126045305162344, acc: 0.9999040876116071\n",
      "Epoch[496/1000] loss: 0.0011018552621473126, acc: 0.9999302455357143\n",
      "Epoch[497/1000] loss: 0.0011199392969761643, acc: 0.9999215262276786\n",
      "Epoch[498/1000] loss: 0.001105035937143839, acc: 0.99993896484375\n",
      "Epoch[499/1000] loss: 0.0011682561097196803, acc: 0.9999128069196429\n",
      "Epoch[500/1000] loss: 0.001056604310568738, acc: 0.99993896484375\n",
      "Epoch[501/1000] loss: 0.0010426144344819477, acc: 0.9999215262276786\n",
      "Epoch[502/1000] loss: 0.0011192848440779407, acc: 0.9999215262276786\n",
      "Epoch[503/1000] loss: 0.0010339197753117852, acc: 0.9999040876116071\n",
      "Epoch[504/1000] loss: 0.001126824366760307, acc: 0.9999128069196429\n",
      "Epoch[505/1000] loss: 0.0010646829341567354, acc: 0.9999564034598214\n",
      "Epoch[506/1000] loss: 0.0010936914187758312, acc: 0.9998866489955357\n",
      "Epoch[507/1000] loss: 0.0010766187993535173, acc: 0.9999215262276786\n",
      "Epoch[508/1000] loss: 0.001079111904930739, acc: 0.9999215262276786\n",
      "Epoch[509/1000] loss: 0.0010639235055610438, acc: 0.9999476841517857\n",
      "Epoch[510/1000] loss: 0.001030579534926801, acc: 0.9999302455357143\n",
      "Epoch[511/1000] loss: 0.0010431358722209033, acc: 0.9999302455357143\n",
      "Epoch[512/1000] loss: 0.0010106974367382854, acc: 0.9999476841517857\n",
      "Epoch[513/1000] loss: 0.0010138353640546224, acc: 0.9999040876116071\n",
      "Epoch[514/1000] loss: 0.001056960660207551, acc: 0.99993896484375\n",
      "Epoch[515/1000] loss: 0.0010390943038406217, acc: 0.9999215262276786\n",
      "Epoch[516/1000] loss: 0.001024049860591601, acc: 0.9999215262276786\n",
      "Epoch[517/1000] loss: 0.0010683736948392056, acc: 0.9998953683035714\n",
      "Epoch[518/1000] loss: 0.0009504068046745877, acc: 0.9999128069196429\n",
      "Epoch[519/1000] loss: 0.000976800684968891, acc: 0.9999564034598214\n",
      "Epoch[520/1000] loss: 0.0011296195357967268, acc: 0.9998517717633929\n",
      "Epoch[521/1000] loss: 0.0010230607079263012, acc: 0.9999040876116071\n",
      "Epoch[522/1000] loss: 0.0010297347084165917, acc: 0.99993896484375\n",
      "Epoch[523/1000] loss: 0.0009595899001786685, acc: 0.9999476841517857\n",
      "Epoch[524/1000] loss: 0.0009928139133990044, acc: 0.9999651227678571\n",
      "Epoch[525/1000] loss: 0.0009635102482466859, acc: 0.9999651227678571\n",
      "Epoch[526/1000] loss: 0.001007239707178087, acc: 0.99993896484375\n",
      "Epoch[527/1000] loss: 0.0010431855046460572, acc: 0.9998953683035714\n",
      "Epoch[528/1000] loss: 0.0010877648897543882, acc: 0.9999040876116071\n",
      "Epoch[529/1000] loss: 0.0009804298123136895, acc: 0.9999564034598214\n",
      "Epoch[530/1000] loss: 0.0010078294565443815, acc: 0.9998692103794643\n",
      "Epoch[531/1000] loss: 0.0010029019636671624, acc: 0.9999651227678571\n",
      "Epoch[532/1000] loss: 0.0010065242983858167, acc: 0.9999128069196429\n",
      "Epoch[533/1000] loss: 0.0009706836959334655, acc: 0.9999476841517857\n",
      "Epoch[534/1000] loss: 0.0009778308919651732, acc: 0.99993896484375\n",
      "Epoch[535/1000] loss: 0.0009329226933394759, acc: 0.9999564034598214\n",
      "Epoch[536/1000] loss: 0.0009823871298846956, acc: 0.9998953683035714\n",
      "Epoch[537/1000] loss: 0.0009314478946700026, acc: 0.99993896484375\n",
      "Epoch[538/1000] loss: 0.0008648087153103136, acc: 0.99993896484375\n",
      "Epoch[539/1000] loss: 0.000985375063464744, acc: 0.9999564034598214\n",
      "Epoch[540/1000] loss: 0.0009373716412872975, acc: 0.9999651227678571\n",
      "Epoch[541/1000] loss: 0.0009181586410704767, acc: 0.99993896484375\n",
      "Epoch[542/1000] loss: 0.0008301049427765454, acc: 0.9999651227678571\n",
      "Epoch[543/1000] loss: 0.0009002746955957264, acc: 0.9999476841517857\n",
      "Epoch[544/1000] loss: 0.0009412512671198263, acc: 0.9999302455357143\n",
      "Epoch[545/1000] loss: 0.00089814634468764, acc: 0.99993896484375\n",
      "Epoch[546/1000] loss: 0.0009057354282049346, acc: 0.9999476841517857\n",
      "Epoch[547/1000] loss: 0.0010364964148590974, acc: 0.9998604910714286\n",
      "Epoch[548/1000] loss: 0.0010404818559826318, acc: 0.9998779296875\n",
      "Epoch[549/1000] loss: 0.0008647978780313029, acc: 0.9999476841517857\n",
      "Epoch[550/1000] loss: 0.0008782757232828382, acc: 0.9999476841517857\n",
      "Epoch[551/1000] loss: 0.0009155138413916575, acc: 0.9999476841517857\n",
      "Epoch[552/1000] loss: 0.0008450429707279129, acc: 0.9999738420758929\n",
      "Epoch[553/1000] loss: 0.000905403073504983, acc: 0.9999564034598214\n",
      "Epoch[554/1000] loss: 0.0009049567085769793, acc: 0.9999215262276786\n",
      "Epoch[555/1000] loss: 0.0009100814152459082, acc: 0.9999215262276786\n",
      "Epoch[556/1000] loss: 0.0008866359634599316, acc: 0.9999476841517857\n",
      "Epoch[557/1000] loss: 0.0008001099813554902, acc: 0.9999651227678571\n",
      "Epoch[558/1000] loss: 0.0009219061770896328, acc: 0.99993896484375\n",
      "Epoch[559/1000] loss: 0.0009372897493449273, acc: 0.9999215262276786\n",
      "Epoch[560/1000] loss: 0.000956730280969558, acc: 0.9999040876116071\n",
      "Epoch[561/1000] loss: 0.0009012139701423751, acc: 0.9999476841517857\n",
      "Epoch[562/1000] loss: 0.0008753875829695192, acc: 0.9999302455357143\n",
      "Epoch[563/1000] loss: 0.0008678497179062106, acc: 0.9999476841517857\n",
      "Epoch[564/1000] loss: 0.0007409517945364184, acc: 0.9999825613839286\n",
      "Epoch[565/1000] loss: 0.0008808943779773212, acc: 0.9999302455357143\n",
      "Epoch[566/1000] loss: 0.0009261839201956588, acc: 0.9999128069196429\n",
      "Epoch[567/1000] loss: 0.0009376953878797524, acc: 0.99993896484375\n",
      "Epoch[568/1000] loss: 0.0008580006459461791, acc: 0.9999302455357143\n",
      "Epoch[569/1000] loss: 0.0008313015899018085, acc: 0.9999738420758929\n",
      "Epoch[570/1000] loss: 0.0009127504558169416, acc: 0.9999476841517857\n",
      "Epoch[571/1000] loss: 0.0008087689095970875, acc: 0.9999651227678571\n",
      "Epoch[572/1000] loss: 0.0008389401703295464, acc: 0.9999215262276786\n",
      "Epoch[573/1000] loss: 0.0008665282408141398, acc: 0.9999476841517857\n",
      "Epoch[574/1000] loss: 0.0007694163575381806, acc: 0.9999738420758929\n",
      "Epoch[575/1000] loss: 0.0007864011879844059, acc: 0.9999825613839286\n",
      "Epoch[576/1000] loss: 0.0008168767018495211, acc: 0.9999476841517857\n",
      "Epoch[577/1000] loss: 0.0008125756937390959, acc: 0.9999651227678571\n",
      "Epoch[578/1000] loss: 0.0008696049603063979, acc: 0.9999215262276786\n",
      "Epoch[579/1000] loss: 0.000869653259412319, acc: 0.9999215262276786\n",
      "Epoch[580/1000] loss: 0.0007843403584826904, acc: 0.9999476841517857\n",
      "Epoch[581/1000] loss: 0.0008082581897334811, acc: 0.9999651227678571\n",
      "Epoch[582/1000] loss: 0.000822599986739598, acc: 0.9999651227678571\n",
      "Epoch[583/1000] loss: 0.0007817783402320597, acc: 0.9999476841517857\n",
      "Epoch[584/1000] loss: 0.000840162945418602, acc: 0.9999302455357143\n",
      "Epoch[585/1000] loss: 0.000791530055201812, acc: 0.9999476841517857\n",
      "Epoch[586/1000] loss: 0.0008571573977990608, acc: 0.9999215262276786\n",
      "Epoch[587/1000] loss: 0.0007770535807399678, acc: 0.9999651227678571\n",
      "Epoch[588/1000] loss: 0.0007324473673569239, acc: 0.9999651227678571\n",
      "Epoch[589/1000] loss: 0.0008454156421586438, acc: 0.9999128069196429\n",
      "Epoch[590/1000] loss: 0.0008420772839079811, acc: 0.9999123917144983\n",
      "Epoch[591/1000] loss: 0.0007962925858789406, acc: 0.9999302455357143\n",
      "Epoch[592/1000] loss: 0.0008493343876969968, acc: 0.9999302455357143\n",
      "Epoch[593/1000] loss: 0.0008045977561518417, acc: 0.9999564034598214\n",
      "Epoch[594/1000] loss: 0.0007699891739321174, acc: 0.9999651227678571\n",
      "Epoch[595/1000] loss: 0.0008033393571947402, acc: 0.99993896484375\n",
      "Epoch[596/1000] loss: 0.0007097265245517649, acc: 0.9999738420758929\n",
      "Epoch[597/1000] loss: 0.0007471883306737125, acc: 0.9999476841517857\n",
      "Epoch[598/1000] loss: 0.000799628897376741, acc: 0.9999302455357143\n",
      "Epoch[599/1000] loss: 0.0007646274768506243, acc: 0.9999651227678571\n",
      "Epoch[600/1000] loss: 0.0007852800138477635, acc: 0.99993896484375\n",
      "Epoch[601/1000] loss: 0.0007498596603129824, acc: 0.9999476841517857\n",
      "Epoch[602/1000] loss: 0.0007139127483242191, acc: 0.9999564034598214\n",
      "Epoch[603/1000] loss: 0.000879970652801733, acc: 0.9999215262276786\n",
      "Epoch[604/1000] loss: 0.0007169152677306556, acc: 0.9999738420758929\n",
      "Epoch[605/1000] loss: 0.0007213486421408431, acc: 0.9999564034598214\n",
      "Epoch[606/1000] loss: 0.0007803004297914283, acc: 0.9999215262276786\n",
      "Epoch[607/1000] loss: 0.0007228699475620358, acc: 0.99993896484375\n",
      "Epoch[608/1000] loss: 0.0007466538481821772, acc: 0.9999651227678571\n",
      "Epoch[609/1000] loss: 0.0007740393454826387, acc: 0.99993896484375\n",
      "Epoch[610/1000] loss: 0.0008209865379415403, acc: 0.9999476841517857\n",
      "Epoch[611/1000] loss: 0.0007412938091095254, acc: 0.9999215262276786\n",
      "Epoch[612/1000] loss: 0.0007756703166056208, acc: 0.9999302455357143\n",
      "Epoch[613/1000] loss: 0.000680864669967767, acc: 0.9999738420758929\n",
      "Epoch[614/1000] loss: 0.0008053048698327205, acc: 0.99993896484375\n",
      "Epoch[615/1000] loss: 0.00064819082100647, acc: 0.9999912806919643\n",
      "Epoch[616/1000] loss: 0.000760042435883536, acc: 0.999921111022534\n",
      "Epoch[617/1000] loss: 0.0007436378304451605, acc: 0.9999651227678571\n",
      "Epoch[618/1000] loss: 0.0007473556105261585, acc: 0.9999302455357143\n",
      "Epoch[619/1000] loss: 0.0007230486235130229, acc: 0.9999564034598214\n",
      "Epoch[620/1000] loss: 0.0006858920150989434, acc: 0.9999651227678571\n",
      "Epoch[621/1000] loss: 0.0007083794260844505, acc: 0.9999651227678571\n",
      "Epoch[622/1000] loss: 0.0007516687928728061, acc: 0.9999738420758929\n",
      "Epoch[623/1000] loss: 0.000718116162196176, acc: 0.9999738420758929\n",
      "Epoch[624/1000] loss: 0.0007638073053125741, acc: 0.9999302455357143\n",
      "Epoch[625/1000] loss: 0.0006616538518885916, acc: 0.9999738420758929\n",
      "Epoch[626/1000] loss: 0.0007868499017474408, acc: 0.99993896484375\n",
      "Epoch[627/1000] loss: 0.0006629623625745548, acc: 0.9999825613839286\n",
      "Epoch[628/1000] loss: 0.0007319277800499029, acc: 0.9999476841517857\n",
      "Epoch[629/1000] loss: 0.000658391759186218, acc: 0.9999651227678571\n",
      "Epoch[630/1000] loss: 0.0006715279341733549, acc: 0.9999564034598214\n",
      "Epoch[631/1000] loss: 0.0006383188664195975, acc: 0.99993896484375\n",
      "Epoch[632/1000] loss: 0.0007253485986698901, acc: 0.99993896484375\n",
      "Epoch[633/1000] loss: 0.0007095114620564605, acc: 0.9999476841517857\n",
      "Epoch[634/1000] loss: 0.0006223903228601557, acc: 0.9999825613839286\n",
      "Epoch[635/1000] loss: 0.0006662533414782956, acc: 0.9999476841517857\n",
      "Epoch[636/1000] loss: 0.0007723620175446351, acc: 0.999921111022534\n",
      "Epoch[637/1000] loss: 0.0008750618506902745, acc: 0.9998953683035714\n",
      "Epoch[638/1000] loss: 0.0006735508550264058, acc: 0.9999476841517857\n",
      "Epoch[639/1000] loss: 0.000640784469396749, acc: 0.9999738420758929\n",
      "Epoch[640/1000] loss: 0.000651792403134875, acc: 0.9999651227678571\n",
      "Epoch[641/1000] loss: 0.0006631042016514195, acc: 0.9999651227678571\n",
      "Epoch[642/1000] loss: 0.0006407034382261502, acc: 0.9999825613839286\n",
      "Epoch[643/1000] loss: 0.0006004482616585197, acc: 0.9999738420758929\n",
      "Epoch[644/1000] loss: 0.0006554454570765042, acc: 0.9999825613839286\n",
      "Epoch[645/1000] loss: 0.000638249861757296, acc: 0.99993896484375\n",
      "Epoch[646/1000] loss: 0.0006008066477858977, acc: 0.9999825613839286\n",
      "Epoch[647/1000] loss: 0.0006099333321409566, acc: 0.9999825613839286\n",
      "Epoch[648/1000] loss: 0.00062295147095678, acc: 0.9999738420758929\n",
      "Epoch[649/1000] loss: 0.0005841815240533574, acc: 0.9999738420758929\n",
      "Epoch[650/1000] loss: 0.0005761758515551005, acc: 0.9999912806919643\n",
      "Epoch[651/1000] loss: 0.0006201034437773549, acc: 0.9999564034598214\n",
      "Epoch[652/1000] loss: 0.0006077699079989023, acc: 0.9999738420758929\n",
      "Epoch[653/1000] loss: 0.0006366248925197786, acc: 0.9999564034598214\n",
      "Epoch[654/1000] loss: 0.0005716342432476397, acc: 0.9999912806919643\n",
      "Epoch[655/1000] loss: 0.0006060543818031354, acc: 0.9999564034598214\n",
      "Epoch[656/1000] loss: 0.0005805862500502761, acc: 0.9999825613839286\n",
      "Epoch[657/1000] loss: 0.0005596251776296413, acc: 0.9999825613839286\n",
      "Epoch[658/1000] loss: 0.00062442894808815, acc: 0.9999651227678571\n",
      "Epoch[659/1000] loss: 0.000586575695706415, acc: 0.9999651227678571\n",
      "Epoch[660/1000] loss: 0.0006286124047879379, acc: 0.9999651227678571\n",
      "Epoch[661/1000] loss: 0.0006219992090856456, acc: 0.9999476841517857\n",
      "Epoch[662/1000] loss: 0.0006384209277062577, acc: 0.9999476841517857\n",
      "Epoch[663/1000] loss: 0.0005846032518093125, acc: 0.9999825613839286\n",
      "Epoch[664/1000] loss: 0.0006081539395381697, acc: 0.9999825613839286\n",
      "Epoch[665/1000] loss: 0.0006642699748096805, acc: 0.99993896484375\n",
      "Epoch[666/1000] loss: 0.0006110067800234122, acc: 0.9999738420758929\n",
      "Epoch[667/1000] loss: 0.0005818870006675882, acc: 0.9999825613839286\n",
      "Epoch[668/1000] loss: 0.0006937322619445954, acc: 0.99993896484375\n",
      "Epoch[669/1000] loss: 0.0006186501216559139, acc: 0.9999564034598214\n",
      "Epoch[670/1000] loss: 0.0006275371390854291, acc: 0.9999651227678571\n",
      "Epoch[671/1000] loss: 0.0005720113879630974, acc: 0.9999738420758929\n",
      "Epoch[672/1000] loss: 0.0006494147850649565, acc: 0.99993896484375\n",
      "Epoch[673/1000] loss: 0.0006351153875324858, acc: 0.9999476841517857\n",
      "Epoch[674/1000] loss: 0.0005375215055859631, acc: 0.9999912806919643\n",
      "Epoch[675/1000] loss: 0.0006038395227473561, acc: 0.9999564034598214\n",
      "Epoch[676/1000] loss: 0.0005707072509072272, acc: 0.9999738420758929\n",
      "Epoch[677/1000] loss: 0.0006171372199657656, acc: 0.9999564034598214\n",
      "Epoch[678/1000] loss: 0.0005261000553998331, acc: 0.9999825613839286\n",
      "Epoch[679/1000] loss: 0.0005814011058516501, acc: 0.9999476841517857\n",
      "Epoch[680/1000] loss: 0.0005865912935405504, acc: 0.9999738420758929\n",
      "Epoch[681/1000] loss: 0.0005514833652081766, acc: 0.9999825613839286\n",
      "Epoch[682/1000] loss: 0.0005560752809157878, acc: 1.0\n",
      "Reached desired accuracy. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "if training_choice == True:\n",
    "    metric = {'loss': [], 'acc': []}\n",
    "    for i_epoch in range(epoch):\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        model.train(mode=True)\n",
    "        for i_batch, (image, label) in enumerate(dataloader):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            pred = model.forward(image)  # inference\n",
    "\n",
    "            loss = criterion(pred, label)  # calculate loss\n",
    "            optimizer.zero_grad()  # reset gradient to zero\n",
    "            loss.backward()  # calculate gradient\n",
    "            optimizer.step()  # optimize weight (using gradient)\n",
    "\n",
    "            train_loss += [loss.item()]\n",
    "            train_acc += [accuracy(pred, label)]\n",
    "\n",
    "        metric['loss'] += [sum(train_loss)/ len(dataloader)]\n",
    "        metric['acc'] += [sum(train_acc)/ len(dataloader)]\n",
    "        print(f'Epoch[{i_epoch+1}/{epoch}] loss: {metric[\"loss\"][-1]}, acc: {metric[\"acc\"][-1]}')\n",
    "        if metric[\"acc\"][-1] >= 1.0:  # 设定一个接近1的阈值\n",
    "            print(f'Reached desired accuracy. Stopping training.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if training_choice == True:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    ax1.plot(range(len(metric[\"loss\"])), metric[\"loss\"])\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax2.plot(range(len(metric[\"acc\"])), metric[\"acc\"])\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(output_path)\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.jit.script(model).save(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = Path('./model/hira.pt')  # TODO: adjust path\n",
    "# assert model_path.exists() is True\n",
    "# image_path = Path(f'./{DATASET_PATH}/99.jpg')  # TODO: adjust path\n",
    "# assert image_path.exists() is True\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((83, 84)),  # TODO: adjust img_size\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# trained_model = torch.jit.load(model_path)\n",
    "# test_image = Image.open(image_path)\n",
    "# trans_image = img_convert(test_image)\n",
    "# # trans_image = transform(test_image)\n",
    "\n",
    "# pred = trained_model(trans_image.unsqueeze(0))\n",
    "# pred_label = pred.max(1)[1]\n",
    "\n",
    "# print(pred_label)\n",
    "# test_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ttttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def img_convert(input: np.ndarray) -> torch.Tensor: #回傳的是torch.Tensor，用於測試\n",
    "    # 将图像转换为灰度影像\n",
    "    gray_image = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY)\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(gray_image, (83, 84))\n",
    "\n",
    "    equalized_image = cv2.equalizeHist(resized_image)\n",
    "\n",
    "    # 多次侵蚀后膨胀\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    eroded_image = cv2.erode(equalized_image, kernel, iterations=2)\n",
    "    dilated_image = cv2.dilate(eroded_image, kernel, iterations=2)\n",
    "\n",
    "    # 调整图像大小为 (83, 84)\n",
    "    resized_image = cv2.resize(dilated_image, (83, 84))\n",
    "\n",
    "    # 将图像转换为张量并调整形状\n",
    "    tensor_image = transforms.ToTensor()(resized_image)\n",
    "    # tensor_image = torch.unsqueeze(tensor_image, 0)\n",
    "\n",
    "    return tensor_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=CNN\n",
       "  (cnn1): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (relu1): RecursiveScriptModule(original_name=ReLU)\n",
       "  (maxpool1): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (cnn2): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (relu2): RecursiveScriptModule(original_name=ReLU)\n",
       "  (maxpool2): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (hidden_layer): RecursiveScriptModule(original_name=Linear)\n",
       "  (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "  (fc1): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\",device)\n",
    "\n",
    "model_path = Path('./model/hira.pt',map_location=device)  # 請調整路徑\n",
    "assert model_path.exists() is True\n",
    "\n",
    "trained_model = torch.jit.load(model_path, map_location=device).to(device)\n",
    "\n",
    "label_dict = {\n",
    "    0: 'A', 1: 'BA', 2: 'CHI', 3: 'DA', 4: 'E', 5: 'FU', 6: 'HA', 7: 'HE', 8: 'HI', 9: 'HO', 10: 'I', 11: 'JI',\n",
    "    12: 'KA', 13: 'KE', 14: 'KI', 15: 'KO', 16: 'KU', 17: 'MA', 18: 'ME', 19: 'MI', 20: 'MO', 21: 'MU', 22: 'N',\n",
    "    23: 'NA', 24: 'NE', 25: 'NI', 26: 'NO', 27: 'NU', 28: 'O', 29: 'PI', 30: 'RA', 31: 'RE', 32: 'RI', 33: 'RO',\n",
    "    34: 'RU', 35: 'SA', 36: 'SE', 37: 'SHI', 38: 'SO', 39: 'SU', 40: 'TA', 41: 'TE', 42: 'TO', 43: 'TSU', 44: 'U',\n",
    "    45: 'WA', 46: 'WO', 47: 'YA', 48: 'YO', 49: 'YU'\n",
    "}\n",
    "\n",
    "trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test path set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_path = 'fdata'\n",
    "# test_data_path = 'data_new'\n",
    "# test_data_path = 'data_color'\n",
    "# test_data_path = 'data_color2'\n",
    "# test_data_path = 'test_color'\n",
    "test_data_path = 'testdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测错误的图像：testdata/219.jpg\t真实标签：I\t预测标签：KE\n",
      "-------------------------------------\n",
      "预测错误的图像：testdata/439.jpg\t真实标签：MU\t预测标签：SHI\n",
      "-------------------------------------\n",
      "预测错误的图像：testdata/531.jpg\t真实标签：NO\t预测标签：YO\n",
      "-------------------------------------\n",
      "预测错误的图像：testdata/898.jpg\t真实标签：U\t预测标签：KO\n",
      "-------------------------------------\n",
      "Accuracy: 99.60%\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(f'{test_data_path}/label.txt', mode='r') as f:\n",
    "    labels = f.read().split('\\n')\n",
    "labels = [label_dict[int(label)] for label in labels]\n",
    "\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    image_path = f'{test_data_path}/{i}.jpg'  # 依據預設命名規則為每個圖片建立對應的路徑\n",
    "    if os.path.exists(image_path):\n",
    "        test_image = cv2.imread(image_path)  # 使用 cv2 讀取圖片\n",
    "        converted_image = img_convert(test_image)  # 將圖片轉換成您需要的格式\n",
    "        # trans_image = transform(converted_image)  # 應用其他轉換（如果需要）\n",
    "        # pred = trained_model(trans_image.unsqueeze(0))\n",
    "        pred = trained_model(converted_image.unsqueeze(0).to(device))\n",
    "        pred_label = label_dict[pred.max(1)[1].item()]\n",
    "        predictions.append((label, pred_label))\n",
    "\n",
    "        if label != pred_label:\n",
    "            print(f\"预测错误的图像：{image_path}\",end='\\t')\n",
    "            # plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "            print(f\"真实标签：{label}\",end='\\t')\n",
    "            print(f\"预测标签：{pred_label}\")\n",
    "            print(\"-------------------------------------\")\n",
    "total = len(predictions)\n",
    "correct = sum(1 for label, pred_label in predictions if label == pred_label)\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      A  BA  CHI  DA   E  FU  HA  HE  HI  HO   I  JI  KA  KE  KI  KO  KU  MA  \\\n",
      "A    20   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "BA    0  20    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "CHI   0   0   20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "DA    0   0    0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "E     0   0    0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "FU    0   0    0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HA    0   0    0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HE    0   0    0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   \n",
      "HI    0   0    0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   \n",
      "HO    0   0    0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   \n",
      "I     0   0    0   0   0   0   0   0   0   0  19   0   0   1   0   0   0   0   \n",
      "JI    0   0    0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   \n",
      "KA    0   0    0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   \n",
      "KE    0   0    0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   \n",
      "KI    0   0    0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   \n",
      "KO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   \n",
      "KU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   \n",
      "MA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   \n",
      "ME    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "N     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "O     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "PI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RI    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "RU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SHI   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TE    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TSU   0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "U     0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   \n",
      "WA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "WO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YA    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YO    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YU    0   0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "     ME  MI  MO  MU   N  NA  NE  NI  NO  NU   O  PI  RA  RE  RI  RO  RU  SA  \\\n",
      "A     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "BA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "CHI   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "DA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "E     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "FU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HI    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "HO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "I     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "JI    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KI    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "KU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "ME   20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MI    0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MO    0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "MU    0   0   0  19   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "N     0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NA    0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NE    0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0   \n",
      "NI    0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   \n",
      "NO    0   0   0   0   0   0   0   0  19   0   0   0   0   0   0   0   0   0   \n",
      "NU    0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   \n",
      "O     0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   \n",
      "PI    0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   \n",
      "RA    0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   \n",
      "RE    0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   0   \n",
      "RI    0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   0   \n",
      "RO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   0   \n",
      "RU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   0   \n",
      "SA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  20   \n",
      "SE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SHI   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "SU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TE    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "TSU   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "U     0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "WA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "WO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YA    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YO    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "YU    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "     SE  SHI  SO  SU  TA  TE  TO  TSU   U  WA  WO  YA  YO  YU  \n",
      "A     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "BA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "CHI   0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "DA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "E     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "FU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "HA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "HE    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "HI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "HO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "I     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "JI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KE    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "KU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "ME    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "MU    0    1   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "N     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NE    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "NO    0    0   0   0   0   0   0    0   0   0   0   0   1   0  \n",
      "NU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "O     0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "PI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RE    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RI    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RO    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "RU    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SA    0    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SE   20    0   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SHI   0   20   0   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SO    0    0  20   0   0   0   0    0   0   0   0   0   0   0  \n",
      "SU    0    0   0  20   0   0   0    0   0   0   0   0   0   0  \n",
      "TA    0    0   0   0  20   0   0    0   0   0   0   0   0   0  \n",
      "TE    0    0   0   0   0  20   0    0   0   0   0   0   0   0  \n",
      "TO    0    0   0   0   0   0  20    0   0   0   0   0   0   0  \n",
      "TSU   0    0   0   0   0   0   0   20   0   0   0   0   0   0  \n",
      "U     0    0   0   0   0   0   0    0  19   0   0   0   0   0  \n",
      "WA    0    0   0   0   0   0   0    0   0  20   0   0   0   0  \n",
      "WO    0    0   0   0   0   0   0    0   0   0  20   0   0   0  \n",
      "YA    0    0   0   0   0   0   0    0   0   0   0  20   0   0  \n",
      "YO    0    0   0   0   0   0   0    0   0   0   0   0  20   0  \n",
      "YU    0    0   0   0   0   0   0    0   0   0   0   0   0  20  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建一个空的数据帧\n",
    "df = pd.DataFrame(index=label_dict.values(), columns=label_dict.values())\n",
    "df = df.fillna(0)  # 将所有值初始化为0\n",
    "\n",
    "# 计算正確值和預測值的交叉计数\n",
    "for label, pred_label in predictions:\n",
    "    df.loc[label, pred_label] += 1\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.60%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 比對預測結果和真實標籤\n",
    "# for label, pred_label in predictions:\n",
    "#     if label == pred_label:\n",
    "#         print(f'Label: {label}\\tPrediction: {pred_label}\\tCorrect')\n",
    "total = len(predictions)\n",
    "correct = sum(1 for label, pred_label in predictions if label == pred_label)\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('testing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22c60e53b9b57bc2d5950e7c6d3f647d37bca2c21c4934a06f07d6484f6bc0f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
